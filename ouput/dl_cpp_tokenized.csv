"BUGGY_CODE","FIXED_CODE"
"std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork ( std::vector<at::Device> devices , int rank , OpType opType ) { return std::make_shared<ProcessGroupNCCL::WorkNCCL> ( devices , rank , opType ) ; } ","std::shared_ptr<ProcessGroupNCCL::WorkNCCL> ProcessGroupNCCL::initWork ( std::vector<at::Device> devices ) { return std::make_shared<ProcessGroupNCCL::WorkNCCL> ( devices ) ; } "
"std::chrono::milliseconds ( kWaitForAbortCommStoreKey ) ) ; LOG ( INFO ) << ""Found key in store: "" << storeKey << "", aborting appropriate communicators"" ; ","std::chrono::milliseconds ( kWaitForAbortCommStoreKey ) ) ; auto val = store_ -> get ( storeKey ) ; std::string rank ( reinterpret_cast < char * > ( val . data ( ) ) , val . size ( ) ) ; LOG ( INFO ) << ""[Rank "" << rank_ << ""] Found key in store: "" << storeKey << "", from rank: "" << rank << "", aborting appropriate communicators"" ; "
"if ( rank_ == 0 | | ( commType != NCCLCommType::COLL & & p2pRank == 0 ) ) { C10D_NCCL_CHECK ( ncclGetUniqueId ( & ncclID ) ) ; } ","if ( rank_ == 0 | | ( isP2POp ( opType ) & & p2pRank == 0 ) ) { C10D_NCCL_CHECK ( ncclGetUniqueId ( & ncclID ) ) ; } "
"int numRanks , rank ; if ( commType == NCCLCommType::COLL ) { numRanks = getSize ( ) * devices . size ( ) ; rank = getRank ( ) * devices . size ( ) + i ; } else { numRanks = 2 ; rank = p2pRank ; } ","int numRanks , rank ; if ( sP2POp ( opType ) ) { numRanks = getSize ( ) * devices . size ( ) ; rank = getRank ( ) * devices . size ( ) + i ; } else { numRanks = 2 ; rank = p2pRank ; } "
"if ( str != ""None"" & & str != """" ) { throw std::runtime_error ( ""invalid default string: "" + str ) ; } ","if ( str != ""None"" ) { default_string = parse_string_literal ( str ) ; } "
"if ( check_has_torch_function ( ( PyObject * ) self ) ) { return handle_torch_function_getter ( self , ""names"" ) ; } size_t size = self -> cdata . dim ( ) ; THPObjectPtr tuple ( PyTuple_New ( size ) ) ; if ( uple ) throw python_error ( ) ; const auto dimnames = self -> cdata . names ( ) ; ","if ( check_has_torch_function ( self ) ) { return handle_torch_function_getter ( ( THPVariable * ) self , ""names"" ) ; } size_t size = ( ( THPVariable * ) self ) -> cdata . dim ( ) ; THPObjectPtr tuple ( PyTuple_New ( size ) ) ; if ( uple ) throw python_error ( ) ; const auto dimnames = ( ( THPVariable * ) self ) -> cdata . names ( ) ; "
"if ( check_has_torch_function ( ( PyObject * ) self ) ) { return handle_torch_function_setter ( self , ""names"" , names ) ; } auto & var = self -> cdata ; ","if ( check_has_torch_function ( self ) ) { return handle_torch_function_setter ( ( THPVariable * ) self , ""names"" , names ) ; } auto & var = ( ( THPVariable * ) self ) -> cdata ; if ( names == Py_None ) "
"template < typename T > using shared_ptr_class_ = py::class_ < T , std::shared_ptr<T> > ; PyObject * dist_autograd_init ( PyObject * ) { auto autograd_module = THPObjectPtr ( PyImport_ImportModule ( ""torch.distributed.autograd"" ) ) ; ","template < typename T > using shared_ptr_class_ = py::class_ < T , std::shared_ptr<T> > ; PyObject * dist_autograd_init ( PyObject * _unused , PyObject * noargs ) { auto autograd_module = THPObjectPtr ( PyImport_ImportModule ( ""torch.distributed.autograd"" ) ) ; "
"PyObject * c10d_init ( PyObject * _unused ) { C10_LOG_API_USAGE_ONCE ( ""c10d.python.import"" ) ; auto c10d_module = THPObjectPtr ( PyImport_ImportModule ( ""torch.distributed"" ) ) ; ","PyObject * c10d_init ( PyObject * _unused , PyObject * noargs ) { C10_LOG_API_USAGE_ONCE ( ""c10d.python.import"" ) ; auto c10d_module = THPObjectPtr ( PyImport_ImportModule ( ""torch.distributed"" ) ) ; "
"static PyMethodDef methods [ ] = { { ""_rpc_init"" , ( PyCFunction ) rpc_init , METH_NOARGS , nullptr } , { nullptr , nullptr , 0 , nullptr } } ; PyMethodDef * python_functions ( ) ","static PyMethodDef methods [ ] = { { ""_rpc_init"" , rpc_init , METH_NOARGS , nullptr } , { nullptr , nullptr , 0 , nullptr } } ; PyMethodDef * python_functions ( ) "
"if ( beta <= 0 ) return at::native::l1_loss ( input , target , reduction ) ; Tensor loss ; auto iter = TensorIterator::binary_op ( loss , input , target ) ; smooth_l1_stub ( iter . device_type ( ) , iter , beta ) ; ","if ( beta == 0 ) { return at::native::l1_loss ( input , target , reduction ) ; } Tensor loss ; auto iter = TensorIterator::binary_op ( loss , input , target ) ; smooth_l1_stub ( iter . device_type ( ) , iter , beta ) ; "
"if ( asyncErrorHandling_ ) { std::unique_lock<std::mutex> lock ( workListMutex_ ) ; for ( auto & work : workList_ ) { work -> checkAndSetException ( ) ; if ( work -> exception ( ) ) { continue ; } ","if ( asyncErrorHandling_ ) { std::unique_lock<std::mutex> lock ( workMetaListMutex_ ) ; for ( auto & work : workMetaList_ ) { work . checkAndSetException ( ) ; if ( work . exception ( ) ) { continue ; } "
"Tensor std_backward ( const Tensor & result , const Tensor & grad , const Tensor & self , bool unbiased ) { return var_backward ( grad / ( result * 2 ) , self , unbiased ) ; } ","Tensor std_backward ( const Tensor & result , const Tensor & grad , const Tensor & self , bool unbiased ) { return var_backward ( ( grad / ( result * 2 ) ) . masked_fill_ ( result == 0 , 0 ) , self , unbiased ) ; } "
"std::unordered_map < std::string , std::string > copied_metadata = owner_ -> metadata ( ) ; if ( owner_ -> metadata ( ) . find ( ""model_name"" ) == owner_ -> metadata ( ) . end ( ) ) { copied_metadata [ ""model_name"" ] = name ( ) ; } if ( observer ) { observer -> onEnterRunMethod ( copied_metadata , function_ -> name ( ) ) ; ","std::unordered_map < std::string , std::string > copied_metadata = owner_ -> metadata ( ) ; if ( owner_ -> metadata ( ) . find ( ""model_name"" ) == owner_ -> metadata ( ) . end ( ) ) { copied_metadata [ ""model_name"" ] = owner_ -> name ( ) ; } if ( observer ) { observer -> onEnterRunMethod ( copied_metadata , function_ -> name ( ) ) ; "
"int device ; C10_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; void * r = nullptr ; if ( size != 0 ) { caching_allocator . malloc ( & r , device , size , cuda::getCurrentCUDAStream ( device ) ) ; } ","int device ; C10_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; void * r = nullptr ; if ( forceUncachedAllocator ( ) ) { C10_CUDA_CHECK ( cudaMalloc ( & r , size ) ) ; return { r , r , & uncached_delete , Device ( DeviceType::CUDA , device ) } ; } if ( size != 0 ) { caching_allocator . malloc ( & r , device , size , cuda::getCurrentCUDAStream ( device ) ) ; } "
"void set_data ( const Tensor & self , const Tensor & new_data ) { AT_ERROR ( ""set_data is not implemented for Tensor"" ) ; } ","void set_data ( Tensor & self , const Tensor & new_data ) { AT_ERROR ( ""set_data is not implemented for Tensor"" ) ; } "
"void retain_grad ( const Tensor & self ) { AT_ERROR ( ""retain_grad is not implemented for Tensor"" ) ; } ","void retain_grad ( Tensor & self ) { AT_ERROR ( ""retain_grad is not implemented for Tensor"" ) ; } "
"Tensor atan2 ( const Tensor & self , const Tensor & other ) { Tensor result = at::empty ( { 0 } , self . options ( ) ) ; return native::atan2_out ( result , self , other ) ; } ","Tensor atan2 ( const Tensor & self , const Tensor & other ) { Tensor result ; auto iter = TensorIterator::binary_float_op ( result , self , other ) ; atan2_stub ( iter . device_type ( ) , iter ) ; return iter . output ( ) ; } "
"int nested_if_then_else_ = 0 ; const Store * nested_store_ { nullptr } ; std::unordered_set < const Var * > thread_local_bufs_ ; ","int nested_if_then_else_ { 0 } ; const Store * nested_store_ { nullptr } ; bool nested_let_ { false } ; std::unordered_set < const Var * > thread_local_bufs_ ; "
"if ( last_thread_dim_ & & xprEquals ( last_thread_dim_ , v -> stop ( ) ) ) { need_sync_ = true ; } last_thread_dim_ = v -> stop ( ) ; const Var * metaVar = gpu_thread_vars_ [ gpu_thread_index ] ; body = Substitute ( Stmt::clone ( body ) , { { v -> var ( ) , metaVar } } ) ; ","const Var * metaVar = gpu_thread_vars_ [ gpu_thread_index ] ; body = Substitute ( Stmt::clone ( body ) , { { v -> var ( ) , metaVar } } ) ; "
"new_block = fuseConditions ( new_block ) ; return fuseSyncThreads ( new_block ) ; ","new_block = fuseConditions ( new_block ) ; return fuseSyncThreads ( new_block ) ; "
"constexpr inline bool lda_cond ( int64_t m , int64_t n , int64_t lda ) { return n == 1 | | lda > std::max<int64_t> ( 1 L , m ) ; } ","constexpr inline bool lda_cond ( int64_t m , int64_t n , int64_t lda ) { return n == 1 | | lda >= std::max<int64_t> ( 1 L , m ) ; } "
"if ( beta != scalar_t ( 1 ) ) scal<scalar_t> ( m , beta , y , incy ) ; for ( int64_t j = 0 ; j < n ; j + + ) { scalar_t * column_ = a + lda * j ; scalar_t z = alpha * x [ j * incx ] ; for ( int64_t i = 0 ; i < m ; i + + ) { y [ i * incy ] += z * column_ [ i ] ; } } ","if ( beta != scalar_t ( 1 ) & & beta != scalar_t ( 0 ) ) scal<scalar_t> ( m , beta , y , incy ) ; for ( int64_t j = 0 ; j < n ; j + + ) { scalar_t * column_ = a + lda * j ; scalar_t z = alpha * x [ j * incx ] ; for ( int64_t i = 0 ; i < m ; i + + ) { if ( j == 0 & & beta == scalar_t ( 0 ) ) { y [ i * incy ] = scalar_t ( 0 ) ; } y [ i * incy ] += z * column_ [ i ] ; } } "
"if ( cond_new -> isConstant ( ) ) { if ( mmediateEquals ( cond_new , 0 ) ) { return Stmt::clone ( true_new ) ; } else { return Stmt::clone ( false_new ) ; } } ","if ( cond_new -> isConstant ( ) ) { if ( mmediateEquals ( cond_new , 0 ) ) { return true_new Stmt::clone ( true_new ) : nullptr ; } else { return false_new Stmt::clone ( false_new ) : nullptr ; } } "
"auto & work = * it ; if ( work -> isCompleted ( ) ) { it = workList_ . erase ( it ) ; } ","auto & work = * it ; if ( work -> isCompleted ( ) ) { work -> handleNCCLGuard ( ) ; it = workList_ . erase ( it ) ; } "
"Stmt * loop = l . getLoopBodyFor ( p . second ) ; if ( torch::jit::tensorexpr::HasRand ( loop ) . has_rand ( ) ) { l . computeInlineWithRandom ( loop ) ; } else { l . computeInline ( loop ) ; } ","l . computeInline ( p . second -> buf ( ) ) ; "
"if ( v -> op_type ( ) == kSigmoid ) { auto x = v -> param ( 0 ) -> accept_mutator ( this ) ; ExprHandle y = ExprHandle ( 1 . 0 f ) / ( ExprHandle ( 1 . 0 f ) + exp ( ExprHandle ( -0 . 0 f ) - ExprHandle ( x ) ) ) ; return y . node ( ) ; } ","if ( v -> op_type ( ) == kSigmoid ) { auto x = v -> param ( 0 ) -> accept_mutator ( this ) ; auto one = ExprHandle ( getImmediateByType ( v -> dtype ( ) , 1 . 0 ) ) ; auto zero = ExprHandle ( getImmediateByType ( v -> dtype ( ) , 0 . 0 ) ) ; ExprHandle y = one / ( one + exp ( zero - ExprHandle ( x ) ) ) ; return y . node ( ) ; } "
"Tensor grad_input ; auto iter = at::TensorIteratorConfig ( ) . set_check_mem_overlap ( true ) . add_output ( grad_input ) . add_input ( input ) . add_input ( buffer ) ","Tensor grad_input ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( buffer ) "
"auto iter = TensorIteratorConfig ( ) . set_check_mem_overlap ( true ) . add_output ( self ) . add_input ( src ) . resize_outputs ( false ) ","auto iter = TensorIteratorConfig ( ) . add_output ( self ) . add_input ( src ) . resize_outputs ( false ) "
"auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . set_check_mem_overlap ( true ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) ","auto norm = reduction == Reduction::Mean 1 . / input . numel ( ) : 1 . ; auto iter = at::TensorIteratorConfig ( ) . add_output ( grad_input ) . add_input ( input ) . add_input ( target ) "
"auto result_stride_bytes = result . stride ( dim ) * elementSize ( result . scalar_type ( ) ) ; auto iter = TensorIteratorConfig ( ) . resize_outputs ( false ) . add_output ( result_slice ) . add_input ( source_slice ) ","auto result_stride_bytes = result . stride ( dim ) * elementSize ( result . scalar_type ( ) ) ; auto iter = TensorIteratorConfig ( ) . set_check_mem_overlap ( false ) . resize_outputs ( false ) . add_output ( result_slice ) . add_input ( source_slice ) "
"if ( optimized_plan_ ) { return * optimized_plan_ ; } ","if ( optimized_plan_ ) { GRAPH_DUMP ( ""plan already optimized:"" , graph ) ; return * optimized_plan_ ; } "
"List < intrusive_ptr<ivalue::Future> > ( ctx -> srcFutures . elementType ( ) ) ; if ( src -> hasError ( ) ) { dst -> setError ( * src -> error ( ) ) ; } else { dst -> markCompleted ( src -> constValue ( ) ) ; } ","List < intrusive_ptr<ivalue::Future> > ( ctx -> srcFutures . elementType ( ) ) ; if ( src -> hasError ( ) ) { dst -> setError ( src -> exception_ptr ( ) ) ; } else { dst -> markCompleted ( src -> constValue ( ) ) ; } "
"if ( future_ ) { future_ -> setError ( Future::FutureError ( ss . str ( ) ) ) ; } else if ( is_jit_exception ) { throw JITException ( ss . str ( ) ) ; } ","if ( future_ ) { future_ -> setError ( std::make_exception_ptr ( Future::FutureError ( ss . str ( ) ) ) ) ; } else if ( is_jit_exception ) { throw JITException ( ss . str ( ) ) ; } "
"if ( value -> type ( ) -> isSubtypeOf ( StringType::get ( ) ) & & DeviceObjType::get ( ) -> isSubtypeOf ( concrete_type ) ) { return graph . insert ( aten::device , { value } , { } , loc ) ; } ","if ( value -> type ( ) -> isSubtypeOf ( StringType::get ( ) ) & & concrete_type -> isSubtypeOf ( DeviceObjType::get ( ) ) ) { return graph . insert ( aten::device , { value } , { } , loc ) ; } "
"std::map < ParallelType , bool > ParallelTypeBitmap::getMap ( ) const { std::map < ParallelType , bool > map ; for ( const auto & pt_offset : pt_to_offset_ ) { map . emplace ( std::make_pair ( pt_offset . first , bitset_ [ pt_offset . second ] ) ) ; } return map ; } ","std::map < ParallelType , bool > ParallelTypeBitmap::getMap ( ) const { std::map < ParallelType , bool > map ; for ( const auto & pt_offset : pt_to_offset_ ) { map . emplace ( pt_offset . first , bitset_ [ pt_offset . second ] ) ; } return map ; } "
"static void THPGenerator_dealloc ( THPGenerator * self ) { self -> cdata . set_pyobj ( nullptr ) ; self -> cdata . ~ Generator ( ) ; Py_TYPE ( self ) -> tp_free ( ( PyObject * ) self ) ; } ","static void THPGenerator_dealloc ( PyObject * _self ) { auto self = reinterpret_cast < THPGenerator * > ( _self ) ; if ( self -> cdata . defined ( ) ) { self -> cdata . set_pyobj ( nullptr ) ; self -> cdata . ~ Generator ( ) ; } Py_TYPE ( _self ) -> tp_free ( _self ) ; } "
"static Tensor norm ( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype ) { Tensor result ; return at::native::norm_out ( result , self , p , dim , keepdim , opt_dtype ) ; } ","static Tensor norm ( const Tensor & self , optional<Scalar> p , IntArrayRef dim , bool keepdim , optional<ScalarType> opt_dtype ) { if ( self . is_sparse ( ) ) { return at::native_norm ( self , p , dim , keepdim , opt_dtype ) ; } else { Tensor result ; return at::native::norm_out ( result , self , p , dim , keepdim , opt_dtype ) ; } } "
"void VImage::addImageMemoryBarrierToGeneral ( VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_GENERAL ) ; } ","void VImage::addImageMemoryBarrierToGeneral ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_GENERAL ) ; } "
"void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } ","void VImage::addImageMemoryBarrierToShaderRead ( const VkCommandBuffer commandBuffer ) const { addImageMemoryBarrier ( commandBuffer , VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL ) ; } "
"VkDescriptorSetLayoutBinding descriptorSetLayoutBinding ( uint32_t binding , VkDescriptorType descriptorType ) { return { binding , descriptorType , 1 , VK_SHADER_STAGE_COMPUTE_BIT , nullptr } ; } ","VkDescriptorSetLayoutBinding descriptorSetLayoutBinding ( const uint32_t binding , const VkDescriptorType descriptorType ) { return { binding , descriptorType , 1 , VK_SHADER_STAGE_COMPUTE_BIT , nullptr } ; } "
"ComputeUnit & ComputeUnitFactory::get ( const std::string & cacheKey , std::function < std::shared_ptr<ComputeUnit> ( ) > factoryFn ) { auto it = computeUnits_ . find ( cacheKey ) ; if ( it != computeUnits_ . end ( ) ) { return * ( it -> second . get ( ) ) ; } ","ComputeUnit & ComputeUnitFactory::get ( const std::string & cacheKey , const std::function < std::shared_ptr<ComputeUnit> ( ) > factoryFn ) { const auto it = computeUnits_ . find ( cacheKey ) ; if ( it != computeUnits_ . end ( ) ) { return * ( it -> second . get ( ) ) ; } "
"VImage * VulkanTensor::image ( c10::optional<ImageSizes> imageSizes ) { return impl ( ) -> image ( imageSizes ) ; } ","VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) { return impl ( ) -> image ( imageSizes ) ; } "
"const VImage * VulkanTensor::image ( c10::optional<ImageSizes> imageSizes ) const { return impl ( ) -> image ( imageSizes ) ; } ","const VImage * VulkanTensor::image ( const c10::optional<ImageSizes> imageSizes ) const { return impl ( ) -> image ( imageSizes ) ; } "
"if ( node -> kind ( ) == prim::CallMethod ) { Value * instance = node -> inputs ( ) [ 0 ] ; auto path = getModuleAccessPath ( instance , self ) ; auto child = findChildModule ( source , path ) ; auto qconfig = module_qconfig_map . at ( child . _ivalue ( ) ) ; instance -> setType ( type_remap_fn ( instance -> type ( ) , qconfig ) ) ; } ","if ( node -> kind ( ) == prim::CallMethod ) { Value * instance = node -> inputs ( ) [ 0 ] ; auto child_opt = getInvokedModuleOpt ( source , node , self ) ; if ( child_opt . has_value ( ) ) { auto qconfig = module_qconfig_map . at ( child_opt -> _ivalue ( ) ) ; instance -> setType ( type_remap_fn ( instance -> type ( ) , qconfig ) ) ; } } "
"if ( n -> kind ( ) == prim::CallMethod ) { invoked_methods . push_back ( std::make_pair ( getInvokedModule ( module , n , graph -> inputs ( ) [ 0 ] ) , n -> s ( attr::name ) ) ) ; } ","if ( n -> kind ( ) == prim::CallMethod ) { auto m_opt = getInvokedModuleOpt ( module , n , graph -> inputs ( ) [ 0 ] ) ; if ( m_opt . has_value ( ) ) { invoked_methods . push_back ( std::make_pair ( * m_opt , n -> s ( attr::name ) ) ) ; } } "
"PropagateRequiresGrad ( body ) ; new_body_outputs_require = fmap ( body -> return_node ( ) -> inputs ( ) . slice ( 1 ) , getRequiresGrad ) ; } while ( new_body_inputs_require != body_inputs_require & & new_body_outputs_require != body_outputs_require ) ; setRequiresGrad ( node , bitwiseOr ( body_outputs_require , loop_inputs_require ) ) ; ","PropagateRequiresGrad ( body ) ; new_body_outputs_require = fmap ( body -> return_node ( ) -> inputs ( ) . slice ( 1 ) , getRequiresGrad ) ; } while ( new_body_inputs_require != body_inputs_require | | new_body_outputs_require != body_outputs_require ) ; setRequiresGrad ( node , bitwiseOr ( body_outputs_require , loop_inputs_require ) ) ; "
"if ( -> dim ( ) ) { return nullptr ; } return * t -> dim ( ) == 0 t : t -> withDim ( * t -> dim ( ) + 1 ) ; ","if ( -> dim ( ) ) { return nullptr ; } return t -> withDim ( * t -> dim ( ) + 1 ) ; "
"if ( HPVariable_Check ( _new_state ) ) { throw TypeError ( ""expected a torch.ByteTensor, but got %s"" , Py_TYPE ( _new_state ) -> tp_name ) ; } ","if ( HPVariable_Check ( _new_state ) ) { throw torch::TypeError ( ""expected a torch.ByteTensor, but got %s"" , Py_TYPE ( _new_state ) -> tp_name ) ; } "
"( uint8_t * ) selfName . c_str ( ) + selfName . length ( ) ) ; addressStore_ -> set ( ""names/"" + c10::to_string ( selfId ) , selfNameVector ) ; workerIdToInfo_ . emplace ( selfId , WorkerInfo ( selfName , selfId ) ) ; ","( uint8_t * ) selfName . c_str ( ) + selfName . length ( ) ) ; rankToNameStore_ . set ( c10::to_string ( selfId ) , selfNameVector ) ; workerIdToInfo_ . emplace ( selfId , WorkerInfo ( selfName , selfId ) ) ; "
"std::vector<uint8_t> workerNameVector = addressStore_ -> get ( ""names/"" + c10::to_string ( workerId ) ) ; ","std::vector<uint8_t> workerNameVector = rankToNameStore_ . get ( c10::to_string ( workerId ) ) ; "
"e -> accept ( this ) ; if ( prec >= self_prec ) { os ( ) << ""("" ; } } ; withParens ( v -> ret_val1 ( ) ) ; ","e -> accept ( this ) ; if ( prec >= self_prec ) { os ( ) << "")"" ; } } ; withParens ( v -> ret_val1 ( ) ) ; "
"int64_t output_zero_point ) { return apply_impl<false> ( input , output_scale , output_zero_point ) ; } ","int64_t output_zero_point ) { return apply_impl<true> ( input , output_scale , output_zero_point ) ; } "
"if ( observed_values_ . count ( v ) ) { return ; } Module observer = observer_module . clone_instance ( ) ; std::string observer_name = ""_observer_"" + c10::to_string ( uid_ + + ) ; ","if ( observed_values_ . count ( v ) ) { return ; } Module observer = observer_module . deepcopy ( ) ; std::string observer_name = ""_observer_"" + c10::to_string ( uid_ + + ) ; "
"for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get < 1 > ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . clone_instance ( ) . _ivalue ( ) ) ; } ","for ( const auto & observer_attrs : block_observer_map_ . at ( block ) ) { const auto & name = std::get < 0 > ( observer_attrs ) ; const auto & observer = std::get < 1 > ( observer_attrs ) ; module . _ivalue ( ) -> setAttr ( name , observer . deepcopy ( ) . _ivalue ( ) ) ; } "
"while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + ""_"" + c10::to_string ( uid + + ) ; } parent_of_leaf . register_module ( child_name , child_module . clone_instance ( ) ) ; return child_name ; ","while ( parent_of_leaf . hasattr ( child_name ) ) { child_name = original_name + ""_"" + c10::to_string ( uid + + ) ; } parent_of_leaf . register_module ( child_name , child_module . deepcopy ( ) ) ; return child_name ; "
"for ( size_t i = 0 ; i < n -> outputs ( ) . size ( ) ; + + i ) { types . push_back ( n -> output ( 0 ) -> type ( ) ) ; } ","for ( size_t i = 0 ; i < n -> outputs ( ) . size ( ) ; + + i ) { types . push_back ( n -> output ( i ) -> type ( ) ) ; } "
"if ( FLAGS_caffe2_log_level < google::GLOG_ERROR ) { FLAGS_logtostderr = 1 ; } ","if ( FLAGS_caffe2_log_level < google::GLOG_WARNING ) { FLAGS_logtostderr = 1 ; } "
"if ( immediateEquals ( loops , 0 ) ) { return new Block ( { } ) ; } else if ( immediateEquals ( loops , 1 ) ) { return Substitute ( body_new , { { var_new , start_new } } ) ; } ","if ( immediateEquals ( loops , 0 ) ) { return new Block ( { } ) ; } else if ( immediateEquals ( loops , 1 ) ) { body_new = Substitute ( body , { { var_new , start_new } } ) ; body_new = body_new -> accept_mutator ( this ) ; return body_new ; } } "
"if ( FLAGS_caffe2_log_level < google::GLOG_WARNING ) { FLAGS_logtostderr = 1 ; } ","if ( FLAGS_caffe2_log_level < google::GLOG_ERROR ) { FLAGS_logtostderr = 1 ; } "
"for ( int64_t i = 0 ; i < n ; i + + ) { recursiveStore ( data , sizes , strides , dim + 1 , elementSize , seq [ i ] ) ; data += strides [ dim ] * elementSize ; } ","for ( int64_t i = 0 ; i < n ; i + + ) { recursiveStore ( data , sizes , strides , dim + 1 , tenElementSize , seq [ i ] ) ; data += strides [ dim ] * tenElementSize ; } "
"llvm::BasicBlock * bb_ ; llvm::Value * value_ ; llvm::JITTargetAddress kernelAddress_ ; ","llvm::BasicBlock * bb_ ; llvm::Value * value_ { nullptr } ; llvm::JITTargetAddress kernelAddress_ ; "
"AT_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; if ( yPoolWindow ) myPoolWindow . reset ( pool . newPoolWindow ( ) ) ; auto handle = myPoolWindow -> reserve ( device ) ; ","AT_CUDA_CHECK ( cudaGetDevice ( & device ) ) ; if ( yPoolWindow ) myPoolWindow . reset ( pool -> newPoolWindow ( ) ) ; auto handle = myPoolWindow -> reserve ( device ) ; "
"std::vector < const Expr * > indices ; for ( size_t i = 0 ; i < buf -> ndim ( ) ; i + + ) { indices . push_back ( this -> args_ [ i ] ) ; } ","std::vector < const Expr * > indices ; for ( int i = 0 ; i < buf -> ndim ( ) ; i + + ) { indices . push_back ( this -> args_ [ i ] ) ; } "
"std::vector < llvm::Type * > params ; for ( int i = 0 ; i < args . size ( ) ; i + + ) { auto const & arg = args [ i ] ; ","std::vector < llvm::Type * > params ; for ( size_t i = 0 ; i < args . size ( ) ; i + + ) { auto const & arg = args [ i ] ; "
"fntype , llvm::Function::PrivateLinkage , ""pytorch"" , module_ . get ( ) ) ; for ( int i = 0 ; i < args . size ( ) ; i + + ) { if ( rgs [ i ] . isVar ( ) ) { ","fntype , llvm::Function::PrivateLinkage , ""pytorch"" , module_ . get ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; i + + ) { if ( rgs [ i ] . isVar ( ) ) { "
"c10::intrusive_ptr<xnnpack::LinearOpContext> createLinearClampPrePackOpContext ( Tensor weight , c10::optional<Tensor> bias , c10::optional<double> output_min , c10::optional<double> output_max ) { return xnnpack::XNNPackLinearOpContext::create_context ( std::move ( weight ) , std::move ( bias ) , output_min , output_max ) ; } ","c10::intrusive_ptr<xnnpack::LinearOpContext> createLinearClampPrePackOpContext ( Tensor weight , c10::optional<Tensor> bias , c10::optional<Scalar> output_min , c10::optional<Scalar> output_max ) { return xnnpack::XNNPackLinearOpContext::create_context ( std::move ( weight ) , std::move ( bias ) , output_min , output_max ) ; } "
"auto x_nearest = x . round ( ) ; auto y_nearest = y . round ( ) ; auto i_x_nearest = convert_to_int_of_same_size<scalar_t> ( x_nearest ) ; auto i_y_nearest = convert_to_int_of_same_size<scalar_t> ( y_nearest ) ; ","auto x_nearest = x . round ( ) ; auto y_nearest = y . round ( ) ; auto i_x_nearest = convert_to_int_of_same_size ( x_nearest ) ; auto i_y_nearest = convert_to_int_of_same_size ( y_nearest ) ; "
"Tensor tensor ; if ( tensor_ . is_quantized ( ) ) { Tensor tensor = tensor_ . dequantize ( ) . to ( kCPU , kDouble ) . contiguous ( ) ; } else { tensor = tensor_ . to ( kCPU , kDouble ) . contiguous ( ) ; } ","Tensor tensor ; if ( tensor_ . is_quantized ( ) ) { tensor = tensor_ . dequantize ( ) . to ( kCPU , kDouble ) . contiguous ( ) ; } else { tensor = tensor_ . to ( kCPU , kDouble ) . contiguous ( ) ; } "
"TORCH_INTERNAL_ASSERT ( false , ""unrecognized message type "" , message . type ( ) ) ; } return 1 ; } ","TORCH_INTERNAL_ASSERT ( false , ""unrecognized message type "" , message . type ( ) ) ; } return true ; } "
"const int64_t size ( ) const { return cache_ . size ( ) ; } void clear ( ) { cache_ . clear ( ) ; } ","int64_t size ( ) const { return cache_ . size ( ) ; } void clear ( ) { cache_ . clear ( ) ; } "
"else { bound_ = true ; dims_ . resize ( size ) ; for ( ssize_t i = 0 ; i < size ; + + i ) { dims_ [ i ] = Dim::create ( py::unicode_from_format ( ""%S%i"" , name_ . ptr ( ) , ( int ) i ) ) ; } } ","else { bound_ = true ; dims_ . resize ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; + + i ) { dims_ [ i ] = Dim::create ( py::unicode_from_format ( ""%S%i"" , name_ . ptr ( ) , ( int ) i ) ) ; } } "
"py::sequence_view seq = sizes ; auto size = seq . size ( ) ; self -> bind_len ( size ) ; for ( ssize_t i = 0 ; i < size ; + + i ) { self -> dims_ [ i ] -> set_size ( py::to_int ( seq [ i ] ) ) ; } Py_RETURN_NONE ; ","py::sequence_view seq = sizes ; auto size = seq . size ( ) ; self -> bind_len ( size ) ; for ( Py_ssize_t i = 0 ; i < size ; + + i ) { self -> dims_ [ i ] -> set_size ( py::to_int ( seq [ i ] ) ) ; } Py_RETURN_NONE ; "
"if ( elf -> is_bound ( ) ) { py::raise_error ( DimensionBindError ( ) , ""DimList not bound"" ) ; } if ( idx >= self -> dims_ . size ( ) ) { py::raise_error ( PyExc_IndexError , ""index out of bounds"" ) ; } py::object r = self -> dims_ [ idx ] ; ","if ( elf -> is_bound ( ) ) { py::raise_error ( DimensionBindError ( ) , ""DimList not bound"" ) ; } if ( idx < 0 | | ( size_t ) idx >= self -> dims_ . size ( ) ) { py::raise_error ( PyExc_IndexError , ""index out of bounds"" ) ; } py::object r = self -> dims_ [ idx ] ; "
"PyMappingMethods DimList_mapping = { 0 , ( binaryfunc ) DimList_subscript , 0 , } ; ","PyMappingMethods DimList_mapping = { 0 , ( binaryfunc ) ( void * ) DimList_subscript , 0 , } ; "
"std::vector < py::obj<Dim> > dims ; size_t size = s . size ( ) ; dims . reserve ( size ) ; for ( ssize_t i = 0 ; i < size ; + + i ) { auto r = s [ i ] ; ","std::vector < py::obj<Dim> > dims ; size_t size = s . size ( ) ; dims . reserve ( size ) ; for ( size_t i = 0 ; i < size ; + + i ) { auto r = s [ i ] ; "
"if ( y::is_none ( size ) ) { d -> set_size ( py::to_int ( size ) ) ; } return d ; } py::object create_dimlist ( py::object name , py::handle size ) { ","if ( y::is_none ( size ) ) { d -> set_size ( py::to_int ( size ) ) ; } return std::move ( d ) ; } py::object create_dimlist ( py::object name , py::handle size ) { "
"const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / std::sqrt ( std::max ( rstd_val , T ( 0 ) ) + eps ) ; if ( gamma_null & & beta_null ) { T * Y_ptr = Y_data + i * inner_size ; ","const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = utils::RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / std::sqrt ( std::max ( rstd_val , T ( 0 ) ) + eps ) ; if ( gamma_null & & beta_null ) { T * Y_ptr = Y_data + i * inner_size ; "
"size_t idx = stack_ . back ( ) . toInt ( ) ; stack_ . pop_back ( ) ; globals_ . at ( idx ) ( ) ; } break ; case PickleOpCode::BINPERSID : { ","size_t idx = stack_ . back ( ) . toInt ( ) ; stack_ . pop_back ( ) ; TORCH_CHECK ( idx < globals_ . size ( ) , ""Parsing error: out of bounds access to globals_"" ) ; globals_ . at ( idx ) ( ) ; } break ; case PickleOpCode::BINPERSID : { "
"std::string op_schema_str { } ; const auto op_schema = fn . operator_schema ( ) ; if ( op_schema . has_value ( ) ) { op_schema_str = c10::toString ( op_schema . value ( ) ) ; } writeJsonNode ( ","std::string op_schema_str { } ; const auto op_schema = fn . operator_schema ( ) ; if ( op_schema . has_value ( ) ) { op_schema_str = json_str_escape ( c10::toString ( op_schema . value ( ) ) ) ; } writeJsonNode ( "
"const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = utils::RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / ","const T * X_ptr = X_data + i * inner_size ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , inner_size ) ; rstd_val = T ( 1 ) / "
"T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = utils::RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; ","T * Y_ptr = Y_data + i * N ; T mean_val ; T rstd_val ; std::tie ( mean_val , rstd_val ) = RowwiseMoments ( X_ptr , N ) ; rstd_val = T ( 1 ) / std::sqrt ( rstd_val + eps ) ; const T_ACC scale = rstd_val ; const T_ACC bias = - rstd_val * mean_val ; "
"else { const auto weight_data = weight . data_ptr<float> ( ) ; at::parallel_for ( 0 , embedding_rows , 1 , [ & ] ( int64_t start_idx , int64_t end_idx ) { fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float> ( ","else { const auto weight_data = weight_contig . data_ptr<float> ( ) ; at::parallel_for ( 0 , embedding_rows , 1 , [ & ] ( int64_t start_idx , int64_t end_idx ) { fbgemm::FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf<float> ( "
"if ( new_values . size ( 0 ) == 1 ) { return new_values [ 0 ] ; } else { return new_values . sum ( 0 ) ; } } else { auto dimIndices = ( arange ( ","if ( new_values . size ( 0 ) == 1 ) { return new_values [ 0 ] ; } else { return at::sum ( new_values , 0 , false , new_values . scalar_type ( ) ) ; } } else { auto dimIndices = ( arange ( "
"vTensor v_output { context , broadcast_first_input ( v_self , v_other ) v_other . sizes ( ) : v_self . sizes ( ) , v_self . options ( ) , } ; ","vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , v_self . options ( ) , } ; "
"vTensor v_output { context , broadcast_first_input ( v_self , v_other ) v_other . sizes ( ) : v_self . sizes ( ) , self . options ( ) . dtype ( c10::kQUInt8 ) , scale , zero_point } ; ","vTensor v_output { context , broadcast_size ( self_arg , other_arg ) , self . options ( ) . dtype ( c10::kQUInt8 ) , scale , zero_point } ; "
"if ( self . dim ( ) < 2 ) { channel_dim = 0 ; } auto self_ = self ; auto target_ = target . unsqueeze ( channel_dim ) ; auto grad_output_ = grad_output ; ","if ( self . dim ( ) < 2 ) { channel_dim = 0 ; } auto target_ = target . unsqueeze ( channel_dim ) ; auto grad_output_ = grad_output ; "
"Tensor weight_ ; if ( weight & & weight -> defined ( ) ) { auto shape = weight -> sizes ( ) ; VmapDimVector new_shape ( self_ . dim ( ) , 1 ) ; new_shape [ channel_dim ] = shape [ 0 ] ; } ","Tensor weight_ ; if ( weight & & weight -> defined ( ) ) { auto self_ = self ; auto shape = weight -> sizes ( ) ; VmapDimVector new_shape ( self_ . dim ( ) , 1 ) ; new_shape [ channel_dim ] = shape [ 0 ] ; } "
"if ( s_input ) { set_history ( tensor , grad_fn ) ; } grad_fn -> saved_tensors_ . push_back ( torch::autograd::SavedVariable ( tensor , s_input ) ) ; } } torch::jit::push ( stack , result ) ; ","if ( s_input ) { set_history ( tensor , grad_fn ) ; } grad_fn -> saved_tensors_ . emplace_back ( tensor , s_input ) ; } } torch::jit::push ( stack , result ) ; "
"void index_put__batch_rule ( Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) ","void index_put__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) "
"void _index_put_impl__batch_rule ( Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) ","void _index_put_impl__batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ) "
"std::tuple < Tensor , optional<int64_t> > index_put_batch_rule ( Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , ","std::tuple < Tensor , optional<int64_t> > index_put_batch_rule ( const Tensor & self , optional<int64_t> self_bdim , ArrayRef < optional<Tensor> > indices , ArrayRef < optional<int64_t> > indices_bdims , "
"for ( unsigned idx = 0 ; idx < grads . size ( ) ; idx + + ) { auto ivalue = torch::jit::toIValue ( PyTuple_GetItem ( out . ptr ( ) , idx ) , TensorType::get ( ) ) ; result . push_back ( ivalue . toTensor ( ) ) ; } return result ; } ","for ( unsigned idx = 0 ; idx < grads . size ( ) ; idx + + ) { auto ivalue = torch::jit::toIValue ( PyTuple_GetItem ( out . ptr ( ) , idx ) , TensorType::get ( ) ) ; result . emplace_back ( ivalue . toTensor ( ) ) ; } return result ; } "
"variable_list grad_inputs ( num_inputs_ ) ; std::vector<Tensor> args ; for ( const auto & g : grads ) { args . push_back ( g ) ; } for ( const auto & saved : saved_tensors_ ) { args . push_back ( saved . unpack ( shared_from_this ( ) ) ) ; } if ( should_compute_output ( { tensors_ix } ) ) { ","variable_list grad_inputs ( num_inputs_ ) ; std::vector<Tensor> args ; for ( auto & g : grads ) { args . emplace_back ( std::move ( g ) ) ; } for ( const auto & saved : saved_tensors_ ) { args . emplace_back ( saved . unpack ( shared_from_this ( ) ) ) ; } if ( should_compute_output ( { tensors_ix } ) ) { "
"std::vector<Tensor> makeBatchedVector ( const std::vector<Tensor> & tensors , optional<int64_t> bdim , int64_t level ) { std::vector<Tensor> res ; for ( size_t idx = 0 ; idx < tensors . size ( ) ; idx + + ) { res . push_back ( makeBatched ( tensors [ idx ] , bdim , level ) ) ; } return res ; } ","std::vector<Tensor> makeBatchedVector ( const std::vector<Tensor> & tensors , optional<int64_t> bdim , int64_t level ) { std::vector<Tensor> res ; for ( size_t idx = 0 ; idx < tensors . size ( ) ; idx + + ) { res . emplace_back ( makeBatched ( tensors [ idx ] , bdim , level ) ) ; } return res ; } "
"static void checkForInvalidMutationOnCaptures ( const c10::OperatorHandle & op , torch::jit::Stack * stack , int64_t cur_level ) { if ( sInplaceOp ( op . schema ( ) ) ) { return ; ","static void checkForInvalidMutationOnCaptures ( const c10::OperatorHandle & op , const torch::jit::Stack * stack , int64_t cur_level ) { if ( sInplaceOp ( op . schema ( ) ) ) { return ; "
"if ( wrapped -> is_alive ( ) ) { ss << ""Wrapper["" ; } else { ss << ""Wrapper["" ; } if ( wrapped -> level ( ) . has_value ( ) ) { ss << wrapped -> level ( ) . value ( ) << "", "" ; } else { ss << ""dead, "" ; } ","ss << ""Wrapper["" ; if ( wrapped -> level ( ) . has_value ( ) ) { ss << ""lvl="" << wrapped -> level ( ) . value ( ) << "", "" ; } else { ss << ""dead, "" ; } "
"std::vector<int> dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { std::vector<int> hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , ","hash_key_t dynamic_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 0 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , "
"std::vector<int> static_hasher ( const LocalState & state , const at::Tensor & v ) { std::vector<int> hash = { 1 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , ","hash_key_t static_hasher ( const LocalState & state , const at::Tensor & v ) { hash_key_t hash = { 1 , static_cast<int> ( packFlags ( state , v ) ) , static_cast<int> ( state . apply ( v . key_set ( ) ) . raw_repr ( ) ) , "
"public : std::size_t operator ( ) ( std::vector<int> const & vec ) const { std::size_t seed = vec . size ( ) ; for ( auto & i : vec ) { seed ^= i + 0x9e3779b9 + ( seed << 6 ) + ( seed >> 2 ) ; ","public : std::size_t operator ( ) ( hash_key_t const & vec ) const { std::size_t seed = vec . size ( ) ; for ( auto & i : vec ) { seed ^= i + 0x9e3779b9 + ( seed << 6 ) + ( seed >> 2 ) ; "
"std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; std::vector<int> cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; ","std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; auto item = cache_ . find ( cacheKey ) ; "
"const py::object & compileFn , PyObject * args ) { std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; LocalState state ; std::vector<int> cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; cache_ . emplace ( cacheKey , compileFn ) ; } ","const py::object & compileFn , PyObject * args ) { std::vector<at::Tensor> tensorArgs = parsePythonArgs ( numArgs , args ) ; LocalState state ; hash_key_t cacheKey = computeCacheKey ( tensorArgs , numArgs , hasherType , id ) ; cache_ . emplace ( cacheKey , compileFn ) ; } "
"VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return std::make_tuple ( self_ . repeat ( sizes_with_bdim ) , 0 ) ; ","VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < ( int64_t ) sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return std::make_tuple ( self_ . repeat ( sizes_with_bdim ) , 0 ) ; "
"const auto & argument = ( * stack ) [ arguments_begin + arg_idx ] ; if ( batched_tensor_inputs_pos_iter == batched_tensor_inputs_position . end ( ) | | arg_idx != * batched_tensor_inputs_pos_iter ) { torch::jit::push ( stack , argument ) ; continue ; ","const auto & argument = ( * stack ) [ arguments_begin + arg_idx ] ; if ( batched_tensor_inputs_pos_iter == batched_tensor_inputs_position . end ( ) | | ( int64_t ) arg_idx != * batched_tensor_inputs_pos_iter ) { torch::jit::push ( stack , argument ) ; continue ; "
"dims = { arguments [ dim_arg_pos ] . toInt ( ) } ; } else if ( arguments [ dim_arg_pos ] . isNone ( ) ) { reduction_case = ReductionCase::DimArray ; auto all_dims = range ( 0 , self . dim ( ) - 1 ) ; dims = std::vector<int64_t> ( all_dims . begin ( ) , all_dims . end ( ) ) ; } else { TORCH_INTERNAL_ASSERT ( false , ""Unexpected dtype found at dims"" ) ; ","dims = { arguments [ dim_arg_pos ] . toInt ( ) } ; } else if ( arguments [ dim_arg_pos ] . isNone ( ) ) { reduction_case = ReductionCase::DimArray ; if ( logical_dim == 0 ) { dims = { 0 } ; } else { auto all_dims = range ( 0 , self . dim ( ) - 1 ) ; dims = std::vector<int64_t> ( all_dims . begin ( ) , all_dims . end ( ) ) ; } } else { TORCH_INTERNAL_ASSERT ( false , ""Unexpected dtype found at dims"" ) ; "
"return { self . repeat ( sizes ) , nullopt } ; } auto self_ = moveBatchDimToFront ( self , self_bdim ) ; VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; return { self_ . repeat ( sizes_with_bdim ) , 0 } ; } ","return { self . repeat ( sizes ) , nullopt } ; } VmapDimVector sizes_with_bdim = { sizes . begin ( ) , sizes . end ( ) } ; sizes_with_bdim . insert ( sizes_with_bdim . begin ( ) , 1 ) ; auto self_ = moveBatchDimToFront ( self , self_bdim ) ; while ( self_ . dim ( ) < sizes_with_bdim . size ( ) ) { self_ = self_ . unsqueeze ( 1 ) ; } return { self_ . repeat ( sizes_with_bdim ) , 0 } ; } "
"static VmapDimVector range ( int64_t start , int64_t stop ) { TORCH_INTERNAL_ASSERT ( stop > start ) ; VmapDimVector dims ; dims . reserve ( stop - start ) ; ","static VmapDimVector range ( int64_t start , int64_t stop ) { TORCH_INTERNAL_ASSERT ( stop >= start ) ; VmapDimVector dims ; dims . reserve ( stop - start ) ; "
"if ( aybe_tensor_wrapper ) { continue ; } maybe_tensor_wrapper -> refreshSizesAndStrides ( ) ; } ","if ( aybe_tensor_wrapper ) { continue ; } maybe_tensor_wrapper -> refreshMetadata ( ) ; } "
"TORCH_INTERNAL_ASSERT ( use_value_sizes_strides ) ; refreshSizesAndStrides ( ) ; } ","TORCH_INTERNAL_ASSERT ( use_value_sizes_strides ) ; refreshMetadata ( ) ; } "
". getElementType ( ) ) ; py::list pyL ; for ( int jdx = 0 ; jdx < l . size ( ) ; jdx + + ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) & & isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; ",". getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx + + ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) & & isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; "
". getElementType ( ) ) ; py::list pyL ; for ( int jdx = 0 ; jdx < l . size ( ) ; jdx + + ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) & & isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; ",". getElementType ( ) ) ; py::list pyL ; for ( unsigned jdx = 0 ; jdx < l . size ( ) ; jdx + + ) { auto nv = l . get ( jdx ) ; if ( nv . isTensor ( ) & & isPythonTensor ( nv . toTensor ( ) ) ) { auto pyTensor = getPythonImpl ( nv . toTensor ( ) ) ; "
"auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim & & B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } ","auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim & & B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } else if ( _bdim & & _bdim ) { return { at::dot ( A_ , B_ ) , nullopt } ; } else { return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } } "
"std::tuple < Tensor , optional<int64_t> > dot_batch_rule ( const Tensor & A , optional<int64_t> A_bdim , const Tensor & B , optional<int64_t> B_bdim ) { auto A_ = moveBatchDimToFront ( A , A_bdim ) ; auto B_ = moveBatchDimToFront ( B , B_bdim ) ; return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } ","std::tuple < Tensor , optional<int64_t> > dot_batch_rule ( const Tensor & A , optional<int64_t> A_bdim , const Tensor & B , optional<int64_t> B_bdim ) { auto A_ = moveBatchDimToFront ( A , A_bdim ) ; auto B_ = moveBatchDimToFront ( B , B_bdim ) ; if ( A_bdim & & B_bdim ) { return { at::matmul ( A_ . unsqueeze ( -2 ) , B_ . unsqueeze ( -1 ) ) . squeeze ( -1 ) . squeeze ( -1 ) , 0 } ; } return { at::matmul ( A_ , B_ . t ( ) ) , 0 } ; } "
"int64_t dim ) { auto self_ = moveBatchDimToFront ( self , self_bdim ) ; auto rank = rankWithoutBatchDim ( self , self_bdim ) ; dim = maybe_wrap_dim ( dim , rank + 1 ) + 1 ; return { self . unsqueeze ( dim ) , valIfNonempty ( self_bdim , 0 ) } ; } TORCH_LIBRARY_IMPL ( aten , FT_BATCHED_KEY , m ) { ","int64_t dim ) { auto self_ = moveBatchDimToFront ( self , self_bdim ) ; auto rank = rankWithoutBatchDim ( self , self_bdim ) ; dim = maybe_wrap_dim ( dim , rank + 1 ) ; if ( self_bdim ) { dim += 1 ; } return { self_ . unsqueeze ( dim ) , valIfNonempty ( self_bdim , 0 ) } ; } TORCH_LIBRARY_IMPL ( aten , FT_BATCHED_KEY , m ) { "
"if ( tensor . is_cuda ( ) ) { key_set = key_set . add ( DispatchKey::CUDA ) ; } return at::detail::make_tensor<BatchedTensorImpl> ( key_set , tensor , std::move ( bdims ) ) ; } ","if ( tensor . is_cuda ( ) ) { key_set = key_set . add ( DispatchKey::CUDA ) ; } auto * batched = maybeGetBatchedImpl ( tensor ) ; if ( batched ) { auto requested_level = bdims . back ( ) . level ( ) ; auto batched_level = batched -> bdims ( ) . back ( ) . level ( ) ; TORCH_INTERNAL_ASSERT ( requested_level > batched_level ) ; } return at::detail::make_tensor<BatchedTensorImpl> ( key_set , tensor , std::move ( bdims ) ) ; } "
"auto bdims = batched -> bdims ( ) ; auto * it = std::find_if ( bdims . begin ( ) , bdims . end ( ) , [ & ] ( const BatchDim & bdim ) { return bdim . level ( ) == level ; } ) ; return it != bdims . end ( ) ; } Tensor _add_batch_dim ( const Tensor & self , int64_t batch_dim , int64_t level ) { ","auto bdims = batched -> bdims ( ) ; return bdims . back ( ) . level ( ) >= level ; } Tensor _add_batch_dim ( const Tensor & self , int64_t batch_dim , int64_t level ) { "
"auto self_sizes = self . sizes ( ) ; VmapDimVector expanded_sizes ( self_sizes . begin ( ) , self_sizes . end ( ) ) ; expanded_sizes . insert ( expanded_sizes . begin ( ) + out_dim , batch_size ) ; return self . expand ( expanded_sizes ) ; } ","auto self_sizes = self . sizes ( ) ; VmapDimVector expanded_sizes ( self_sizes . begin ( ) , self_sizes . end ( ) ) ; expanded_sizes . insert ( expanded_sizes . begin ( ) + out_dim , batch_size ) ; auto result = self . expand ( expanded_sizes ) ; return result ; } "
"Tensor self_without_bdim ; int64_t newly_exposed_logical_dim ; std::tie ( self_without_bdim , newly_exposed_logical_dim ) = remove_existing_batch_dim ( batched , level ) ; return _movedim ( self_without_bdim , newly_exposed_logical_dim , out_dim ) ; } Tensor _wrap_for_grad ( const Tensor & self , int64_t level ) { ","Tensor self_without_bdim ; int64_t newly_exposed_logical_dim ; std::tie ( self_without_bdim , newly_exposed_logical_dim ) = remove_existing_batch_dim ( batched , level ) ; auto result = _movedim ( self_without_bdim , newly_exposed_logical_dim , out_dim ) ; return result ; } Tensor _wrap_for_grad ( const Tensor & self , int64_t level ) { "
"if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) at::kDouble : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( ","if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) default_dtype : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( "
"if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) at::kDouble : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( ","if ( aybe_dtype_option ) return { } ; auto dtype = ( maybe_dtype_option -> isNone ( ) default_dtype : maybe_dtype_option -> toScalarType ( ) ) ; return { TensorType::create ( "
"scalar_t * A_working_ptr = & A_data [ i * A_mat_stride ] ; scalar_t * b_working_ptr = & b_data [ i * b_mat_stride ] ; lapackCholeskySolve<scalar_t> ( uplo , n , nrhs , A_working_ptr , ldab , b_working_ptr , ldab , & info ) ; infos [ i ] = info ; if ( info != 0 ) { return ; } ","scalar_t * A_working_ptr = & A_data [ i * A_mat_stride ] ; scalar_t * b_working_ptr = & b_data [ i * b_mat_stride ] ; lapackCholeskySolve<scalar_t> ( uplo , n , nrhs , A_working_ptr , ldab , b_working_ptr , ldab , & info ) ; infos_data [ i ] = info ; if ( info != 0 ) { return ; } "
"std::tuple < Tensor , Tensor > _symeig_helper_cpu ( const Tensor & self , bool eigenvectors , bool upper ) { std::vector<int64_t> infos ( batchCount ( self ) , 0 ) ; auto self_sizes = self . sizes ( ) . vec ( ) ; self_sizes . pop_back ( ) ; ","std::tuple < Tensor , Tensor > _symeig_helper_cpu ( const Tensor & self , bool eigenvectors , bool upper ) { auto infos = at::zeros ( { batchCount ( self ) } , self . options ( ) . dtype ( kInt ) ) ; auto self_sizes = self . sizes ( ) . vec ( ) ; self_sizes . pop_back ( ) ; "
"} template < typename scalar_t > void apply_eig ( const Tensor & self , bool eigenvectors , Tensor & vals_ , Tensor & vecs_ , int64_t * info_ptr ) { if T_BUILD_WITH_LAPACK ( ) TORCH_CHECK ( false , ""Calling torch.eig on a CPU tensor requires compiling "" , ""PyTorch with LAPACK. Please use PyTorch built with LAPACK support."" ) ; } ","template < typename scalar_t > void apply_eig ( const Tensor & self , bool eigenvectors , Tensor & vals_ , Tensor & vecs_ , int * info_ptr ) { if T_BUILD_WITH_LAPACK ( ) TORCH_CHECK ( false , ""Calling torch.eig on a CPU tensor requires compiling "" , ""PyTorch with LAPACK. Please use PyTorch built with LAPACK support."" ) ; } "
"char * other_data = data [ 1 ] ; for ( const auto i : c10::irange ( dim_size ) ) { ( void ) i ; if ( * ( ( scalar_t * ) self_data ) != * ( ( scalar_t * ) other_data ) ) { result = false ; return ; } ","char * other_data = data [ 1 ] ; for ( const auto i : c10::irange ( dim_size ) ) { ( void ) i ; if ( c10::load<scalar_t> ( self_data ) != c10::load<scalar_t> ( other_data ) ) { result = false ; return ; } "
"VaryingShape<size_t> stride_indices ; VaryingShape<int64_t> strides ; VaryingShape<int64_t> sizes ; if ( t . layout ( ) == at::kStrided ) { sizes = VaryingShape<int64_t> { t . sizes ( ) . vec ( ) } ; strides = VaryingShape<int64_t> { t . strides ( ) . vec ( ) } ; } ","VaryingShape<size_t> stride_indices ; VaryingShape<int64_t> strides ; VaryingShape<int64_t> sizes ; if ( t . layout ( ) == at::kStrided & & . is_nested ( ) ) { sizes = VaryingShape<int64_t> { t . sizes ( ) . vec ( ) } ; strides = VaryingShape<int64_t> { t . strides ( ) . vec ( ) } ; } "
"bool insertableTensor ( const at::Tensor & ten ) { return en . requires_grad ( ) & & ten . has_storage ( ) ; } ","bool insertableTensor ( const at::Tensor & ten ) { return en . requires_grad ( ) & & ten . has_storage ( ) & & en . is_nested ( ) ; } "
"if ( lhs . is_mkldnn ( ) | | rhs . is_mkldnn ( ) ) { return false ; } if ( lhs . device ( ) != rhs . device ( ) ) { return false ; } ","if ( lhs . is_mkldnn ( ) | | rhs . is_mkldnn ( ) ) { return false ; } if ( lhs . is_nested ( ) | | rhs . is_nested ( ) ) { return false ; } if ( lhs . device ( ) != rhs . device ( ) ) { return false ; } "
"annotations_ . emplace_back ( ""Python id"" , std::to_string ( t . id_ ) ) ; annotations_ . emplace_back ( ""Python parent id"" , y_metadata_ . empty ( ) py_metadata_ . at ( 0 ) . name_ : ""null"" ) ; annotations_ . emplace_back ( ""Python thread"" , std::to_string ( t . python_tid_ ) ) ; } ","annotations_ . emplace_back ( ""Python id"" , std::to_string ( t . id_ ) ) ; annotations_ . emplace_back ( ""Python parent id"" , y_metadata_ . empty ( ) std::to_string ( py_metadata_ . at ( 0 ) . id_ ) : ""null"" ) ; annotations_ . emplace_back ( ""Python thread"" , std::to_string ( t . python_tid_ ) ) ; } "
"at::from_blob ( data , c10::IntArrayRef ( shapes ) , c10::IntArrayRef ( strides ) , at::kChar ) . to ( at::kCPU ) ; auto options = c10::TensorOptions ( ) . dtype ( at::kChar ) . device ( at::kCPU ) ; data_node -> t_ ( Symbol::attr ( ""value"" ) , data_value . clone ( ) ) ; ","at::from_blob ( data , c10::IntArrayRef ( shapes ) , c10::IntArrayRef ( strides ) , at::kChar ) . to ( at::kCPU ) ; data_node -> t_ ( Symbol::attr ( ""value"" ) , data_value . clone ( ) ) ; "
"auto input = emitSugaredExpr ( apply . inputs ( ) [ 0 ] , 1 ) -> asValue ( loc , method ) ; return std::make_shared<SimpleValue> ( emitIndex ( loc , self , { input } ) ) ; } default : ","auto input = emitSugaredExpr ( apply . inputs ( ) [ 0 ] , 1 ) -> asValue ( loc , method ) ; if ( input -> type ( ) -> kind ( ) == TypeKind::TupleType ) { return std::make_shared<SimpleValue> ( emitIndex ( loc , self , createTupleUnpack ( input ) ) ) ; } return std::make_shared<SimpleValue> ( emitIndex ( loc , self , { input } ) ) ; } default : "
"static PyObject * THPStorage_elementSize ( PyObject * _self , PyObject * noargs ) { HANDLE_TH_ERRORS auto self = ( THPStorage * ) _self ; return THPUtils_packInt64 ( sizeof ( uint8_t ) ) ; END_HANDLE_TH_ERRORS } ","static PyObject * THPStorage_elementSize ( PyObject * _self , PyObject * noargs ) { HANDLE_TH_ERRORS return THPUtils_packInt64 ( sizeof ( uint8_t ) ) ; END_HANDLE_TH_ERRORS } "
"HANDLE_TH_ERRORS TORCH_CHECK ( PyTuple_Size ( args ) == 2 , ""_new_with_file takes exactly two arguments"" ) ; PyObject * fd_obj = PyTuple_GetItem ( args , 0 ) ; int fd = PyObject_AsFileDescriptor ( PyTuple_GetItem ( args , 0 ) ) ; THPUtils_assert ( fd != -1 , ""_new_with_file couldn't retrieve a file "" ""descriptor from given object"" ) ; ","HANDLE_TH_ERRORS TORCH_CHECK ( PyTuple_Size ( args ) == 2 , ""_new_with_file takes exactly two arguments"" ) ; int fd = PyObject_AsFileDescriptor ( PyTuple_GetItem ( args , 0 ) ) ; THPUtils_assert ( fd != -1 , ""_new_with_file couldn't retrieve a file "" ""descriptor from given object"" ) ; "
"auto list_node = n -> input ( 0 ) -> node ( ) ; auto seq_node = n -> input ( 0 ) -> node ( ) ; auto t_type = n -> input ( 1 ) -> type ( ) -> cast<TensorType> ( ) ; ","auto seq_node = n -> input ( 0 ) -> node ( ) ; auto t_type = n -> input ( 1 ) -> type ( ) -> cast<TensorType> ( ) ; "
"""schema"" , [ ] ( Node & n ) { std::stringstream ss ; if ( auto sch = n . maybeSchema ( ) ) { ss << n . schema ( ) ; } else { ss << ""(no schema)"" ; ","""schema"" , [ ] ( Node & n ) { std::stringstream ss ; if ( n . maybeSchema ( ) ) { ss << n . schema ( ) ; } else { ss << ""(no schema)"" ; "
"} void print_init_message ( const char * message ) { size_t unused ; unused = write ( 1 , message , strlen ( message ) ) ; unused = write ( 1 , ""n"" , 1 ) ; } bool object_exists ( const char * name ) { ","} void print_init_message ( const char * message ) { write ( 1 , message , strlen ( message ) ) ; write ( 1 , ""n"" , 1 ) ; } bool object_exists ( const char * name ) { "
"cpu_kernel ( iter , [ ] ( scalar_t a , scalar_t b ) -> scalar_t { TORCH_CHECK ( b != 0 , ""ZeroDivisionError"" ) ; scalar_t r = a % b ; if ( ( r != 0 ) & & ( ( r < 0 ) != ( b < 0 ) ) ) { r += b ; } return r ; ","cpu_kernel ( iter , [ ] ( scalar_t a , scalar_t b ) -> scalar_t { TORCH_CHECK ( b != 0 , ""ZeroDivisionError"" ) ; scalar_t r = a % b ; if ( ( r != 0 ) & & ( c10::is_negative ( r ) != c10::is_negative ( b ) ) ) { r += b ; } return r ; "
"const std::vector < IterDomain * > & new_root_domain , std::vector < IterDomain * > & rfactor_domain ) override { TORCH_INTERNAL_ASSERT ( index_ >= 0 & & ( index_ + 1 ) < new_root_domain . size ( ) , ""Index: t"" , index_ , ""t Domain Size:t"" , ","const std::vector < IterDomain * > & new_root_domain , std::vector < IterDomain * > & rfactor_domain ) override { TORCH_INTERNAL_ASSERT ( ( index_ + 1 ) < new_root_domain . size ( ) , ""Index: t"" , index_ , ""t Domain Size:t"" , "
"case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; if ( buffers . size ( ) < 0 ) { break ; } message = ""compressAllBuffers(l.root_stmt());n"" ; randomization_helper::printHistory ( n_transform , message ) ; ","case COMPRESS_ALL_BUFFERS : { auto buffers = BufFinder::find ( l . root_stmt ( ) ) ; message = ""compressAllBuffers(l.root_stmt());n"" ; randomization_helper::printHistory ( n_transform , message ) ; "
"if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } ","if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; break ; } else { code_ . operators_ [ i ] = * func ; } "
"if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; break ; } else { code_ . operators_ [ i ] = * func ; } ","if ( unc . has_value ( ) ) { unsupported_op_names . insert ( operator_str ( opname ) ) ; all_ops_supported = false ; } else { code_ . operators_ [ i ] = * func ; } "
"variable_list variableIndices ; int64_t specified_dims = count_specified_dimensions ( holder . get ( ) ) ; if ( specified_dims == -1 ) { return handle_torch_function_indexing ( self , index ) ; } Variable sliced = applySlicing ( self_ , holder . get ( ) , variableIndices , is_tracing , self_ . device ( ) , self_ . sizes ( ) , specified_dims ) ; ","variable_list variableIndices ; int64_t specified_dims = count_specified_dimensions ( holder . get ( ) ) ; if ( specified_dims == -1 ) { return handle_torch_function_indexing ( self , holder . get ( ) ) ; } Variable sliced = applySlicing ( self_ , holder . get ( ) , variableIndices , is_tracing , self_ . device ( ) , self_ . sizes ( ) , specified_dims ) ; "
"const Tensor & info , const c10::string_view api_name , bool is_matrix ) { if ( is_matrix ) { singleCheckErrors ( info . item<int64_t> ( ) , api_name ) ; } else { ","const Tensor & info , const c10::string_view api_name , bool is_matrix ) { if ( info . is_meta ( ) ) { return ; } if ( is_matrix ) { singleCheckErrors ( info . item<int64_t> ( ) , api_name ) ; } else { "
"at::ScalarType bias_dtype = bn_rm . scalar_type ( ) ; at::ScalarType weight_dtype = conv_w . scalar_type ( ) ; at::DeviceType weight_device = conv_w . device ( ) . type ( ) ; if ( weight_device == at::kCUDA & & ( weight_dtype == at::kHalf | | weight_dtype == at::kBFloat16 ) & & bias_dtype == at::kFloat ) { bias_dtype = weight_dtype ; } ","at::ScalarType bias_dtype = bn_rm . scalar_type ( ) ; at::ScalarType weight_dtype = conv_w . scalar_type ( ) ; if ( ( weight_dtype == at::kHalf | | weight_dtype == at::kBFloat16 ) & & bias_dtype == at::kFloat ) { bias_dtype = weight_dtype ; } "
"if ( is_up_to_date ( ) ) { return ; } auto any_updates = apply_updates ( ) ; if ( any_updates ) { regenerate_from_base ( ) ; } } void FunctionalTensorWrapper::regenerate_from_base ( ) { ","if ( is_up_to_date ( ) ) { return ; } apply_updates ( ) ; regenerate_from_base ( ) ; } void FunctionalTensorWrapper::regenerate_from_base ( ) { "
"Tensor d = at::full_like ( sizes , D ) ; sizes = at::cat ( { sizes , d } , 1 ) ; return at::_nested_from_padded ( t , sizes , false ) ; } ","Tensor d = at::full_like ( sizes , D ) ; sizes = at::cat ( { sizes , d } , 1 ) . to ( kCPU ) ; return at::_nested_from_padded ( t , sizes , false ) ; } "
"} alias_info = std::move ( container ) ; } else if ( L . nextIf ( '?' ) ) { fake_value = c10::TypeFactory::create<c10::OptionalType> ( fake_value ) ; real_value = c10::TypeFactory::create<c10::OptionalType> ( real_value ) ; } else { break ; } ","} alias_info = std::move ( container ) ; } else if ( L . nextIf ( '?' ) ) { fake_value = c10::OptionalType::get ( fake_value ) ; real_value = c10::OptionalType::get ( real_value ) ; } else { break ; } "
"if ( result . device ( ) == kMeta ) { auto selfSlice = result . select ( dim , 0 ) ; auto sourceSlice = source . select ( dim , 0 ) ; auto iter = TensorIterator::borrowing_binary_op ( selfSlice , selfSlice , sourceSlice ) ; ","if ( result . device ( ) == kMeta & & result . dim ( ) > 0 ) { auto selfSlice = result . select ( dim , 0 ) ; auto sourceSlice = source . select ( dim , 0 ) ; auto iter = TensorIterator::borrowing_binary_op ( selfSlice , selfSlice , sourceSlice ) ; "
"static inline Tensor unary_op_impl_with_complex_to_float ( const Tensor & self , OutImpl & out_impl ) { if ( self . is_complex ( ) ) { const auto float_type = c10::toRealValueType ( self . scalar_type ( ) ) ; Tensor result = at::empty ( { 0 } , self . options ( ) . dtype ( float_type ) ) ; return out_impl ( result , self ) ; } ","static inline Tensor unary_op_impl_with_complex_to_float ( const Tensor & self , OutImpl & out_impl ) { if ( self . is_complex ( ) ) { const auto float_type = c10::toRealValueType ( self . scalar_type ( ) ) ; Tensor result = at::empty_like ( self , self . options ( ) . dtype ( float_type ) ) ; return out_impl ( result , self ) ; } "
"} else if ( strides [ a ] > strides [ b ] ) { return 1 ; } else { if ( sizes [ a ] < sizes [ b ] | | a > b ) { return 1 ; } } ","} else if ( strides [ a ] > strides [ b ] ) { return 1 ; } else { if ( sizes [ a ] > sizes [ b ] ) { return 1 ; } } "
"case ConvBackend::SlowTranspose3d : { input = input . contiguous ( backend_memory_format ) ; if ( params . groups == 1 ) { std::tie ( backend_grad_input , backend_grad_weight , backend_grad_bias ) = _convolution_backward_nogroup_backend ( ","case ConvBackend::SlowTranspose3d : { input = input . contiguous ( backend_memory_format ) ; weight = weight . contiguous ( backend_memory_format ) ; if ( params . groups == 1 ) { std::tie ( backend_grad_input , backend_grad_weight , backend_grad_bias ) = _convolution_backward_nogroup_backend ( "
"if ( min & & max ) { clamp_stub ( device_type ( ) , * this ) ; } else if ( min ) { clamp_min_stub ( device_type ( ) , * this ) ; } else if ( max ) { clamp_max_stub ( device_type ( ) , * this ) ; } } ","if ( min & & max ) { clamp_stub ( device_type ( ) , * this ) ; } else if ( min ) { maximum_stub ( device_type ( ) , * this ) ; } else if ( max ) { minimum_stub ( device_type ( ) , * this ) ; } } "
"if ( batch_mode ) { AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , ""replication_pad3d_cpu"" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; auto nbatch = input . size ( 0 ) ; ","if ( batch_mode ) { AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , ""reflection_pad3d_cpu"" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; auto nbatch = input . size ( 0 ) ; "
"} ) ; } else { AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , ""replication_pad3d_cpu"" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( ","} ) ; } else { AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND1 ( kHalf , input . scalar_type ( ) , ""reflection_pad3d_cpu"" , [ & ] { auto input_data = input . data_ptr<scalar_t> ( ) ; auto output_data = output . data_ptr<scalar_t> ( ) ; reflection_pad3d_out_frame ( "
"public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; ","public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; "
"public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; ","public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; "
"public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; ","public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; "
"public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; ","public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::max ( * self_data , * src_data ) ; } } ; static ReduceMaximum reduce_maximum ; "
"public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; ","public : template < typename scalar_t > constexpr void operator ( ) ( scalar_t * self_data , scalar_t * src_data ) const { * self_data = at::_isnan<scalar_t> ( * src_data ) * src_data : std::min ( * self_data , * src_data ) ; } } ; static ReduceMinimum reduce_minimum ; "
"std::string BackendDevice::toString ( ) const { TORCH_INTERNAL_ASSERT ( type_ ) ; std::string str = type_ -> toString ( ) ; if ( has_index ( ) ) { str . append ( std::to_string ( ordinal_ ) ) ; } return str ; } int BackendDevice::compare ( const BackendDevice & rhs ) const { ","std::string BackendDevice::toString ( ) const { TORCH_INTERNAL_ASSERT ( type_ ) ; return c10::str ( type_ -> toString ( ) , ordinal_ ) ; } int BackendDevice::compare ( const BackendDevice & rhs ) const { "
"if ( bj ) throw python_error ( ) ; PyList_SET_ITEM ( list . get ( ) , i , obj ) ; auto advance_data_ptr = strides [ dim ] * elementSize ; TORCH_INTERNAL_ASSERT ( data | | advance_data_ptr == 0 ) ; data += advance_data_ptr ; } return list . release ( ) ; ","if ( bj ) throw python_error ( ) ; PyList_SET_ITEM ( list . get ( ) , i , obj ) ; auto advance_data_ptr = strides [ dim ] * elementSize ; TORCH_INTERNAL_ASSERT ( data | | ( advance_data_ptr == 0 ) ) ; data += advance_data_ptr ; } return list . release ( ) ; "
"TORCH_CHECK ( tensor . numel ( ) == 0 | | data . data_ptr ( ) , ""tolist() shouldn't be called on a tensor with unallocated storage"" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , data . dtype ( ) . itemsize ( ) ) ; } } } ","TORCH_CHECK ( tensor . numel ( ) == 0 | | data . data_ptr ( ) , ""tolist() shouldn't be called on a tensor with unallocated storage"" ) ; return recursive_to_list ( ( char * ) data . data_ptr ( ) , data . sizes ( ) , data . strides ( ) , 0 , data . scalar_type ( ) , tensor . numel ( ) == 0 0 : data . dtype ( ) . itemsize ( ) ) ; } } } "
"Tensor max_indices = std::get < 0 > ( indices . max ( 1 , false ) ) ; Tensor cpu_min_indices , cpu_max_indices ; if ( indices . is_cuda ( ) ) { cpu_min_indices = min_indices . to ( at::DeviceType::CPU ) ; cpu_max_indices = max_indices . to ( at::DeviceType::CPU ) ; } ","Tensor max_indices = std::get < 0 > ( indices . max ( 1 , false ) ) ; Tensor cpu_min_indices , cpu_max_indices ; if ( ndices . is_cpu ( ) ) { cpu_min_indices = min_indices . to ( at::DeviceType::CPU ) ; cpu_max_indices = max_indices . to ( at::DeviceType::CPU ) ; } "
"const auto & ty = static_cast < detail::ListImpl * > ( payload . u . as_intrusive_ptr ) -> elementType ; return ty == c10::getTypePtr < c10::optional<at::Tensor> > ( ) ; } bool IValue::isIntList ( ) const { ","const auto & ty = static_cast < detail::ListImpl * > ( payload . u . as_intrusive_ptr ) -> elementType ; const auto expected_ty = c10::getTypePtr < c10::optional<at::Tensor> > ( ) ; return expected_ty == ty ; } bool IValue::isIntList ( ) const { "
"SharedParserData & shared ; } ; Parser::Parser ( const std::shared_ptr<SourceView> & src ) : pImpl ( new ParserImpl ( src ) ) { } Parser : : ~ Parser ( ) = default ; ","SharedParserData & shared ; } ; Parser::Parser ( const std::shared_ptr<Source> & src ) : pImpl ( new ParserImpl ( src ) ) { } Parser : : ~ Parser ( ) = default ; "
"if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } ","if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } "
"if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } ","if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } "
"line_end = start ( ) ; while ( line_start < range_end ) { while ( str [ line_end ] != 'n' & & line_end < str . size ( ) ) { } ","line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) & & str [ line_end ] != 'n' ) { + line_end ; } "
"const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } ","const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } "
"std::shared_ptr<SourceView> findSourceInArchiveFromQualifier ( caffe2::serialize::PyTorchStreamReader & reader , const std::string & export_prefix , const std::string & qualifier ) ","std::shared_ptr<Source> findSourceInArchiveFromQualifier ( caffe2::serialize::PyTorchStreamReader & reader , const std::string & export_prefix , const std::string & qualifier ) "
"loaded_sources_ . insert ( qualifier ) ; std::shared_ptr<SourceView> src = source_loader_ ( qualifier ) ; ","loaded_sources_ . insert ( qualifier ) ; std::shared_ptr<Source> src = source_loader_ ( qualifier ) ; "
"auto input = qx ; if ( ndim == 4 ) { input = qx . contiguous ( MemoryFormat::ChannelsLast ) ; } else { std::vector<int64_t> new_sizes { 1 , qx . size ( 0 ) , qx . size ( 1 ) , qx . size ( 2 ) } ; input = qx . view ( new_sizes ) ; } ","auto input = qx ; if ( ndim == 4 ) { input = qx . to ( MemoryFormat::ChannelsLast ) ; } else { std::vector<int64_t> new_sizes { 1 , qx . size ( 0 ) , qx . size ( 1 ) , qx . size ( 2 ) } ; input = qx . view ( new_sizes ) ; } "
"uint8_t output_alignment ; int8_t bias_alignment ; } ; std::unordered_map < CacheKey , cudnn_frontend::ManagedOpaqueDescriptor , at::native::ParamsHash<CacheKey> , at::native::ParamsEqual<CacheKey> > execution_plan_cache ; } ","uint8_t output_alignment ; int8_t bias_alignment ; bool kReluFused ; } ; std::unordered_map < CacheKey , cudnn_frontend::ManagedOpaqueDescriptor , at::native::ParamsHash<CacheKey> , at::native::ParamsEqual<CacheKey> > execution_plan_cache ; } "
"{ key . bias_alignment = -1 ; } auto run = [ & ] ( cudnn_frontend::ManagedOpaqueDescriptor plan_desc ) { auto workspace_size = 0 ; } ","{ key . bias_alignment = -1 ; } key . kReluFused = kReluFused ; auto run = [ & ] ( cudnn_frontend::ManagedOpaqueDescriptor plan_desc ) { auto workspace_size = 0 ; } "
"ParsedArgs < 5 > parsed_args ; auto r = parser . parse ( args , kwargs , parsed_args ) ; if ( r . has_torch_function ( ) ) { return handle_torch_function ( r , args , kwargs , THPNNVariableFunctionsModule , ""torch.nn"" ) ; } auto parsed = parse_to_conversion ( r , false ) ; auto & device = std::get < 0 > ( parsed ) ; ","ParsedArgs < 5 > parsed_args ; auto r = parser . parse ( args , kwargs , parsed_args ) ; if ( r . has_torch_function ( ) ) { return handle_torch_function ( r , args , kwargs , THPNNVariableFunctionsModule , ""torch.nn"" , ""_parse_to"" ) ; } auto parsed = parse_to_conversion ( r , false ) ; auto & device = std::get < 0 > ( parsed ) ; "
"void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; } else { add_next_edge ( autograd::Edge { } ) ; } } ","void addOutputForIValue ( const IValue & value ) { if ( value . isTensorList ( ) ) { input_tensor_lists_ . insert ( { index_ , value . toTensorList ( ) . size ( ) } ) ; for ( const at::Tensor tensor : value . toTensorList ( ) ) { addOutputForTensor ( tensor ) ; index_ + + ; } } else if ( value . isTensor ( ) ) { addOutputForTensor ( value . toTensor ( ) ) ; index_ + + ; } else { add_next_edge ( autograd::Edge { } ) ; index_ + + ; } } "
"bool DeduplicateInitializersByValue ( at::Tensor & t1 , at::Tensor & t2 ) { return t1 . dtype ( ) == t2 . dtype ( ) & & t1 . equal ( t2 ) ; } ","bool DeduplicateInitializersByValue ( at::Tensor & t1 , at::Tensor & t2 ) { if ( t1 . dtype ( ) != t2 . dtype ( ) | | 1 . sizes ( ) . equals ( t2 . sizes ( ) ) | | 1 . strides ( ) . equals ( t2 . strides ( ) ) ) { return false ; } if ( t1 . device ( ) != t2 . device ( ) ) { return t1 . to ( ""cpu"" ) . equal ( t2 . to ( ""cpu"" ) ) ; } return t1 . equal ( t2 ) ; } "
"try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; C10_CUDA_CHECK ( cudaEventDestroy ( event_ ) ) ; if ( udaIPCGlobalEntities::alive ) { return ; } ","try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } "
"c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; C10_CUDA_CHECK ( cudaMemGetInfo ( & device_free , & device_total ) ) ; return { device_free , device_total } ; } ) ; } ","c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; cudaMemGetInfo ( & device_free , & device_total ) ; return { device_free , device_total } ; } ) ; } "
"s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; AT_CUDA_CHECK ( cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } ","s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } "
"if ( measure_kernel_time_ | | isDebugDumpEnabled ( DebugDumpOption::EffectiveBandwidth ) ) { C10_CUDA_CHECK ( cudaEventCreate ( & start_event ) ) ; C10_CUDA_CHECK ( cudaEventCreate ( & finish_event ) ) ; C10_CUDA_CHECK ( cudaEventRecord ( start_event ) ) ; } if ( execute_kernel_ ) { ","if ( measure_kernel_time_ | | isDebugDumpEnabled ( DebugDumpOption::EffectiveBandwidth ) ) { cudaEventCreate ( & start_event ) ; cudaEventCreate ( & finish_event ) ; cudaEventRecord ( start_event ) ; } if ( execute_kernel_ ) { "
"if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; C10_CUDA_CHECK ( cudaFree ( nullptr ) ) ; } } ","if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; cudaFree ( nullptr ) ; } } "
"try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } ","try { if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; C10_CUDA_CHECK ( cudaEventDestroy ( event_ ) ) ; if ( udaIPCGlobalEntities::alive ) { return ; } "
"if ( comms ) { for ( const auto i : c10::irange ( ndevices ) ) { int dummy_var ; if ( cudaGetDevice ( & dummy_var ) != cudaSuccess ) { ","if ( comms ) { for ( const auto i : c10::irange ( ndevices ) ) { int dummy_var ; if ( C10_CUDA_ERROR_HANDLED ( cudaGetDevice ( & dummy_var ) ) != cudaSuccess ) { "
"c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; cudaMemGetInfo ( & device_free , & device_total ) ; return { device_free , device_total } ; } ) ; } ","c10::cuda::CUDAGuard guard ( device ) ; size_t device_free = 0 ; size_t device_total = 0 ; C10_CUDA_CHECK ( cudaMemGetInfo ( & device_free , & device_total ) ) ; return { device_free , device_total } ; } ) ; } "
"s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } ","s_ipc_event_handle . c_str ( ) ) ; cudaEvent_t event ; AT_CUDA_CHECK ( cudaIpcOpenEventHandle ( & event , * ipc_event_handle ) ) ; AT_CUDA_CHECK ( cudaStreamWaitEvent ( c10::cuda::getCurrentCUDAStream ( device ) , event , 0 ) ) ; } "
"if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; cudaFree ( nullptr ) ; } } ","if ( ctx ) { std::unique_lock<std::mutex> cudaFreeMutexLock ( * ( c10::cuda::CUDACachingAllocator::getFreeMutex ( ) ) ) ; C10_CUDA_CHECK ( cudaFree ( nullptr ) ) ; } } "
"if ( * largest == 0 ) { size_t tmp_bytes ; cudaMemGetInfo ( largest , & tmp_bytes ) ; } cache_info_aux ( large_blocks , total , largest ) ; cache_info_aux ( small_blocks , total , largest ) ; ","if ( * largest == 0 ) { size_t tmp_bytes ; C10_CUDA_CHECK ( cudaMemGetInfo ( largest , & tmp_bytes ) ) ; } cache_info_aux ( large_blocks , total , largest ) ; cache_info_aux ( small_blocks , total , largest ) ; "
"cudaEvent_t event = e . first ; Block * block = e . second ; cudaError_t err = cudaEventQuery ( event ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; ","cudaEvent_t event = e . first ; Block * block = e . second ; cudaError_t err = C10_CUDA_ERROR_HANDLED ( cudaEventQuery ( event ) ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; "
"Tensor ravel ( const Tensor & self ) { return self . reshape ( -1 ) ; } static inline void handle_unflatten_exception ( const std::runtime_error & e , ","Tensor ravel ( const Tensor & self ) { return self . contiguous ( ) . view ( -1 ) ; } static inline void handle_unflatten_exception ( const std::runtime_error & e , "
"{ facebook::jni::throwNewJavaException ( facebook::jni::gJavaLangIllegalArgumentException , ""at::Tensor scalar type is not supported on java side"" ) ; } const auto & tensorShape = tensor . sizes ( ) ; ","{ facebook::jni::throwNewJavaException ( facebook::jni::gJavaLangIllegalArgumentException , ""at::Tensor scalar type %s is not supported on java side"" , c10::toString ( scalarType ) ) ; } const auto & tensorShape = tensor . sizes ( ) ; "
"mobile::serialization::Module * module , ExtraFilesMap & extra_files ) { auto extra_files_offsets = module -> extra_files ( ) ; parseExtraFilesFromVector ( module -> extra_files ( ) , & extra_files ) ; } mobile::Module FlatbufferLoader::parseModule ( ","mobile::serialization::Module * module , ExtraFilesMap & extra_files ) { auto extra_files_offsets = module -> extra_files ( ) ; parseExtraFilesFromVector ( extra_files_offsets , & extra_files ) ; } mobile::Module FlatbufferLoader::parseModule ( "
") ; return PyObject_Call ( func , py_args . ptr ( ) , kwargs ) ; END_HANDLE_TH_ERRORS } ",") ; auto r = PyObject_Call ( func , py_args . ptr ( ) , kwargs ) ; if ( r == nullptr ) throw python_error ( ) ; return r ; END_HANDLE_TH_ERRORS } "
"TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , ""renorm: expected maxnorm to be >= 0 but got "" , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , ""renorm: input needs at least 2 dimensions, got "" , ndim , ""dimensions"" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } ","TORCH_CHECK ( maxnorm . toDouble ( ) >= 0 . 0 , ""renorm: expected maxnorm to be >= 0 but got "" , maxnorm . toDouble ( ) ) ; const auto ndim = self . dim ( ) ; TORCH_CHECK ( ndim > 1 , ""renorm: input needs at least 2 dimensions, got "" , ndim , "" dimensions"" ) ; set_output ( self . sizes ( ) , self . options ( ) ) ; } "
"payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; TORCH_CHECK ( payloadSection . has_value ( ) , ""Missing the payload section"" ) ; break ; } } ","payloadStart = payloadSection -> start ; customLoader = s . customLoader ; size = payloadSection -> len ; MULTIPY_CHECK ( payloadSection . has_value ( ) , ""Missing the payload section"" ) ; break ; } } "
"shdrList_ = ( Elf64_Shdr * ) ( fileData + ehdr_ -> e_shoff ) ; auto strtabSecNo = ehdr_ -> e_shstrndx ; TORCH_CHECK ( strtabSecNo >= 0 & & strtabSecNo < numSections_ , ""e_shstrndx out of range"" ) ; ","shdrList_ = ( Elf64_Shdr * ) ( fileData + ehdr_ -> e_shoff ) ; auto strtabSecNo = ehdr_ -> e_shstrndx ; MULTIPY_CHECK ( strtabSecNo >= 0 & & strtabSecNo < numSections_ , ""e_shstrndx out of range"" ) ; "
"at::optional<Section> ElfFile::findSection ( const char * name ) const { TORCH_CHECK ( name != nullptr , ""Null name"" ) ; at::optional<Section> found = at::nullopt ; for ( const auto & section : sections_ ) { if ( strcmp ( name , section . name ) == 0 ) { ","at::optional<Section> ElfFile::findSection ( const char * name ) const { MULTIPY_CHECK ( name != nullptr , ""Null name"" ) ; at::optional<Section> found = at::nullopt ; for ( const auto & section : sections_ ) { if ( strcmp ( name , section . name ) == 0 ) { "
"xnn_operator_t xnnp_op = nullptr ; this -> input_scale = input_scale ; ","xnn_operator_t xnnp_op = nullptr ; input_scale = act_input_scale ; "
"vec_conv . emplace_back ( FusionBehavior::DYNAMIC , pair . second ) ; } else { TORCH_INTERNAL_ASSERT ( ""FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: "" , pair . first ) ; } ","vec_conv . emplace_back ( FusionBehavior::DYNAMIC , pair . second ) ; } else { TORCH_INTERNAL_ASSERT ( false , ""FusionBehavior only supported 'STATIC' or 'DYNAMIC', got: "" , pair . first ) ; } "
"return [ ] ( ProcessedNode * p_node ) { const auto inputs = p_node -> Input ( 0 ) . toTensorVector ( ) ; const auto dim = p_node -> Input ( 1 ) . toInt ( ) ; if ( p_node -> Output ( 0 ) . isNone ( ) ) { p_node -> Output ( 0 ) = at::native::_stack_cpu ( inputs , dim ) ; ","return [ ] ( ProcessedNode * p_node ) { const auto inputs = p_node -> Input ( 0 ) . toTensorVector ( ) ; TORCH_CHECK ( inputs . size ( ) > 0 , ""stack expects non-empty tensor list"" ) ; const auto dim = p_node -> Input ( 1 ) . toInt ( ) ; if ( p_node -> Output ( 0 ) . isNone ( ) ) { p_node -> Output ( 0 ) = at::native::_stack_cpu ( inputs , dim ) ; "
"static inline Variable valueToTensor ( c10::TensorOptions options , PyObject * value , const at::Device & device ) { if ( THPVariable_Check ( value ) ) { return THPVariable_Unpack ( value ) ; } ","inline Variable valueToTensor ( c10::TensorOptions options , PyObject * value , const at::Device & device ) { if ( THPVariable_Check ( value ) ) { return THPVariable_Unpack ( value ) ; } "
"if ( s_root_block_ | | C10_UNLIKELY ( chema ) ) { TORCH_CHECK ( kwargs . empty ( ) , ""Schema is not available, but BlockRunner got kwargs."" ) ; for ( size_t i = 0 ; i < args . size ( ) ; + + i ) { set_arg ( i , std::forward<IValueList> ( args ) ) ; } ","if ( s_root_block_ | | C10_UNLIKELY ( chema ) ) { TORCH_CHECK ( kwargs . empty ( ) , ""Schema is not available, but BlockRunner got kwargs."" ) ; const auto total_num_inputs = args . size ( ) + first_input_is_self_ ; TORCH_CHECK ( total_num_inputs == block_info_ . num_inputs ( ) ) ; for ( size_t i = 0 ; i < args . size ( ) ; + + i ) { set_arg ( i , std::forward<IValueList> ( args ) ) ; } "
"std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; ","std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( const auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; "
"std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . push_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . push_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } ","std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . emplace_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . emplace_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } "
"return per_bucket_variable_indices ; } std::vector<int> Logger::get_bucket_sizes ( ) { std::vector<int> bucket_sizes ; for ( const auto & bucket : reducer_ -> buckets_ ) { const auto & variables = bucket . variables ; int bucket_size = 0 ; for ( const auto & v : variables ) { bucket_size += v . numel ( ) * v . element_size ( ) ; } ","return per_bucket_variable_indices ; } std::vector<int64_t> Logger::get_bucket_sizes ( ) { std::vector<int64_t> bucket_sizes ; for ( const auto & bucket : reducer_ -> buckets_ ) { const auto & variables = bucket . variables ; int64_t bucket_size = 0 ; for ( const auto & v : variables ) { bucket_size += v . numel ( ) * v . element_size ( ) ; } "
"struct BucketKey { BucketKey ( c10::ScalarType type , c10::Device device ) : type ( std::move ( type ) ) , device ( std::move ( device ) ) { } const c10::ScalarType type ; const c10::Device device ; ","struct BucketKey { BucketKey ( c10::ScalarType type , c10::Device device ) : type ( type ) , device ( device ) { } const c10::ScalarType type ; const c10::Device device ; "
"if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } ","if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } "
"c10::string_view str = source_view_ -> text ( ) ; if ( size ( ) == str . size ( ) ) { ","auto str = source_view_ -> text_str ( ) . str ( ) ; if ( size ( ) == str . size ( ) ) { "
"line_end = start ( ) ; while ( line_start < range_end ) { while ( str [ line_end ] != 'n' & & line_end < str . size ( ) ) { } ","line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) & & str [ line_end ] != 'n' ) { + line_end ; } "
"const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } ","const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } "
": data ( std::move ( data ) ) , size ( size ) , deserializer ( new SourceRangeDeserializer ( ) ) , unpickled_records ( nullptr ) { } void ConcreteSourceRangeUnpickler::unpickle ( ) { ",": data ( std::move ( data ) ) , size ( size ) , deserializer ( nullptr ) , unpickled_records ( nullptr ) { } void ConcreteSourceRangeUnpickler::unpickle ( ) { "
"Tensor & normal_meta_ ( Tensor & self , double mean , double std , c10::optional<Generator> gen ) { TORCH_CHECK ( std > 0 . 0 , ""normal_ expects std > 0.0, but found std="" , std ) ; return self ; } ","Tensor & normal_meta_ ( Tensor & self , double mean , double std , c10::optional<Generator> gen ) { TORCH_CHECK ( std >= 0 . 0 , ""normal_ expects std >= 0.0, but found std="" , std ) ; return self ; } "
"std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( const auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; ","std::vector < std::vector<at::Tensor> > output_tensors ; output_tensors . reserve ( tensors_to_verify . size ( ) ) ; for ( auto & tensor_shape : tensors_to_verify ) { std::vector<at::Tensor> outputs ; outputs . reserve ( pg -> getSize ( ) ) ; for ( const auto i : c10::irange ( pg -> getSize ( ) ) ) { ( void ) i ; outputs . emplace_back ( at::zeros_like ( tensor_shape ) ) ; } output_tensors . emplace_back ( outputs ) ; "
"std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . emplace_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . emplace_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } ","std::vector<std::string> dtype_strs ; std::vector<std::string> device_type_strs ; for ( const auto & tensor_dtype : collective_fingerprint . tensor_dtypes_ ) { dtype_strs . push_back ( c10::toString ( static_cast<at::ScalarType> ( tensor_dtype ) ) ) ; } for ( const auto & tensor_device_type : collective_fingerprint . tensor_device_types_ ) { device_type_strs . push_back ( c10::toString ( static_cast<at::DeviceType> ( tensor_device_type ) ) ) ; } "
"P = ( L + R ) >> 1 ; swap_fn ( P , L + 1 ) ; if ( gt_or_nan ( arr [ L + 1 ] , arr [ R ] ) ) { swap_fn ( L + 1 , R ) ; ","P = L + ( R - L ) / 2 ; swap_fn ( P , L + 1 ) ; if ( gt_or_nan ( arr [ L + 1 ] , arr [ R ] ) ) { swap_fn ( L + 1 , R ) ; "
"if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) . str ( ) , expr . range ( ) ) ) { return typePtr ; } } ","if ( resolver_ ) { if ( auto typePtr = resolver_ -> resolveType ( expr . range ( ) . text ( ) , expr . range ( ) ) ) { return typePtr ; } } "
"auto str = source_view_ -> text_str ( ) . str ( ) ; if ( size ( ) == str . size ( ) ) ","c10::string_view str = source_view_ -> text ( ) ; if ( size ( ) == str . size ( ) ) "
"line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) & & str [ line_end ] != 'n' ) { } ","line_end = start ( ) ; while ( line_start < range_end ) { while ( str [ line_end ] != 'n' & & line_end < str . size ( ) ) { + line_end ; } "
"if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer -> deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } ","if ( tup_elems . size ( ) == 3 ) { int64_t debug_handle = tup_elems [ kSourceRangeTagIndex ] . toInt ( ) ; auto source_range = deserializer . deserialize ( tup_elems [ kSourceRangeIndex ] ) ; source_range_map . emplace ( debug_handle , std::move ( source_range ) ) ; } } "
"const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } ","const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } "
"line_end = start ( ) ; while ( line_start < range_end ) { while ( str [ line_end ] != 'n' & & line_end < str . size ( ) ) { } ","line_end = start ( ) ; while ( line_start < range_end ) { while ( line_end < str . size ( ) & & str [ line_end ] != 'n' ) { + line_end ; } "
"const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text ( ) == source_m -> text ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } ","const auto source_n = n -> sourceRange ( ) . source ( ) ; const auto source_m = m -> sourceRange ( ) . source ( ) ; return ( ( source_n -> text_str ( ) == source_m -> text_str ( ) ) & & ( source_n -> starting_line_no ( ) == source_m -> starting_line_no ( ) ) ) ; } "
"{ auto qengines = at::globalContext ( ) . supportedQEngines ( ) ; auto list = THPObjectPtr ( PyList_New ( qengines . size ( ) ) ) ; for ( const auto i : c10::irange ( qengines . size ( ) ) ) { PyObject * i64 = THPUtils_packInt64 ( static_cast<int> ( qengines [ i ] ) ) ; if ( 64 ) { throw python_error ( ) ; } PyList_SET_ITEM ( list . get ( ) , i , i64 ) ; } return list . release ( ) ; ","{ auto qengines = at::globalContext ( ) . supportedQEngines ( ) ; auto list = THPObjectPtr ( PyList_New ( qengines . size ( ) ) ) ; if ( ist ) return nullptr ; for ( const auto i : c10::irange ( qengines . size ( ) ) ) { PyObject * i64 = THPUtils_packInt64 ( static_cast<int> ( qengines [ i ] ) ) ; if ( 64 ) return nullptr ; PyList_SET_ITEM ( list . get ( ) , i , i64 ) ; } return list . release ( ) ; "
"if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; ","if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; "
"auto numel = add_indices . numel ( ) ; int64_t ddim = src . sizes ( ) [ 1 ] ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; ","auto numel = add_indices . numel ( ) ; int64_t ddim = src . size ( 1 ) ; auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; "
"if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; ","if ( bag_size . defined ( ) ) { bag_size_data = bag_size . data_ptr<index_t> ( ) ; } auto vocab_size = src . size ( 0 ) ; auto src_stride0 = src . strides ( ) [ 0 ] ; auto src_stride1 = src . strides ( ) [ 1 ] ; auto output_stride0 = output . strides ( ) [ 0 ] ; "
"parent -> replace_stmt ( loops . front ( ) , empty_block ) ; for ( size_t i = 1 ; i < loops . size ( ) ; + + i ) { auto block = to<Block> ( loops [ i ] -> get_parent ( ) ) ; TORCH_INTERNAL_ASSERT ( block ) ; block -> remove_stmt ( loops [ i ] ) ; } ","parent -> replace_stmt ( loops . front ( ) , empty_block ) ; for ( size_t i = 1 ; i < loops . size ( ) ; + + i ) { auto block = to<Block> ( loops [ i ] -> get_parent ( ) ) ; TORCH_INTERNAL_ASSERT ( block , buildErrorMessage ( ""Expected parent stmt to be a non-null Block in reorder transformation the fuser."" ) ) ; block -> remove_stmt ( loops [ i ] ) ; } "
"auto get_new_indices = [ & ] ( const std::vector<ExprPtr> & indices ) { TORCH_INTERNAL_ASSERT ( indices . size ( ) == dims . size ( ) ) ; std::vector<ExprPtr> new_indices ( indices ) ; for ( size_t i = 0 ; i < dims . size ( ) ; + + i ) { if ( dims [ i ] ) { ","auto get_new_indices = [ & ] ( const std::vector<ExprPtr> & indices ) { TORCH_INTERNAL_ASSERT ( indices . size ( ) == dims . size ( ) , buildErrorMessage ( ""Expected ranks to match in compressBuffer in the fuser."" ) ) ; std::vector<ExprPtr> new_indices ( indices ) ; for ( size_t i = 0 ; i < dims . size ( ) ; + + i ) { if ( dims [ i ] ) { "
"TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad | | info . kind == kMutate ; bool hasWrites = info . kind == kStore | | info . kind == kMutate ; ","TORCH_INTERNAL_ASSERT ( bounds_it -> second . size ( ) == 1 , buildErrorMessage ( ""Unexpected number of bound info entries in cacheAccesses in the fuser."" ) ) ; TensorAccessBoundsInfo & info = bounds_it -> second [ 0 ] ; bool hasReads = info . kind == kLoad | | info . kind == kMutate ; bool hasWrites = info . kind == kStore | | info . kind == kMutate ; "
"TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; + + i ) { ","TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( ""Expected ranks to match in RfactorStoreRewriter in the fuser."" ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; + + i ) { "
"TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; + + i ) { ","TORCH_INTERNAL_ASSERT ( old_indices_ . size ( ) == v -> indices ( ) . size ( ) , buildErrorMessage ( ""Expected ranks to match in RfactorStoreRewriter in the fuser."" ) ) ; bool equal_indices = true ; for ( size_t i = 0 ; i < v -> indices ( ) . size ( ) ; + + i ) { "
"if ( aStrides . empty ( ) | | oStrides . empty ( ) ) { return false ; } TORCH_INTERNAL_ASSERT ( info -> bounds ( ) . size ( ) == other -> bounds ( ) . size ( ) ) ; for ( size_t b = 0 ; b < info -> bounds ( ) . size ( ) ; + + b ) { ExprPtr aIndexStride = aStrides [ b ] ; ExprPtr oIndexStride = oStrides [ b ] ; ","if ( aStrides . empty ( ) | | oStrides . empty ( ) ) { return false ; } TORCH_INTERNAL_ASSERT ( info -> bounds ( ) . size ( ) == other -> bounds ( ) . size ( ) , buildErrorMessage ( ""Dimension mismatch for two accesses in mem dep checker in the fuser."" ) ) ; for ( size_t b = 0 ; b < info -> bounds ( ) . size ( ) ; + + b ) { ExprPtr aIndexStride = aStrides [ b ] ; ExprPtr oIndexStride = oStrides [ b ] ; "
"VarPtr var = v -> buffer_var ( ) ; auto it = intermediates_ . find ( var ) ; TORCH_INTERNAL_ASSERT ( it != intermediates_ . end ( ) ) ; IndexBounds bounds = it -> second -> bounds ( ) ; auto info = std::make_shared<AccessInfo> ( ","VarPtr var = v -> buffer_var ( ) ; auto it = intermediates_ . find ( var ) ; TORCH_INTERNAL_ASSERT ( it != intermediates_ . end ( ) , buildErrorMessage ( ""Expected to find '"" + var -> name_hint ( ) + ""' in intermediate vars in mem dep checker in the fuser."" ) ) ; IndexBounds bounds = it -> second -> bounds ( ) ; auto info = std::make_shared<AccessInfo> ( "
"TORCH_INTERNAL_ASSERT ( ( typeConstraints & ( kQintTypes | kComplexTypes ) ) == 0 ) ; return false ; } ","TORCH_INTERNAL_ASSERT ( ( typeConstraints & ( kQintTypes | kComplexTypes ) ) == 0 , buildErrorMessage ( ""Qint and Complex types are not supported in the fuser."" ) ) ; return false ; } "
"auto attrVal = tryInsertConstant ( * graph , attr ) ; n -> output ( ) -> replaceAllUsesWith ( * attrVal ) ; n -> destroy ( ) ; } } } ","auto attrVal = tryInsertConstant ( * graph , attr ) ; n -> output ( ) -> replaceAllUsesWith ( * attrVal ) ; nodesToDestroy . emplace ( n ) ; } } } "
"std::end ( nextParameterIValues ) ) ; } } return parameterIValues ; } ","std::end ( nextParameterIValues ) ) ; } } for ( auto n : nodesToDestroy ) { n -> destroy ( ) ; } return parameterIValues ; } "
"void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; ","void ReturnRefCounter ( const std::string & handle , uint64_t offset ) { if ( udaIPCGlobalEntities::alive ) { return ; } std::lock_guard<std::mutex> lock ( cuda_ipc_global_entities . ref_counters_mutex_ ) ; auto & map = cuda_ipc_global_entities . ref_counters_files_ ; "
"if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; cuda_ipc_global_entities . sync_events_used_ - - ; } } catch ( . . . ) { ","if ( event_sync_required_ ) { at::cuda::CUDAGuard device_guard ( device_ . index ( ) ) ; cudaEventDestroy ( event_ ) ; if ( udaIPCGlobalEntities::alive ) { return ; } cuda_ipc_global_entities . sync_events_used_ - - ; } } catch ( . . . ) { "
"bool CudaIPCCollect ( ) { bool freed_memory = cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . collect ( ) ; if ( cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . size ( ) == 0 ) { cuda_ipc_global_entities . safe_clean_current_file ( ) ; ","bool CudaIPCCollect ( ) { if ( udaIPCGlobalEntities::alive ) { return true ; } bool freed_memory = cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . collect ( ) ; if ( cuda_ipc_global_entities . CudaIPCSentDataLimbo_ . size ( ) == 0 ) { cuda_ipc_global_entities . safe_clean_current_file ( ) ; "
"auto reduce_tensor = result . toTensorVector ( ) [ 0 ] ; decompressed_tensor . copy_ ( reduce_tensor ) ; return c10::IValue ( decompressed_tensor ) ; } ; ","auto reduce_tensor = result . toTensorVector ( ) [ 0 ] ; TORCH_INTERNAL_ASSERT_DEBUG_ONLY ( reduce_tensor . scalar_type ( ) == at::ScalarType::Half , ""Expected reduced tensor to be fp16 in FP16CompressHook, but got type "" , reduce_tensor . scalar_type ( ) ) ; decompressed_tensor . copy_ ( reduce_tensor ) ; return c10::IValue ( decompressed_tensor ) ; } ; "
"} if ( FLAGS_torch_jit_enable_rethrow_caught_exception ) { if ( future_ ) { future_ -> setError ( std::make_exception_ptr ( e ) ) ; } throw ; } ","} if ( FLAGS_torch_jit_enable_rethrow_caught_exception ) { if ( future_ ) { future_ -> setError ( std::current_exception ( ) ) ; return false ; } throw ; } "
"isOpSupportedInMobile ( op ) , toString ( op ) , "" is not supported in mobile module."" ) ; code_ -> instructions_ . emplace_back ( op , X , N ) ; code_ -> debug_handles_ . emplace_back ( dbg_handle ) ; } bool Function::append_operator ( ","isOpSupportedInMobile ( op ) , toString ( op ) , "" is not supported in mobile module."" ) ; code_ -> instructions_with_handles_ . emplace_back ( Instruction ( op , X , N ) , dbg_handle ) ; } bool Function::append_operator ( "
"size_t pc = 0 ; while ( true ) { try { Instruction inst = code_ -> instructions_ [ pc ] ; ","size_t pc = 0 ; while ( true ) { try { Instruction inst = code_ -> instructions_with_handles_ [ pc ] . instruction ; "
". sequenceNr ( ctx -> sequenceNr ) . fwdThreadId ( ctx -> fwdThreadId ) . scope ( ctx -> recFunScope ) . setAsync ( fn . isAsync ( ) ) ; if ( ctx -> shapes & & tx -> shapes -> empty ( ) ) { kineto_events_ . back ( ) . shapes ( * ctx -> shapes ) ; } ",". sequenceNr ( ctx -> sequenceNr ) . fwdThreadId ( ctx -> fwdThreadId ) . scope ( ctx -> recFunScope ) . setAsync ( fn . isAsync ( ) ) . debugHandle ( ctx -> debug_handle ) ; if ( ctx -> shapes & & tx -> shapes -> empty ( ) ) { kineto_events_ . back ( ) . shapes ( * ctx -> shapes ) ; } "
"at::set_record_function_tls_ ( state . rf_tls_ ) ; c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo ( state . debug_info_ ) ; c10::impl::_force_tls_local_dispatch_key_set ( state . dispatch_key_ ) ; ","at::set_record_function_tls_ ( state . rf_tls_ ) ; SavedTensorDefaultHooks::set_hooks ( state . saved_tensors_default_hooks_ . first , state . saved_tensors_default_hooks_ . second ) ; c10::ThreadLocalDebugInfo::_forceCurrentDebugInfo ( state . debug_info_ ) ; c10::impl::_force_tls_local_dispatch_key_set ( state . dispatch_key_ ) ; "
"DWORD low = 1 , high = 0 ; OVERLAPPED offset = { 0 , 0 , 0 , 0 , NULL } ; if ( hdl < 0 ) return -1 ; switch ( op ) { ","DWORD low = 1 , high = 0 ; OVERLAPPED offset = { 0 , 0 , 0 , 0 , NULL } ; if ( ( intptr_t ) hdl < 0 ) return -1 ; switch ( op ) { "
"} ifdef _WIN32 struct timeval timeoutTV = { value . count ( ) / 1000 , ( value . count ( ) % 1000 ) * 1000 } ; else struct timeval timeoutTV = { . tv_sec = value . count ( ) / 1000 , . tv_usec = ( value . count ( ) % 1000 ) * 1000 } ; ","} ifdef _WIN32 struct timeval timeoutTV = { static_cast<long> ( value . count ( ) / 1000 ) , static_cast<long> ( ( value . count ( ) % 1000 ) * 1000 ) } ; else struct timeval timeoutTV = { . tv_sec = value . count ( ) / 1000 , . tv_usec = ( value . count ( ) % 1000 ) * 1000 } ; "
"} void PyDefaultSavedVariableHooks::reset_hooks ( ) { if ( Py_IsInitialized ( ) ) { py::gil_scoped_acquire gil ; Py_XDECREF ( pack_hook_ ) ; ","} void PyDefaultSavedVariableHooks::reset_hooks ( ) { std::lock_guard<std::mutex> lock ( mutex_ ) ; if ( Py_IsInitialized ( ) ) { py::gil_scoped_acquire gil ; Py_XDECREF ( pack_hook_ ) ; "
"if ( s_output | | is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } ","if ( s_output | | is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } "
"nnapi_processed = pyMethod ( wrapped_mod , inp . toTensor ( ) ) ; } else { py::list pyInp ; for ( torch::Tensor inpElem : inp . toTensorList ( ) ) { pyInp . append ( inpElem ) ; } nnapi_processed = pyMethod ( wrapped_mod , pyInp ) ; ","nnapi_processed = pyMethod ( wrapped_mod , inp . toTensor ( ) ) ; } else { py::list pyInp ; for ( at::Tensor inpElem : inp . toTensorList ( ) ) { pyInp . append ( inpElem ) ; } nnapi_processed = pyMethod ( wrapped_mod , pyInp ) ; "
"if ( s_output | | is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } ","if ( s_output | | is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } "
"data_ = variable . tensor_data ( ) ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; } } ","data_ = variable . tensor_data ( ) ; } } "
"static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , ""polygamma"" , [ & ] ( ) { cpu_kernel ( ","static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else if ( n == 1 ) { trigamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , ""polygamma"" , [ & ] ( ) { cpu_kernel ( "
"int * ipiv , c10::complex<double> * ret , int ldb , int * info ) { TORCH_CUSOLVER_CHECK ( cusolverDnZgetrs ( handle , CUBLAS_OP_N , n , nrhs , reinterpret_cast < cuDoubleComplex * > ( dA ) , ","int * ipiv , c10::complex<double> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnZgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuDoubleComplex * > ( dA ) , "
"int * ipiv , c10::complex<float> * ret , int ldb , int * info ) { TORCH_CUSOLVER_CHECK ( cusolverDnCgetrs ( handle , CUBLAS_OP_N , n , nrhs , reinterpret_cast < cuComplex * > ( dA ) , ","int * ipiv , c10::complex<float> * ret , int ldb , int * info , cublasOperation_t trans ) { TORCH_CUSOLVER_CHECK ( cusolverDnCgetrs ( handle , trans , n , nrhs , reinterpret_cast < cuComplex * > ( dA ) , "
"static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else if ( n == 1 ) { trigamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , ""polygamma"" , [ & ] ( ) { cpu_kernel ( ","static void polygamma_kernel ( TensorIteratorBase & iter , int64_t n ) { if ( n == 0 ) { digamma_kernel ( iter ) ; } else { AT_DISPATCH_FLOATING_TYPES_AND ( kBFloat16 , iter . dtype ( ) , ""polygamma"" , [ & ] ( ) { cpu_kernel ( "
"if ( s_output | | is_leaf_ ) { saved_original_ = true ; data_ = variable ; return ; } ","if ( s_output | | is_leaf_ ) { saved_original_ = true ; data_ = variable ; register_hooks ( Engine::get_default_engine ( ) . get_default_saved_variable_hooks ( ) ) ; return ; } "
"cudaError_t err = cudaEventQuery ( event ) ; if ( err == cudaErrorNotReady ) { break ; } else if ( err != cudaSuccess ) { return err ; ","cudaError_t err = cudaEventQuery ( event ) ; if ( err == cudaErrorNotReady ) { cudaGetLastError ( ) ; break ; } else if ( err != cudaSuccess ) { return err ; "
"struct THCCachingHostAllocator final : public at::Allocator { at::DataPtr allocate ( size_t size ) const override { THAssert ( size >= 0 ) ; void * ptr ; THCudaCheck ( allocator . malloc ( & ptr , size ) ) ; return { ptr , ptr , & THCCachingHostDeleter , at::DeviceType::CPU } ; ","struct THCCachingHostAllocator final : public at::Allocator { at::DataPtr allocate ( size_t size ) const override { void * ptr ; THCudaCheck ( allocator . malloc ( & ptr , size ) ) ; return { ptr , ptr , & THCCachingHostDeleter , at::DeviceType::CPU } ; "
"collectCatNodes ( graph_ -> block ( ) ) ; bool changed = false ; for ( auto c : cat_nodes_ ) { changed = changed | | replaceWithVariadicCat ( c ) ; } return changed ; } ","collectCatNodes ( graph_ -> block ( ) ) ; bool changed = false ; for ( auto c : cat_nodes_ ) { changed = replaceWithVariadicCat ( c ) | | changed ; } return changed ; } "
"at::assert_no_internal_overlap ( result ) ; const Tensor * pnotSkippedTensor = [ ] ( TensorList tensors ) -> const Tensor * { for ( auto const & tensor : tensors ) { if ( should_skip ( tensor ) ) { continue ; ","at::assert_no_internal_overlap ( result ) ; const Tensor * pnotSkippedTensor = [ ] ( const TensorList & tensors ) -> const Tensor * { for ( auto const & tensor : tensors ) { if ( should_skip ( tensor ) ) { continue ; "
"if ( nsafe ) { auto min_length = lengths_value . min ( ) . item<int64_t> ( ) ; TORCH_CHECK ( ( min_length >= 0 ) , ""lengths contains negative value!"" ) ; TORCH_CHECK ( min_length != 0 | | initial . has_value ( ) ) ; TORCH_CHECK ( lengths_value . sum ( ) . item<int64_t> ( ) == data . size ( axis ) ) ; } ","if ( nsafe ) { auto min_length = lengths_value . min ( ) . item<int64_t> ( ) ; TORCH_CHECK ( ( min_length >= 0 ) , ""lengths contains negative value!"" ) ; TORCH_CHECK ( lengths_value . sum ( ) . item<int64_t> ( ) == data . size ( axis ) ) ; } "
"{ auto stream = c10::cuda::getCurrentCUDAStream ( device . index ( ) ) ; C10_CUDA_CHECK ( cudaStreamSynchronize ( stream ) ) ; event_sync_required_ = false ; } else ","{ auto stream = c10::cuda::getCurrentCUDAStream ( device . index ( ) ) ; C10_CUDA_CHECK ( cudaStreamSynchronize ( stream ) ) ; event_ = nullptr ; event_sync_required_ = false ; } else "
"if ( at::detail::getCUDAHooks ( ) . hasPrimaryContext ( idx ) ) { endif caller_current_streams_ [ idx ] = guard . getStream ( { c10::DeviceType::CUDA , idx } ) ; caller_default_streams_ [ idx ] = guard . getDefaultStream ( { c10::DeviceType::CUDA , idx } ) ; } else { caller_current_streams_ [ idx ] = c10::nullopt ; caller_default_streams_ [ idx ] = c10::nullopt ; } } } ","if ( at::detail::getCUDAHooks ( ) . hasPrimaryContext ( idx ) ) { endif caller_current_streams_ [ idx ] = guard . getStream ( { c10::DeviceType::CUDA , idx } ) ; } else { caller_current_streams_ [ idx ] = c10::nullopt ; } } } "
"SET roots ; for ( const auto & F : visibleFuncs ) { std::string name = F -> getName ( ) ; auto demangled = _demangle ( name ) ; if ( RootSymbolPatternLoc . pattern -> match ( demangled ) ) { roots . insert ( name ) ; ","SET roots ; for ( const auto & F : visibleFuncs ) { std::string name = _name ( F ) ; auto demangled = _demangle ( name ) ; if ( RootSymbolPatternLoc . pattern -> match ( demangled ) ) { roots . insert ( name ) ; "
"SmallVector < Constant * , 16 > worklist ; SmallPtrSet < Constant * , 16 > visited ; if ( auto CS = CallSite ( & I ) ) { Function * callee = CS . getCalledFunction ( ) ; if ( callee & & allee -> isIntrinsic ( ) & & visited . insert ( callee ) . second ) { CB ( callee ) ; } ","SmallVector < Constant * , 16 > worklist ; SmallPtrSet < Constant * , 16 > visited ; if ( _isCallSite ( & I ) ) { Function * callee = _getCalledFunction ( & I ) ; if ( callee & & allee -> isIntrinsic ( ) & & visited . insert ( callee ) . second ) { CB ( callee ) ; } "
"if ( isitedOps -> empty ( ) ) { if ( Verbose ) { std::cerr << ""[INFO] ignore extra op schema str: "" << * schemaStr << "" in: "" << _demangle ( src -> getFunction ( ) -> getName ( ) ) << "", because already found valid op schema str: "" << * visitedOps -> begin ( ) << std::endl ; } ","if ( isitedOps -> empty ( ) ) { if ( Verbose ) { std::cerr << ""[INFO] ignore extra op schema str: "" << * schemaStr << "" in: "" << _demangle ( _name ( src -> getFunction ( ) ) ) << "", because already found valid op schema str: "" << * visitedOps -> begin ( ) << std::endl ; } "
"if ( visitedFunctions ) { ( * visitedFunctions ) . insert ( F -> getName ( ) ) ; } if ( Verbose > 1 ) { std::cerr << ""[DEBUG][FUNC] "" << _demangle ( F -> getName ( ) ) << std::endl ; printDebugPath ( debugPath . get ( ) , src , V ) ; } } ","if ( visitedFunctions ) { ( * visitedFunctions ) . insert ( _name ( F ) ) ; } if ( Verbose > 1 ) { std::cerr << ""[DEBUG][FUNC] "" << _demangle ( _name ( F ) ) << std::endl ; printDebugPath ( debugPath . get ( ) , src , V ) ; } } "
"for ( auto V : instructions ) { auto I = dyn_cast<Instruction> ( V ) ; if ( | | allSite ( I ) ) { continue ; } auto contextualNamespace = inferContextualNamespace ( I ) ; ","for ( auto V : instructions ) { auto I = dyn_cast<Instruction> ( V ) ; if ( | | isCallSite ( I ) ) { continue ; } auto contextualNamespace = inferContextualNamespace ( I ) ; "
"if ( visitedFunctions . empty ( ) ) { std::cerr << ""[WARNING] could not find registered function for op: "" << op << "" in function: "" << _demangle ( I -> getFunction ( ) -> getName ( ) ) << "" contextualNamespace: "" << contextualNamespace << std::endl ; } ","if ( visitedFunctions . empty ( ) ) { std::cerr << ""[WARNING] could not find registered function for op: "" << op << "" in function: "" << _demangle ( _name ( I -> getFunction ( ) ) ) << "" contextualNamespace: "" << contextualNamespace << std::endl ; } "
"static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( I -> getFunction ( ) -> getName ( ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; ","static std::string inferContextualNamespace ( Instruction * I ) { auto functionName = _demangle ( _name ( I -> getFunction ( ) ) ) ; for ( auto & pattern : TorchLibraryInitPattern ) { if ( attern . pattern -> match ( functionName ) ) { continue ; "
"static void printDebugValue ( Value * V ) { if ( auto F = dyn_cast<Function> ( V ) ) { std::cerr << ""[FUNC] "" << _demangle ( F -> getName ( ) ) ; } else if ( isa<Constant> ( V ) ) { std::cerr << ""[CONST] "" << * V ; } else if ( isa<Instruction> ( V ) ) { ","static void printDebugValue ( Value * V ) { if ( auto F = dyn_cast<Function> ( V ) ) { std::cerr << ""[FUNC] "" << _demangle ( _name ( F ) ) ; } else if ( isa<Constant> ( V ) ) { std::cerr << ""[CONST] "" << * V ; } else if ( isa<Instruction> ( V ) ) { "
"in0_t , dtype , layout , device , pin_memory , memory_format ) ; } auto & out_t = p_node -> Output ( 0 ) . toTensor ( ) ; at::native::fill_out ( out_t , in1_s ) ; } ; } ) ; ","in0_t , dtype , layout , device , pin_memory , memory_format ) ; } auto & out_t = p_node -> Output ( 0 ) . toTensor ( ) ; at::native::resize_ ( out_t , in0_t . sizes ( ) , c10::nullopt ) ; at::native::fill_out ( out_t , in1_s ) ; } ; } ) ; "
"const auto operand = operands [ i ] ; const auto labels = op_labels [ i ] ; const auto ndims = operand . dim ( ) ; int64_t nlabels = labels . size ( ) ; bool has_ellipsis = false ; for ( const auto & label : labels ) { ","const auto operand = operands [ i ] ; const auto labels = op_labels [ i ] ; const auto ndims = operand . dim ( ) ; int64_t nlabels = static_cast<int64_t> ( labels . size ( ) ) ; bool has_ellipsis = false ; for ( const auto & label : labels ) { "
"TORCH_CHECK ( operand . size ( j ) == operand . size ( dim ) , ""einsum(): subscript "" , char ( label + 'a' ) , "" is repeated for operand "" , i , "" but the sizes don't match, "" , ","TORCH_CHECK ( operand . size ( j ) == operand . size ( dim ) , ""einsum(): subscript "" , einsum_index_to_label ( label ) , "" is repeated for operand "" , i , "" but the sizes don't match, "" , "
"const auto self_sizes = self . sizes ( ) ; const auto total_nonzero = thread_count_nonzero . back ( ) ; const int64_t ndim = self_sizes . size ( ) ; resize_output ( result , { total_nonzero , ndim } ) ; if ( result . numel ( ) == 0 ) { return result ; ","const auto self_sizes = self . sizes ( ) ; const auto total_nonzero = thread_count_nonzero . back ( ) ; const int64_t ndim = self_sizes . size ( ) ; if ( resize_output ( result , { total_nonzero , ndim } ) ) { result . as_strided_ ( { total_nonzero , ndim } , { 1 , total_nonzero } ) ; } if ( result . numel ( ) == 0 ) { return result ; "
"( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , ","( options . layout ( ) == c10::kStrided ) ) ; if ( memory_format == MemoryFormat::Preserve ) { if ( self . is_non_overlapping_and_dense ( ) & & options . device ( ) . supports_as_strided ( ) ) { auto r = at::empty_strided ( self . sizes ( ) , self . strides ( ) , "
"CudaIPCSentData::CudaIPCSentData ( std::string handle , int64_t offset , int64_t * counter_ptr , at::Device device ) ","CudaIPCSentData::CudaIPCSentData ( const std::string & handle , int64_t offset , int64_t * counter_ptr , at::Device device ) "
"Tensor unbind_backward ( const variable_list & grads , int64_t dim ) { IntArrayRef sizes ; at::TensorOptions o ; for ( auto v : grads ) { if ( v . defined ( ) ) { sizes = v . sizes ( ) ; o = static_cast<Tensor> ( v ) . options ( ) ; ","Tensor unbind_backward ( const variable_list & grads , int64_t dim ) { IntArrayRef sizes ; at::TensorOptions o ; for ( const auto & v : grads ) { if ( v . defined ( ) ) { sizes = v . sizes ( ) ; o = static_cast<Tensor> ( v ) . options ( ) ; "
"bool any_variable_defined ( variable_list & variables ) { for ( auto variable : variables ) { if ( variable . defined ( ) ) { return true ; } ","bool any_variable_defined ( const variable_list & variables ) { for ( const auto & variable : variables ) { if ( variable . defined ( ) ) { return true ; } "
"auto start_j = tau . size ( -1 ) - 1 ; for ( int64_t j = start_j ; j >= 0 ; j - - ) { auto v = input_ . index ( { ""..."" , Slice ( ) , j } ) ; auto v1 = v , v2 = v ; auto tau_unsqueezed = tau . index ( { ""..."" , j } ) . unsqueeze ( -1 ) ; ","auto start_j = tau . size ( -1 ) - 1 ; for ( int64_t j = start_j ; j >= 0 ; j - - ) { const auto v = input_ . index ( { ""..."" , Slice ( ) , j } ) ; const auto & v1 = v ; const auto & v2 = v ; auto tau_unsqueezed = tau . index ( { ""..."" , j } ) . unsqueeze ( -1 ) ; "
"void operator ( ) ( int i ) { if ( cuda ) { int device_id = i % models_ . size ( ) ; auto d = torch::Device ( torch::DeviceType::CUDA , device_id ) ; to_device ( models_ [ device_id ] . forward ( to_device_vec ( eg_ , d ) ) , ","void operator ( ) ( int i ) { if ( cuda ) { const auto device_id = i % models_ . size ( ) ; auto d = torch::Device ( torch::DeviceType::CUDA , device_id ) ; to_device ( models_ [ device_id ] . forward ( to_device_vec ( eg_ , d ) ) , "
"unsigned previous = y - 1 ; for ( size_t x = 1 ; x <= n ; + + x ) { int old_row = row [ x ] ; row [ x ] = std::min ( previous + ( word1 [ y - 1 ] == word2 [ x - 1 ] 0 u : 1 u ) , std::min ( row [ x - 1 ] , row [ x ] ) + 1 ) ; ","unsigned previous = y - 1 ; for ( size_t x = 1 ; x <= n ; + + x ) { const auto old_row = row [ x ] ; row [ x ] = std::min ( previous + ( word1 [ y - 1 ] == word2 [ x - 1 ] 0 u : 1 u ) , std::min ( row [ x - 1 ] , row [ x ] ) + 1 ) ; "
"for ( const IValue & v : stack ) { if ( v . isTensor ( ) ) { at::Tensor t = v . toTensor ( ) ; if ( t . defined ( ) & & t . requires_grad ( ) ) { return c10::nullopt ; ","for ( const IValue & v : stack ) { if ( v . isTensor ( ) ) { const at::Tensor & t = v . toTensor ( ) ; if ( t . defined ( ) & & t . requires_grad ( ) ) { return c10::nullopt ; "
"auto size = tuple PyTuple_GET_SIZE ( obj ) : PyList_GET_SIZE ( obj ) ; if ( size == 0 ) { return true ; } ","const auto size = tuple PyTuple_GET_SIZE ( obj ) : PyList_GET_SIZE ( obj ) ; if ( size == 0 ) { return true ; } "
"KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; kernel -> kernel_objects_ . push_back ( this ) ; } ","KernelScopedObject::KernelScopedObject ( ) { KernelArena * kernel = KernelArena::GetCurrentKernelArena ( ) ; if ( kernel == nullptr ) { throw std::runtime_error ( ""KernelScope() must be constructed before calling this"" ) ; } kernel -> kernel_objects_ . push_back ( this ) ; } "
"if ( v . isTensor ( ) ) { v = IValue ( detach ( std::move ( v ) . toTensor ( ) ) ) ; } else if ( v . isTensorList ( ) ) { c10::List<at::Tensor> lst = std::move ( v ) . toTensorList ( ) ; for ( size_t i = 0 ; i < lst . size ( ) ; + + i ) { lst . set ( i , detach ( lst . extract ( i ) ) ) ; } v = std::move ( lst ) ; } ","if ( v . isTensor ( ) ) { v = IValue ( detach ( std::move ( v ) . toTensor ( ) ) ) ; } else if ( v . isTensorList ( ) ) { std::vector<at::Tensor> lst = v . toTensorVector ( ) ; for ( auto & tensor : lst ) { tensor = detach ( tensor ) ; } v = std::move ( lst ) ; } "
"cleanupKey_ ( ""cleanup/"" ) , regularPrefix_ ( ""/"" ) { if ( numWorkers_ < 1 ) { throw std::runtime_error ( ""Number of workers for FileStore should be greater than zero"" ) ; } } ","cleanupKey_ ( ""cleanup/"" ) , regularPrefix_ ( ""/"" ) { if ( numWorkers_ < 1 ) { TORCH_CHECK ( false , ""Number of workers for FileStore should be greater than zero"" ) ; } } "
"const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout & & elapsed > timeout ) { throw std::runtime_error ( ""Wait timeout"" ) ; } ","const auto elapsed = std::chrono::duration_cast<std::chrono::seconds> ( std::chrono::steady_clock::now ( ) - start ) ; if ( timeout != kNoTimeout & & elapsed > timeout ) { TORCH_CHECK ( false , ""Wait timeout"" ) ; } "
"makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , """" ) ; if ( evice ) { throw std::runtime_error ( ""makeDeviceForInterface(): unsupported gloo device"" ) ; } return device ; } ","makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , """" ) ; if ( evice ) { TORCH_CHECK ( false , ""makeDeviceForInterface(): unsupported gloo device"" ) ; } return device ; } "
"makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { throw std::runtime_error ( ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } ","makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } "
"if ( ompleted_ ) { throw std::runtime_error ( ""Operation timed out!"" ) ; } } if ( exception_ ) { ","if ( ompleted_ ) { TORCH_CHECK ( false , ""Operation timed out!"" ) ; } } if ( exception_ ) { "
"collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { throw std::runtime_error ( ""No device(s) specified"" ) ; } ","collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , ""No device(s) specified"" ) ; } "
"invalidArgument ( ""unsupported layout"" ) ; } } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","invalidArgument ( ""unsupported layout"" ) ; } } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"at::Tensor & , at::Tensor & , const AllgatherOptions & ) { throw std::runtime_error ( ""no support for _allgather_base in Gloo process group"" ) ; } ","at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , ""no support for _allgather_base in Gloo process group"" ) ; } "
"work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { throw std::runtime_error ( ""ProcessGroupGloo does not support reduce_scatter"" ) ; } ","std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , ""ProcessGroupGloo does not support reduce_scatter"" ) ; } "
"for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) | | ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { throw std::runtime_error ( ""Tensors are not equal in size or data type"" ) ; } checkSingleTensorHelper ( tensor ) ; } ","for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) | | ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , ""Tensors are not equal in size or data type"" ) ; } checkSingleTensorHelper ( tensor ) ; } "
"bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } ","bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } "
"MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 | | size < 0 ) { throw std::runtime_error ( ""Failed to get the world_size / rank"" ) ; } } } ","MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 | | size < 0 ) { TORCH_CHECK ( false , ""Failed to get the world_size / rank"" ) ; } } } "
"bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { throw std::runtime_error ( ""Not able to create/get the NCCL Communicator since "" ""the GPU devices are not known"" ) ; } ","bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , ""Not able to create/get the NCCL Communicator since "" ""the GPU devices are not known"" ) ; } "
"for ( auto i = size_t { } ; i < num_devices ; + + i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } ","for ( auto i = size_t { } ; i < num_devices ; + + i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } "
"void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { throw std::runtime_error ( ""Failed to create the control pipe to start the "" ""BackgroundThread run"" ) ; } ","void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { TORCH_CHECK ( false , ""Failed to create the control pipe to start the "" ""BackgroundThread run"" ) ; } "
"listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return listenPort ; } ","listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return listenPort ; } "
"__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return address ; } ","__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return address ; } "
"cleanupKey_ ( ""cleanup/"" ) , regularPrefix_ ( ""/"" ) { if ( numWorkers_ < 1 ) { TORCH_CHECK ( false , ""Number of workers for FileStore should be greater than zero"" ) ; } } ","cleanupKey_ ( ""cleanup/"" ) , regularPrefix_ ( ""/"" ) { if ( numWorkers_ < 1 ) { throw std::runtime_error ( ""Number of workers for FileStore should be greater than zero"" ) ; } } "
"makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } ","makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { throw std::runtime_error ( ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } "
"invalidArgument ( ""unsupported layout"" ) ; } } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; ","invalidArgument ( ""unsupported layout"" ) ; } } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; "
"invalidArgument ( ""unsupported layout"" ) ; } } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","invalidArgument ( ""unsupported layout"" ) ; } } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) | | ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , ""Tensors are not equal in size or data type"" ) ; } checkSingleTensorHelper ( tensor ) ; } ","for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) | | ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { throw std::runtime_error ( ""Tensors are not equal in size or data type"" ) ; } checkSingleTensorHelper ( tensor ) ; } "
"bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } ","bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } "
"MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 | | size < 0 ) { TORCH_CHECK ( false , ""Failed to get the world_size / rank"" ) ; } } } ","MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 | | size < 0 ) { throw std::runtime_error ( ""Failed to get the world_size / rank"" ) ; } } } "
"bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , ""Not able to create/get the NCCL Communicator since "" ""the GPU devices are not known"" ) ; } ","bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { throw std::runtime_error ( ""Not able to create/get the NCCL Communicator since "" ""the GPU devices are not known"" ) ; } "
"for ( auto i = size_t { } ; i < num_devices ; + + i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } ","for ( auto i = size_t { } ; i < num_devices ; + + i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } "
"c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::allreduce_coalesced ( std::vector<at::Tensor> & tensors , const AllreduceCoalescedOptions & opts ) { TORCH_CHECK ( false , ""allreduce_coalesced is currently not supported with NCCL"" ) ; } ","c10::intrusive_ptr<ProcessGroup::Work> ProcessGroupNCCL::allreduce_coalesced ( std::vector<at::Tensor> & tensors , const AllreduceCoalescedOptions & opts ) { throw std::runtime_error ( ""allreduce_coalesced is currently not supported with NCCL"" ) ; } "
"std::vector<at::Tensor> & , std::vector<at::Tensor> & , const AllToAllOptions & ) { TORCH_CHECK ( false , ""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"" ) ; } ","std::vector<at::Tensor> & , std::vector<at::Tensor> & , const AllToAllOptions & ) { throw std::runtime_error ( ""ProcessGroupNCCL only supports alltoall* for NCCL lib version >= 2.7.0"" ) ; } "
"watchHandler ( socket ) ; } else { TORCH_CHECK ( false , ""Unexpected query type"" ) ; } } ","watchHandler ( socket ) ; } else { throw std::runtime_error ( ""Unexpected query type"" ) ; } } "
"if ( response == detail::CheckResponseType::NOT_READY ) { return false ; } TORCH_CHECK ( false , ""ready or not_ready response expected"" ) ; } void TCPStore::wait ( const std::vector<std::string> & keys ) { ","if ( response == detail::CheckResponseType::NOT_READY ) { return false ; } throw std::runtime_error ( ""ready or not_ready response expected"" ) ; } void TCPStore::wait ( const std::vector<std::string> & keys ) { "
"auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { TORCH_CHECK ( false , ""Stop_waiting response is expected"" ) ; } } ","auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { throw std::runtime_error ( ""Stop_waiting response is expected"" ) ; } } "
"listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return listenPort ; } ","listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return listenPort ; } "
"__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return address ; } ","__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return address ; } "
"while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , ""waiting for processes to "" ""connect has timed out"" ) ; } else if ( res == -1 ) { ","while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { throw std::runtime_error ( ""waiting for processes to "" ""connect has timed out"" ) ; } else if ( res == -1 ) { "
"pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { TORCH_CHECK ( false , ""The specified group name has already been "" ""created, please use a different group name"" ) ; } ","pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { throw std::runtime_error ( ""The specified group name has already been "" ""created, please use a different group name"" ) ; } "
"void setMemoryFraction ( double fraction , int device ) { TORCH_INTERNAL_ASSERT ( 0 <= device & & device < device_allocator . size ( ) , ""Allocator not initialized for device "" , device , "": did you call init?"" ) ; ","void setMemoryFraction ( double fraction , int device ) { TORCH_INTERNAL_ASSERT ( 0 <= device & & static_cast<size_t> ( device ) < device_allocator . size ( ) , ""Allocator not initialized for device "" , device , "": did you call init?"" ) ; "
"void emptyCache ( ) { int count = device_allocator . size ( ) ; for ( int i = 0 ; i < count ; i + + ) device_allocator [ i ] -> emptyCache ( ) ; } ","void emptyCache ( ) { for ( auto & da : device_allocator ) da -> emptyCache ( ) ; } "
"std::vector<SegmentInfo> snapshot ( ) { std::vector<SegmentInfo> result ; int count = device_allocator . size ( ) ; for ( int i = 0 ; i < count ; i + + ) { auto snap = device_allocator [ i ] -> snapshot ( ) ; result . insert ( result . end ( ) , snap . begin ( ) , snap . end ( ) ) ; } ","std::vector<SegmentInfo> snapshot ( ) { std::vector<SegmentInfo> result ; for ( auto & da : device_allocator ) { auto snap = da -> snapshot ( ) ; result . insert ( result . end ( ) , snap . begin ( ) , snap . end ( ) ) ; } "
"static inline void assertValidDevice ( int device ) { int device_num = caching_allocator . device_allocator . size ( ) ; TORCH_CHECK ( 0 <= device & & device < device_num , ""Invalid device argument."" ) ; } ","static inline void assertValidDevice ( int device ) { const auto device_num = caching_allocator . device_allocator . size ( ) ; TORCH_CHECK ( 0 <= device & & device < device_num , ""Invalid device argument."" ) ; } "
"cleanupKey_ ( ""cleanup/"" ) , regularPrefix_ ( ""/"" ) { if ( numWorkers_ < 1 ) { throw std::runtime_error ( ""Number of workers for FileStore should be greater than zero"" ) ; } } ","cleanupKey_ ( ""cleanup/"" ) , regularPrefix_ ( ""/"" ) { if ( numWorkers_ < 1 ) { TORCH_CHECK ( false , ""Number of workers for FileStore should be greater than zero"" ) ; } } "
"makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , """" ) ; if ( evice ) { throw std::runtime_error ( ""makeDeviceForInterface(): unsupported gloo device"" ) ; } return device ; } ","makeDeviceForInterface ( const std::string & interfaceName ) { auto device = makeGlooDevice ( interfaceName , """" ) ; if ( evice ) { TORCH_CHECK ( false , ""makeDeviceForInterface(): unsupported gloo device"" ) ; } return device ; } "
"makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { throw std::runtime_error ( ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } ","makeDeviceForHostname ( const std::string & hostname ) { auto device = makeGlooDevice ( """" , hostname ) ; if ( evice ) { TORCH_CHECK ( false , ""makeDeviceForHostname(): unsupported gloo device"" ) ; } return device ; } "
"collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { throw std::runtime_error ( ""No device(s) specified"" ) ; } ","collectiveCounter_ ( 0 ) { auto & devices = options -> devices ; if ( devices . empty ( ) ) { TORCH_CHECK ( false , ""No device(s) specified"" ) ; } "
"work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncAllgatherCUDAWork> ( std::move ( context ) , outputs , inputs , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"at::Tensor & , at::Tensor & , const AllgatherOptions & ) { throw std::runtime_error ( ""no support for _allgather_base in Gloo process group"" ) ; } ","at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , ""no support for _allgather_base in Gloo process group"" ) ; } "
"work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncGatherCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { throw std::runtime_error ( ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; ","work = c10::make_intrusive<AsyncScatterCUDAWork> ( std::move ( context ) , outputs , inputs , opts . rootRank , tag ) ; } else { TORCH_CHECK ( false , ""Invalid backend"" ) ; } enqueue ( work ) ; return work ; "
"std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { throw std::runtime_error ( ""ProcessGroupGloo does not support reduce_scatter"" ) ; } namespace { ","std::vector<at::Tensor> & outputs , std::vector < std::vector<at::Tensor> > & inputs , const ReduceScatterOptions & opts ) { TORCH_CHECK ( false , ""ProcessGroupGloo does not support reduce_scatter"" ) ; } namespace { "
"for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) | | ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { throw std::runtime_error ( ""Tensors are not equal in size or data type"" ) ; } checkSingleTensorHelper ( tensor ) ; } ","for ( const auto & tensor : tensors ) { if ( ( tensor . numel ( ) != t_in . numel ( ) ) | | ( tensor . scalar_type ( ) != t_in . scalar_type ( ) ) ) { TORCH_CHECK ( false , ""Tensors are not equal in size or data type"" ) ; } checkSingleTensorHelper ( tensor ) ; } "
"bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { throw std::runtime_error ( ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } ","bool ProcessGroupMPI::AsyncWork::isSuccess ( ) const { if ( request_ != MPI_REQUEST_NULL ) { TORCH_CHECK ( false , ""Invalid call to AsyncWork::isSuccess before work has completed"" ) ; } "
"MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 | | size < 0 ) { throw std::runtime_error ( ""Failed to get the world_size / rank"" ) ; } } } ","MPI_CHECK ( MPI_Comm_size ( groupComm , & size ) ) ; if ( rank < 0 | | size < 0 ) { TORCH_CHECK ( false , ""Failed to get the world_size / rank"" ) ; } } } "
"std::vector < std::vector<at::Tensor> > & , std::vector<at::Tensor> & , const AllgatherOptions & ) { throw std::runtime_error ( ""ProcessGroupMPI does not support allgather_coalesced"" ) ; } ","std::vector < std::vector<at::Tensor> > & , std::vector<at::Tensor> & , const AllgatherOptions & ) { TORCH_CHECK ( false , ""ProcessGroupMPI does not support allgather_coalesced"" ) ; } "
"at::Tensor & , at::Tensor & , const AllgatherOptions & ) { throw std::runtime_error ( ""no support for _allgather_base in MPI process group"" ) ; } ","at::Tensor & , at::Tensor & , const AllgatherOptions & ) { TORCH_CHECK ( false , ""no support for _allgather_base in MPI process group"" ) ; } "
"bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { throw std::runtime_error ( ""Not able to create/get the NCCL Communicator since "" ""the GPU devices are not known"" ) ; } ","bool isSendRecvSelf ) { if ( devicesKey . empty ( ) ) { TORCH_CHECK ( false , ""Not able to create/get the NCCL Communicator since "" ""the GPU devices are not known"" ) ; } "
"for ( auto i = size_t { } ; i < num_devices ; + + i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { throw std::runtime_error ( ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } ","for ( auto i = size_t { } ; i < num_devices ; + + i ) { if ( tensor_lists [ i ] . size ( ) != world_size * num_devices ) { TORCH_CHECK ( false , ""Tensor list input to scatter/gather must match number of collective"" "" participants"" ) ; } "
"std::vector<at::Tensor> & , int , int ) { throw std::runtime_error ( ""ProcessGroupNCCL only supports send for NCCL lib version >= 2.7.0"" ) ; } ","std::vector<at::Tensor> & , int , int ) { TORCH_CHECK ( false , ""ProcessGroupNCCL only supports send for NCCL lib version >= 2.7.0"" ) ; } "
"void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { throw std::runtime_error ( ""Failed to create the control pipe to start the "" ""BackgroundThread run"" ) ; } ","void BackgroundThread::initStopSignal ( ) { ghStopEvent_ = CreateEvent ( NULL , TRUE , FALSE , NULL ) ; if ( ghStopEvent_ == NULL ) { TORCH_CHECK ( false , ""Failed to create the control pipe to start the "" ""BackgroundThread run"" ) ; } "
"watchHandler ( socket ) ; } else { throw std::runtime_error ( ""Unexpected query type"" ) ; } } ","watchHandler ( socket ) ; } else { TORCH_CHECK ( false , ""Unexpected query type"" ) ; } } "
"auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { throw std::runtime_error ( ""Stop_waiting response is expected"" ) ; } } ","auto response = client_ -> receiveValue<detail::WaitResponseType> ( ) ; if ( response != detail::WaitResponseType::STOP_WAITING ) { TORCH_CHECK ( false , ""Stop_waiting response is expected"" ) ; } } "
"listenPort = ntohs ( addr -> sin6_port ) ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return listenPort ; } ","listenPort = ntohs ( addr -> sin6_port ) ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return listenPort ; } "
"__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { throw std::runtime_error ( ""unsupported protocol"" ) ; } return address ; } ","__output != nullptr ) address [ INET6_ADDRSTRLEN ] = '0' ; } else { TORCH_CHECK ( false , ""unsupported protocol"" ) ; } return address ; } "
"if ( timeout != kNoTimeout ) { const auto elapsed = std::chrono::high_resolution_clock::now ( ) - start ; if ( elapsed > timeout ) { throw std::runtime_error ( kConnectTimeoutMsg ) ; } } std::this_thread::sleep_for ( std::chrono::seconds ( 1 ) ) ; ","if ( timeout != kNoTimeout ) { const auto elapsed = std::chrono::high_resolution_clock::now ( ) - start ; if ( elapsed > timeout ) { TORCH_CHECK ( false , kConnectTimeoutMsg ) ; } } std::this_thread::sleep_for ( std::chrono::seconds ( 1 ) ) ; "
"while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { throw std::runtime_error ( ""waiting for processes to "" ""connect has timed out"" ) ; } else if ( res == -1 ) { ","while ( true ) { int res = tcputil::poll ( events . get ( ) , 1 , timeout . count ( ) ) ; if ( res == 0 ) { TORCH_CHECK ( false , ""waiting for processes to "" ""connect has timed out"" ) ; } else if ( res == -1 ) { "
"pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { throw std::runtime_error ( ""The specified group name has already been "" ""created, please use a different group name"" ) ; } ","pg_name ) { return pg_name . second == * group_name ; } ) ; if ( it != pg_names_ . end ( ) ) { TORCH_CHECK ( false , ""The specified group name has already been "" ""created, please use a different group name"" ) ; } "
"try { value = torch::jit::toIValue ( obj , c10::TensorType::get ( ) ) ; } catch ( py::cast_error & e ) { throw std::runtime_error ( ""RRef should contain a tensor for .backward()"" ) ; } } ","try { value = torch::jit::toIValue ( obj , c10::TensorType::get ( ) ) ; } catch ( py::cast_error & e ) { TORCH_CHECK ( false , ""RRef should contain a tensor for .backward()"" ) ; } } "
"if ( jitFuture . hasError ( ) ) { auto errMsg = jitFuture . tryRetrieveErrorMessage ( ) ; VLOG ( 1 ) << ""Got exception: "" << errMsg ; throw std::runtime_error ( errMsg ) ; } } ","if ( jitFuture . hasError ( ) ) { auto errMsg = jitFuture . tryRetrieveErrorMessage ( ) ; VLOG ( 1 ) << ""Got exception: "" << errMsg ; TORCH_CHECK ( false , errMsg ) ; } } "
"""tried to send() a message of type "" , requestMessage -> type ( ) , "" but RPC is no longer running on this node."" ) ; throw std::runtime_error ( err ) ; } const auto & url = findWorkerURL ( toWorkerInfo ) ; ","""tried to send() a message of type "" , requestMessage -> type ( ) , "" but RPC is no longer running on this node."" ) ; TORCH_CHECK ( false , err ) ; } const auto & url = findWorkerURL ( toWorkerInfo ) ; "
"ptr += headerEnt . second ; } if ( ptr != endp ) { throw std::runtime_error ( ""failed bounds"" ) ; } return out ; } ","ptr += headerEnt . second ; } if ( ptr != endp ) { TORCH_CHECK ( false , ""failed bounds"" ) ; } return out ; } "
