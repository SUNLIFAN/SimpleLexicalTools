"BUGGY_CODE","FIXED_CODE"
"def __sizeof__ ( self ) : return super ( _StorageBase , self ) . __sizeof__ ( ) + self . element_size ( ) * self . size ( ) ","def __sizeof__ ( self ) : return ( super ( _StorageBase , self ) . __sizeof__ ( ) + self . element_size ( ) * self . size ( ) ) "
"self . quant_min : int = quant_min self . quant_max : int = quant_max self . register_buffer ( ""fake_quant_enabled"" , torch . tensor ( [ 0 ] , dtype = torch . long ) ) self . register_buffer ( ""observer_enabled"" , torch . tensor ( [ 0 ] , dtype = torch . long ) ) self . is_symmetric_quant = _is_symmetric_quant ( self . activation_post_process . qscheme ) ","self . quant_min : int = quant_min self . quant_max : int = quant_max self . register_buffer ( ""fake_quant_enabled"" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . register_buffer ( ""observer_enabled"" , torch . tensor ( [ 1 ] , dtype = torch . long ) ) self . is_symmetric_quant = _is_symmetric_quant ( self . activation_post_process . qscheme ) "
"def get_default_qat_qconfig ( backend = gemm version = None ) : Histogram observer is too slow for quantization aware training if version is None : if backend == gemm ","def get_default_qat_qconfig ( backend = gemm version = 1 ) : Histogram observer is too slow for quantization aware training if version is None : if backend == gemm "
"exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad , value = 1 - beta2 ) if amsgrad : ","exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = 1 - beta1 ) exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad . conj ( ) , value = 1 - beta2 ) if amsgrad : "
"def get_post_build_suffix ( self ) -> str : if self . gpu_arch_type == ""cuda"" : return f ""+cu{self.gpu_arch_version.replace('.', '')}"" return f ""+{self.gpu_arch_type}{self.gpu_arch_version}"" ","def get_post_build_suffix ( self ) -> str : if self . no_build_suffix : return """" if self . gpu_arch_type == ""cuda"" : return f ""+cu{self.gpu_arch_version.replace('.', '')}"" return f ""+{self.gpu_arch_type}{self.gpu_arch_version}"" "
"torch . _C . _log_api_usage_once ( ""quantization_api._numeric_suite_fx.extract_weights"" ) base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops ( ) type_a_related_to_b = get_type_a_related_to_b ( base_name_to_sets_of_related_ops ) ","torch . _C . _log_api_usage_once ( ""quantization_api._numeric_suite_fx.extract_weights"" ) if base_name_to_sets_of_related_ops is None : base_name_to_sets_of_related_ops = get_base_name_to_sets_of_related_ops ( ) type_a_related_to_b = get_type_a_related_to_b ( base_name_to_sets_of_related_ops ) "
"def inner ( * args , * * kwargs ) : a0 , a1 , * a_other = args if isinstance ( a0 , tuple ) and isinstance ( a1 , tuple ) : results = [ ] for el0 , el1 in zip ( a0 , a1 ) : new_args = ( el0 , el1 , * a_other ) ","def inner ( * args , * * kwargs ) : a0 , a1 , * a_other = args if ( isinstance ( a0 , tuple ) and isinstance ( a1 , tuple ) ) or ( isinstance ( a0 , list ) and isinstance ( a1 , list ) ) : results = [ ] for el0 , el1 in zip ( a0 , a1 ) : new_args = ( el0 , el1 , * a_other ) "
"if node . op == tput rv = map_arg ( node . args [ 0 ] , lambda n : val_map [ n ] ) return rv val_map [ node ] = self . node_copy ( node , lambda n : val_map [ n ] ) return None ","if node . op == tput rv = map_arg ( node . args [ 0 ] , lambda n : val_map [ n ] ) return rv if not return_output_node else ( rv , node ) val_map [ node ] = self . node_copy ( node , lambda n : val_map [ n ] ) return None "
"g = Graph ( ) output_val = g . graph_copy ( self , val_map = memo ) g . output ( output_val ) return g ","g = Graph ( ) output_vals = g . graph_copy ( self , val_map = memo , return_output_node = True ) assert isinstance ( output_vals , tuple ) output_val , old_output_val = output_vals g . output ( output_val , type_expr = getattr ( old_output_val , pe None ) ) return g "
"def _handle_reduce_dim_none ( g , self , op_name ) : dim_size = _get_tensor_dim_size ( self , 0 ) rank = _get_tensor_rank ( self ) if rank is not None and any ( [ _get_tensor_dim_size ( self , i ) == 0 for i in range ( rank ) ] ) : ","def _handle_reduce_dim_none ( g , self , op_name ) : rank = _get_tensor_rank ( self ) if rank is not None and any ( [ _get_tensor_dim_size ( self , i ) == 0 for i in range ( rank ) ] ) : "
"self . module_groups = [ ] self . enable_mask_update = False self . activation_handle = None self . bias_handle = None self . model = model ","self . module_groups = [ ] self . enable_mask_update = False self . activation_handles = [ ] self . bias_handles = [ ] self . model = model "
"def __getitem__ ( self , idx ) : return self . dataset [ self . indices [ idx ] ] ","def __getitem__ ( self , idx ) : if isinstance ( idx , list ) : return self . dataset [ [ self . indices [ i ] for i in idx ] ] return self . dataset [ self . indices [ idx ] ] "
"self . register_parameter ( _proj_bias None ) self . out_proj = Linear ( embed_dim , embed_dim , bias = bias , * * factory_kwargs ) if add_bias_kv : self . bias_k = Parameter ( torch . empty ( ( 1 , 1 , embed_dim ) , * * factory_kwargs ) ) ","self . register_parameter ( _proj_bias None ) self . out_proj = NonDynamicallyQuantizableLinear ( embed_dim , embed_dim , bias = bias , * * factory_kwargs ) if add_bias_kv : self . bias_k = Parameter ( torch . empty ( ( 1 , 1 , embed_dim ) , * * factory_kwargs ) ) "
"if right is Any or left == right : return True if right == type ( None ) : return False ","if right is Any or left == right : return True if isinstance ( right , _GenericAlias ) : if getattr ( right , origin__ None ) is Generic : return True if right == type ( None ) : return False "
"def __eq__ ( self , other ) : if isinstance ( other , _DataPipeType ) : return self . issubtype ( other ) and other . issubtype ( self ) return NotImplemented ","def __eq__ ( self , other ) : if isinstance ( other , _DataPipeType ) : return self . param == other . param return NotImplemented "
"def issubtype ( self , other ) : if isinstance ( other , _DataPipeType ) : return issubtype ( self . param , other . param ) ","def issubtype ( self , other ) : if isinstance ( other . param , _GenericAlias ) : if getattr ( other . param , origin__ None ) is Generic : return True if isinstance ( other , _DataPipeType ) : return issubtype ( self . param , other . param ) "
"if torch . cuda . is_available ( ) : torch . cuda . synchronize ( ) ","if self . use_cuda : torch . cuda . synchronize ( ) "
"def get_type ( arg ) : if isinstance ( arg , fx . Proxy ) : old_meta = self . node_map [ arg ] . meta return old_meta [ pe if pe n old_meta else None return create_type_hint ( arg ) arg_types = tuple ( [ get_type ( arg ) for arg in args ] ) ","def get_type ( arg ) : if isinstance ( arg , fx . Node ) : return n . meta [ pe if pe n n . meta else None return type ( arg ) arg_types = map_aggregate ( n . args , get_type ) assert ( isinstance ( arg_types , tuple ) ) arg_types = tuple ( [ create_type_hint ( i ) for i in arg_types ] ) "
"if found_tensor : n . meta [ nsor_meta = meta n . meta [ pe = create_type_hint ( result ) return result ","if found_tensor : n . meta [ nsor_meta = meta n . meta [ pe = type ( result ) return result "
"fan_in , _ = init . _calculate_fan_in_and_fan_out ( self . weight ) bound = 1 / math . sqrt ( fan_in ) init . uniform_ ( self . bias , - bound , bound ) ","fan_in , _ = init . _calculate_fan_in_and_fan_out ( self . weight ) bound = 1 / math . sqrt ( fan_in ) if fan_in > 0 else 0 init . uniform_ ( self . bias , - bound , bound ) "
"if str ( t . elem ) == nsor if mutable : return NamedCType ( binds , MutRefCType ( BaseCType ( tensorT ) ) ) ","if str ( t . elem ) == nsor if mutable and not local . use_const_ref_for_mutable_tensors ( ) : return NamedCType ( binds , MutRefCType ( BaseCType ( tensorT ) ) ) "
"tensor_type : OptionalCType = OptionalCType ( BaseCType ( tensorT ) ) if mutable : return NamedCType ( binds , MutRefCType ( tensor_type ) ) ","tensor_type : OptionalCType = OptionalCType ( BaseCType ( tensorT ) ) if mutable and not local . use_const_ref_for_mutable_tensors ( ) : return NamedCType ( binds , MutRefCType ( tensor_type ) ) "
"if len ( quant_uses ) == 1 : quantized . graph . erase_node ( node ) for arg in quant_args [ 1 : ] : quantized . graph . erase_node ( arg ) return quantized ","if len ( quant_uses ) == 1 : quantized . graph . erase_node ( node ) for arg in quant_args [ 1 : ] : if isinstance ( arg , Node ) : quantized . graph . erase_node ( arg ) return quantized "
"if norm_type == inf : total_norm = max ( p . grad . detach ( ) . abs ( ) . max ( ) . to ( device ) for p in parameters ) ","if norm_type == inf : norms = [ p . grad . detach ( ) . abs ( ) . max ( ) . to ( device ) for p in parameters ] total_norm = norms [ 0 ] if len ( norms ) == 1 else torch . max ( torch . stack ( norms ) ) "
"guard_field = 0 : : cuda::OptionalCUDAGuard guard_ ; elif self . dispatch_key == DispatchKey . DefaultBackend : ","guard_field = 0 : : cuda::OptionalCUDAGuard guard_ ; elif self . dispatch_key == DispatchKey . CompositeExplicitAutograd : "
"else : observed . linear_Q = other . q_proj_weight observed . linear_K = other . k_proj_weight observed . linear_V = other . v_proj_weight if other . in_proj_bias is None : ","else : observed . linear_Q . weight = nn . Parameter ( other . q_proj_weight ) observed . linear_K . weight = nn . Parameter ( other . k_proj_weight ) observed . linear_V . weight = nn . Parameter ( other . v_proj_weight ) if other . in_proj_bias is None : "
"target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device ","if ( target_device == u ctx . target_device = u else : target_device = _get_device_index ( target_device , True ) ctx . target_device = target_device "
"assert key . size ( 0 ) == value . size ( 0 ) and key . size ( 1 ) == value . size ( 1 ) head_dim = embed_dim assert head_dim * num_heads == embed_dim , ""embed_dim must be divisible by num_heads"" scaling = float ( head_dim ) * * -0 . 5 ","assert key . size ( 0 ) == value . size ( 0 ) and key . size ( 1 ) == value . size ( 1 ) if isinstance ( embed_dim , torch . Tensor ) : head_dim = embed_dim . div ( num_heads , rounding_mode = unc else : head_dim = embed_dim assert head_dim * num_heads == embed_dim , ""embed_dim must be divisible by num_heads"" scaling = float ( head_dim ) * * -0 . 5 "
"elif str ( t ) == nsor [ ] return BaseCType ( nst c10::List < c10::optional<Tensor> > & binds ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) ","elif str ( t ) == nsor [ ] return ConstRefCType ( BaseCType ( ""c10::List<c10::optional<Tensor>>"" , binds ) ) return cpp . argumenttype_type ( t , mutable = mutable , binds = binds ) "
"Y = torch . rand ( N , 1 , device = device ) Predefined nu_alpha and nu_beta , nu_alpha . shape : ( 1 , 1 ) , nu_beta . shape : ( 1 , 1 ) nu_alpha = torch . randn ( 1 , 1 , device = device ) nu_beta = torch . rand ( 1 , 1 , device = device ) nu = dist . Gamma ( nu_alpha , nu_beta ) ","Y = torch . rand ( N , 1 , device = device ) Predefined nu_alpha and nu_beta , nu_alpha . shape : ( 1 , 1 ) , nu_beta . shape : ( 1 , 1 ) nu_alpha = torch . rand ( 1 , 1 , device = device ) nu_beta = torch . rand ( 1 , 1 , device = device ) nu = dist . Gamma ( nu_alpha , nu_beta ) "
"def icdf ( self , value ) : if self . _validate_args : self . _validate_sample ( value ) return torch . tan ( math . pi * ( value - 0 . 5 ) ) * self . scale + self . loc ","def icdf ( self , value ) : return torch . tan ( math . pi * ( value - 0 . 5 ) ) * self . scale + self . loc "
"elif node . op == tput if node . type is not None : maybe_return_annotation = f "" -> {type_repr(node.type)}"" body . append ( f turn { repr ( node . args [ 0 ] ) } return ","elif node . op == tput if node . type is not None : maybe_return_annotation [ 0 ] = f "" -> {type_repr(node.type)}"" body . append ( f turn { repr ( node . args [ 0 ] ) } return "
"else : args . extend ( func . arguments . out ) args . extend ( func . arguments . non_out ) return [ r for arg in args for r in argument ( arg ) ] ","else : args . extend ( func . arguments . out ) args . extend ( func . arguments . non_out ) return [ r for arg in args for r in argument ( arg , is_out = func . is_out_fn ( ) ) ] "
"if self . dispatch_key not in f . dispatch : return None op_name = f ""aten::{f.func.name}"" if self . target is Target . REGISTRATION and not self . selector . is_operator_selected ( op_name ) : ","if self . dispatch_key not in f . dispatch : return None if f . manual_kernel_registration : return None op_name = f ""aten::{f.func.name}"" if self . target is Target . REGISTRATION and not self . selector . is_operator_selected ( op_name ) : "
"def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if f . manual_kernel_registration : return None if Variant . function not in f . variants : return None ","def __call__ ( self , f : NativeFunction ) -> Optional [ str ] : if Variant . function not in f . variants : return None "
"torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) return [ input_tensor ] ","torch . matmul ( p , q . t ( ) , out = tensor ) assert not torch . any ( torch . isnan ( tensor ) ) if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor return [ input_tensor ] "
"out_args : List [ Dict [ str , Any ] ] = [ ] for arg in f . func . arguments . positional : if arg . default is not None : continue ","out_args : List [ Dict [ str , Any ] ] = [ ] for arg in f . func . arguments . flat_positional : if arg . default is not None : continue "
"def arguments ( func : FunctionSchema ) -> Sequence [ MetaArgument ] : assert not func . arguments . out return list ( map ( argument , itertools . chain ( func . arguments . positional , func . arguments . kwarg_only ) ) ) ","def arguments ( func : FunctionSchema ) -> Sequence [ MetaArgument ] : assert not func . arguments . out return list ( map ( argument , func . arguments . flat_non_out ) ) "
"args = tuple ( a for a in cpp_args if isinstance ( a , Argument ) ) input_arg_set = set ( a . name for a in f . func . arguments . positional ) kwarg_only_set = set ( a . name for a in f . func . arguments . kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) ","args = tuple ( a for a in cpp_args if isinstance ( a , Argument ) ) input_arg_set = set ( a . name for a in f . func . arguments . flat_positional ) kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) "
"kwarg_only_set = set ( a . name for a in f . func . arguments . kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) sig_group = CppSignatureGroup . from_schema ( f . func , method = False ) ","kwarg_only_set = set ( a . name for a in f . func . arguments . flat_kwarg_only ) out_arg_set = set ( a . name for a in f . func . arguments . out ) sig_group = CppSignatureGroup . from_schema ( f . func , method = False ) "
"out_and_self = list ( self . arguments . out ) + [ arg for arg in self . arguments . positional if arg . name == ""self"" ] mutable_returns = [ ret for ret in self . returns if ret . annotation is not None and ret . annotation . is_write ] ","out_and_self = list ( self . arguments . out ) + [ arg for arg in self . arguments . flat_positional if arg . name == ""self"" ] mutable_returns = [ ret for ret in self . returns if ret . annotation is not None and ret . annotation . is_write ] "
"def __str__ ( self ) -> str : all_arguments : List [ str ] = [ ] all_arguments . extend ( map ( str , self . positional ) ) if self . kwarg_only or self . out : all_arguments . append ( '*' ) all_arguments . extend ( map ( str , self . kwarg_only ) ) all_arguments . extend ( map ( str , self . out ) ) return oin ( all_arguments ) ","def __str__ ( self ) -> str : all_arguments : List [ str ] = [ ] all_arguments . extend ( map ( str , self . flat_positional ) ) if self . flat_kwarg_only or self . out : all_arguments . append ( '*' ) all_arguments . extend ( map ( str , self . flat_kwarg_only ) ) all_arguments . extend ( map ( str , self . out ) ) return oin ( all_arguments ) "
"if state . use_error_feedback : state . error_dict [ input_tensor ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] ","if state . use_error_feedback : state . error_dict [ bucket_index ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] "
"model = copy . deepcopy ( model ) model . eval ( ) prepare ( model , inplace = True ) run_fn ( model , run_args ) convert ( model , mapping , inplace = True ) return model ","model = copy . deepcopy ( model ) model . eval ( ) prepare ( model , inplace = True ) run_fn ( model , * run_args ) convert ( model , mapping , inplace = True ) return model "
"powerSGD_state = powerSGD . PowerSGDState ( process_group = state , matrix_approximation_rank = matrix_approximation_rank , random_seed = random_seed , ) model . register_comm_hook ( powerSGD_state , comm_hook ) ","powerSGD_state = powerSGD . PowerSGDState ( process_group = state , matrix_approximation_rank = matrix_approximation_rank , use_error_feedback = use_error_feedback , random_seed = random_seed , ) model . register_comm_hook ( powerSGD_state , comm_hook ) "
"q = fut . value ( ) [ 0 ] . div_ ( world_size ) torch . matmul ( p , q . t ( ) , out = matrix ) ret = input_tensor . resize_ ( total_length ) return [ ret ] ","q = fut . value ( ) [ 0 ] . div_ ( world_size ) torch . matmul ( p , q . t ( ) , out = matrix ) if state . use_error_feedback : state . error_dict [ input_tensor ] = input_tensor_cp - input_tensor ret = input_tensor . resize_ ( total_length ) return [ ret ] "
"self . bn = _BN_CLASS_MAP [ dim ] ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) self . zero_bias = torch . zeros ( out_channels ) if bias : self . bias = Parameter ( torch . Tensor ( out_channels ) ) ","self . bn = _BN_CLASS_MAP [ dim ] ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) if bias : self . bias = Parameter ( torch . Tensor ( out_channels ) ) "
"scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight , self . zero_bias ) conv_orig = conv / scale_factor . reshape ( bias_shape ) ","scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight ) conv_orig = conv / scale_factor . reshape ( bias_shape ) "
"def __setattr__ ( self , key : Any , value : Any ) -> None : if getattr ( self , ""_initialized"" , False ) and not isinstance ( value , torch . nn . Parameter ) : warnings . warn ( ""Setting attributes on ParameterDict is not supported."" ) super ( ParameterDict , self ) . __setattr__ ( key , value ) ","def __setattr__ ( self , key : Any , value : Any ) -> None : if getattr ( self , ""_initialized"" , False ) : if not hasattr ( self , key ) and not isinstance ( value , torch . nn . Parameter ) : warnings . warn ( ""Setting attributes on ParameterDict is not supported."" ) super ( ParameterDict , self ) . __setattr__ ( key , value ) "
"scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight ) conv_orig = conv / scale_factor . reshape ( bias_shape ) ","scale_factor . reshape ( weight_shape ) ) conv = self . _conv_forward ( input , scaled_weight , self . zero_bias ) conv_orig = conv / scale_factor . reshape ( bias_shape ) "
"def replace_node_module ( node : fx . Node , modules : Dict [ str , Any ] , new_module : torch . nn . Module ) : parent_name , name = _parent_name ( node . target ) ","def replace_node_module ( node : fx . Node , modules : Dict [ str , Any ] , new_module : torch . nn . Module ) : assert ( isinstance ( node . target , str ) ) parent_name , name = _parent_name ( node . target ) "
"def __init__ ( self , scale : float , zero_point : int , negative_slope : float = 1 e -2 , inplace : bool = False ) : super ( ) . __init__ ( negative_slope , inplace ) self . register_buffer ( ale torch . tensor ( [ scale ] ) ) self . register_buffer ( ro_point torch . tensor ( [ zero_point ] ) ) ","def __init__ ( self , scale : float , zero_point : int , negative_slope : float = 1 e -2 , inplace : bool = False ) : super ( ) . __init__ ( negative_slope , inplace ) self . register_buffer ( ale torch . tensor ( scale ) ) self . register_buffer ( ro_point torch . tensor ( zero_point ) ) "
"def __init__ ( self , dtype = torch . float16 , custom_op_name = """" , compute_dtype = None ) : super ( PlaceholderObserver , self ) . __init__ ( dtype = dtype ) ","def __init__ ( self , dtype = torch . float32 , custom_op_name = """" , compute_dtype = None ) : super ( PlaceholderObserver , self ) . __init__ ( dtype = dtype ) "
"start_daemon = rank == 0 store = TCPStore ( result . hostname , result . port , world_size , start_daemon , timeout ) yield ( store , rank , world_size ) ","start_daemon = rank == 0 assert result . hostname is not None store = TCPStore ( result . hostname , result . port , world_size , start_daemon , timeout ) yield ( store , rank , world_size ) "
"partition . logical_device_ids . append ( device . logical_id ) partition . nodes . add ( node ) partition_to_left_mem_bytes [ partition ] -= total_size_of_input_nodes ","partition . logical_device_ids . append ( device . logical_id ) partition . add_node ( node ) partition_to_left_mem_bytes [ partition ] -= total_size_of_input_nodes "
"if total_size_of_input_nodes > available_mem_bytes : raise RuntimeError ( node . target + too large to fit into a device partition . nodes . add ( node ) partition . used_mem_bytes += total_size_of_input_nodes ","if total_size_of_input_nodes > available_mem_bytes : raise RuntimeError ( node . target + too large to fit into a device partition . add_node ( node ) partition . used_mem_bytes += total_size_of_input_nodes "
"if IS_HIP_EXTENSION : cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags ( cuda_post_cflags ) cuda_post_cflags = cuda_post_cflags + COMMON_HIPCC_FLAGS else : cuda_post_cflags = unix_cuda_flags ( cuda_post_cflags ) append_std14_if_no_std_present ( cuda_post_cflags ) ","if IS_HIP_EXTENSION : cuda_post_cflags = cuda_post_cflags + _get_rocm_arch_flags ( cuda_post_cflags ) cuda_post_cflags = COMMON_HIP_FLAGS + COMMON_HIPCC_FLAGS + cuda_post_cflags else : cuda_post_cflags = unix_cuda_flags ( cuda_post_cflags ) append_std14_if_no_std_present ( cuda_post_cflags ) "
"if allow_list is None : allow_list = get_qconfig_propagation_list ( ) module_qconfig = qconfig_dict . get ( type ( module ) , qconfig_parent ) ","if allow_list is None : allow_list = get_default_qconfig_propagation_list ( ) module_qconfig = qconfig_dict . get ( type ( module ) , qconfig_parent ) "
"if qconfig_propagation_list is None : qconfig_propagation_list = get_qconfig_propagation_list ( ) if custom_module_class_mapping is None : ","if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) if custom_module_class_mapping is None : "
"if qconfig_propagation_list is None : qconfig_propagation_list = get_qconfig_propagation_list ( ) propagate_qconfig_ ( model , qconfig_dict = None ) ","if qconfig_propagation_list is None : qconfig_propagation_list = get_default_qconfig_propagation_list ( ) propagate_qconfig_ ( model , qconfig_dict = None ) "
"torch . _C . _log_api_usage_once ( ""quantization_api.quantize.quantize"" ) if mapping is None : mapping = get_static_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) model . eval ( ) ","torch . _C . _log_api_usage_once ( ""quantization_api.quantize.quantize"" ) if mapping is None : mapping = get_default_static_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) model . eval ( ) "
"if mapping is None : mapping = get_dynamic_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) ","if mapping is None : mapping = get_default_dynamic_quant_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) "
"if mapping is None : mapping = get_qat_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) ","if mapping is None : mapping = get_default_qat_module_mappings ( ) if not inplace : model = copy . deepcopy ( model ) "
"if mapping is None : mapping = get_static_quant_module_mappings ( ) if convert_custom_config_dict is None : convert_custom_config_dict = { } ","if mapping is None : mapping = get_default_static_quant_module_mappings ( ) if convert_custom_config_dict is None : convert_custom_config_dict = { } "
"buf = io . BytesIO ( ) storage . _write_file ( buf , _should_read_directly ( buf ) ) buf_value = buf . getvalue ( ) ","buf = io . BytesIO ( ) storage . _write_file ( buf , _should_read_directly ( buf ) , False ) buf_value = buf . getvalue ( ) "
"def with_cache ( self , cache_size = 1 ) : return self . inv . with_cache ( cache_size ) . inv ","def with_cache ( self , cache_size = 1 ) : assert self . _inv is not None return self . inv . with_cache ( cache_size ) . inv "
"def sign ( self ) : if isinstance ( self . scale , numbers . Number ) : return 1 if self . scale > 0 else -1 if self . scale < 0 else 0 return self . scale . sign ( ) ","def sign ( self ) : if isinstance ( self . scale , numbers . Real ) : return 1 if float ( self . scale ) > 0 else -1 if float ( self . scale ) < 0 else 0 return self . scale . sign ( ) "
"def log_abs_det_jacobian ( self , x , y ) : shape = x . shape scale = self . scale if isinstance ( scale , numbers . Number ) : result = torch . full_like ( x , math . log ( abs ( scale ) ) ) else : result = torch . abs ( scale ) . log ( ) ","def log_abs_det_jacobian ( self , x , y ) : shape = x . shape scale = self . scale if isinstance ( scale , numbers . Real ) : result = torch . full_like ( x , math . log ( abs ( scale ) ) ) else : result = torch . abs ( scale ) . log ( ) "
"z_cumprod = ( 1 - z ) . cumprod ( -1 ) y = pad ( z , ( 0 , 1 ) , value = 1 ) * pad ( z_cumprod , ( 1 , 0 ) , value = 1 ) return y ","z_cumprod = ( 1 - z ) . cumprod ( -1 ) y = pad ( z , [ 0 , 1 ] , value = 1 ) * pad ( z_cumprod , [ 1 , 0 ] , value = 1 ) return y "
"pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , ( pad , pad ) , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) ","pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , [ pad , pad ] , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) "
"return map_arg ( a , lambda node : env [ node . name ] ) for producer_node in producer_nodes : env [ producer_node . name ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] . name ) ) graph_module = GraphModule ( root , graph ) return graph_module ","def load_arg ( a ) : return map_arg ( a , lambda node : env [ node ] ) for producer_node in producer_nodes : env [ producer_node ] = graph . node_copy ( producer_node , load_arg ) graph . output ( load_arg ( producer_nodes [ -1 ] ) ) graph_module = GraphModule ( root , graph ) return graph_module "
"_all_gather_sequence_id += 1 is_leader = leader_name == self_name timeout = 5 if is_leader : ","_all_gather_sequence_id += 1 is_leader = leader_name == self_name if timeout == UNSET_RPC_TIMEOUT : timeout = get_rpc_timeout ( ) if is_leader : "
"if node . op == ll_module if node . target . split ( '.' ) [ -1 ] . startswith ( tivation_post_process_ observer_module = self . modules [ node . target ] prev_node = node . args [ 0 ] if observer_module . dtype == torch . float16 : ","if node . op == ll_module if is_activation_post_process ( self . modules [ node . target ] ) : observer_module = self . modules [ node . target ] prev_node = node . args [ 0 ] if observer_module . dtype == torch . float16 : "
"for node in self . quantized_graph . nodes : if node . op == ll_module nd node . target . split ( '.' ) [ -1 ] . startswith ( tivation_post_process_ env [ node . name ] = env [ node . args [ 0 ] . name ] ","for node in self . quantized_graph . nodes : if node . op == ll_module nd is_activation_post_process ( self . modules [ node . target ] ) : env [ node . name ] = env [ node . args [ 0 ] . name ] "
"if curr_modules . keys ( ) != prev_modules . keys ( ) : raise ValueError ( ""The keys to the given mappings must have the same set of names of modules"" ) summed_norms = 0 if None in prev_modules . values ( ) : return False ","if curr_modules . keys ( ) != prev_modules . keys ( ) : raise ValueError ( ""The keys to the given mappings must have the same set of names of modules"" ) summed_norms = torch . tensor ( 0 . ) if None in prev_modules . values ( ) : return False "
"for name in curr_modules . keys ( ) : difference = curr_modules [ name ] . weight . sub ( prev_modules [ name ] . weight ) summed_norms += torch . norm ( difference ) return summed_norms < threshold ","for name in curr_modules . keys ( ) : difference = curr_modules [ name ] . weight . sub ( prev_modules [ name ] . weight ) summed_norms += torch . norm ( difference ) return bool ( summed_norms < threshold ) "
"def forward ( self , input ) : return self . activation_post_process ( F . relu ( F . linear ( input , self . weight_fake_quant ( self . weight ) , self . bias ) ) ) ","def forward ( self , input ) : return F . relu ( F . linear ( input , self . weight_fake_quant ( self . weight ) , self . bias ) ) "
"def forward ( self , input ) : return self . _forward ( input ) ","def forward ( self , input ) : return self . activation_post_process ( self . _forward ( input ) ) "
"def forward ( self , input ) : return F . relu ( ConvBn2d . _forward ( self , input ) ) ","def forward ( self , input ) : return self . activation_post_process ( F . relu ( ConvBn2d . _forward ( self , input ) ) ) "
"self . freeze_bn = freeze_bn if self . training else True self . bn = nn . BatchNorm2d ( out_channels , eps , momentum , True , True ) self . activation_post_process = self . qconfig . activation ( ) self . weight_fake_quant = self . qconfig . weight ( ) ","self . freeze_bn = freeze_bn if self . training else True self . bn = nn . BatchNorm2d ( out_channels , eps , momentum , True , True ) self . weight_fake_quant = self . qconfig . weight ( ) "
"def forward ( self , input ) : return self . activation_post_process ( F . relu ( ConvBn2d . _forward ( self , input ) ) ) ","def forward ( self , input ) : return F . relu ( ConvBn2d . _forward ( self , input ) ) "
"if start_record is None and name == start_profile start_record = record elif name == cuda_start_event assert record . device ( ) != -1 ","if start_record is None and name == start_profile start_record = record elif cuda_start_event n name : assert record . device ( ) != -1 "
"if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : prev_zeros = ( min_val . item ( ) == 0 ) and ( max_val . item ( ) == 0 ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or prev_zeros : min_val = torch . min ( x ) max_val = torch . max ( x ) ","if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : same_values = min_val . item ( ) == max_val . item ( ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or same_values : min_val = torch . min ( x ) max_val = torch . max ( x ) "
"min_val = self . min_val max_val = self . max_val if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 : min_val = torch . min ( x ) max_val = torch . max ( x ) ","min_val = self . min_val max_val = self . max_val prev_zeros = False if min_val . numel ( ) > 0 and max_val . numel ( ) > 0 : prev_zeros = ( min_val . item ( ) == 0 ) and ( max_val . item ( ) == 0 ) if min_val . numel ( ) == 0 or max_val . numel ( ) == 0 or prev_zeros : min_val = torch . min ( x ) max_val = torch . max ( x ) "
"for dll in dlls : is_loaded = False if with_load_library_flags : res = kernel32 . LoadLibraryExW ( dll , 0 , 0x00001100 ) last_error = ctypes . get_last_error ( ) if res == 0 and last_error != 126 : err = ctypes . WinError ( last_error ) ","for dll in dlls : is_loaded = False if with_load_library_flags : res = kernel32 . LoadLibraryExW ( dll , None , 0x00001100 ) last_error = ctypes . get_last_error ( ) if res is None and last_error != 126 : err = ctypes . WinError ( last_error ) "
"res = kernel32 . LoadLibraryW ( dll ) if res == 0 : err = ctypes . WinError ( ctypes . get_last_error ( ) ) ","res = kernel32 . LoadLibraryW ( dll ) if res is None : err = ctypes . WinError ( ctypes . get_last_error ( ) ) "
"def __rpow__ ( self , other ) : return self . new_tensor ( other ) * * self ","def __rpow__ ( self , other ) : dtype = torch . result_type ( other , self ) return torch . tensor ( other , dtype = dtype , device = self . device ) * * self "
"if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : ","if not torch . jit . is_scripting ( ) : if any ( type ( t ) is not Tensor for t in operands ) and has_torch_function ( operands ) : return handle_torch_function ( einsum , operands , equation , * operands ) if len ( operands ) == 1 and isinstance ( operands [ 0 ] , ( list , tuple ) ) : "
"fin_path = os . path . join ( output_directory , filepath ) with open ( fin_path , 'r' ) as fin : output_source = fin . read ( ) fout_path = os . path . join ( output_directory , get_hip_file_path ( filepath ) ) ","fin_path = os . path . join ( output_directory , filepath ) with open ( fin_path , 'r' , encoding = f -8 as fin : output_source = fin . read ( ) fout_path = os . path . join ( output_directory , get_hip_file_path ( filepath ) ) "
"do_write = True if os . path . exists ( fout_path ) : with open ( fout_path , 'r' ) as fout_old : do_write = fout_old . read ( ) != output_source ","do_write = True if os . path . exists ( fout_path ) : with open ( fout_path , 'r' , encoding = f -8 as fout_old : do_write = fout_old . read ( ) != output_source "
"if do_write : with clean_ctx . open ( fout_path , 'w' ) as fout : fout . write ( output_source ) return ""ok"" ","if do_write : with clean_ctx . open ( fout_path , 'w' , encoding = f -8 as fout : fout . write ( output_source ) return ""ok"" "
"def __rdiv__ ( self , other ) : if self . dtype . is_floating_point : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) ","def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) "
"self . _module_copies [ 0 ] = self . module for module_copy in self . _module_copies [ 1 : ] : for param , copy_param in zip ( self . module . parameters ( ) , module_copy . parameters ( ) ) : copy_param . requires_grad = param . requires_grad ","self . _module_copies [ 0 ] = self . module for module_copy in self . _module_copies [ 1 : ] : for param , copy_param in zip ( self . module . parameters ( ) , parameters ( module_copy ) ) : copy_param . requires_grad = param . requires_grad "
"self . _module_copies = [ self . module ] self . modules_params = [ list ( m . parameters ( ) ) for m in self . _module_copies ] self . modules_buffers = [ list ( m . buffers ( ) ) for m in self . _module_copies ] ","self . _module_copies = [ self . module ] self . modules_params = [ list ( parameters ( m ) ) for m in self . _module_copies ] self . modules_buffers = [ list ( m . buffers ( ) ) for m in self . _module_copies ] "
"def __rdiv__ ( self , other ) : if self . dtype . is_floating_point or self . dtype . is_complex : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) ","def __rdiv__ ( self , other ) : if self . dtype . is_floating_point : return self . reciprocal ( ) * other else : return ( self . double ( ) . reciprocal ( ) * other ) . type_as ( self ) "
"arranged_img_CHW = make_grid ( make_np ( label_img ) , ncols = nrow ) arranged_augment_square_HWC = np . ndarray ( ( arranged_img_CHW . shape [ 2 ] , arranged_img_CHW . shape [ 2 ] , 3 ) ) arranged_img_HWC = arranged_img_CHW . transpose ( 1 , 2 , 0 ) ","arranged_img_CHW = make_grid ( make_np ( label_img ) , ncols = nrow ) arranged_augment_square_HWC = np . zeros ( ( arranged_img_CHW . shape [ 2 ] , arranged_img_CHW . shape [ 2 ] , 3 ) ) arranged_img_HWC = arranged_img_CHW . transpose ( 1 , 2 , 0 ) "
"ssi = SessionStartInfo ( ) for k , v in hparam_dict . items ( ) : if isinstance ( v , int ) or isinstance ( v , float ) : ssi . hparams [ k ] . number_value = v continue ","ssi = SessionStartInfo ( ) for k , v in hparam_dict . items ( ) : if v is None : continue if isinstance ( v , int ) or isinstance ( v , float ) : ssi . hparams [ k ] . number_value = v continue "
"def size ( g , self , dim ) : return sym_help . _size_helper ( g , self , dim ) ","def size ( g , self , dim = None ) : if dim is None : return g . op ( ""Shape"" , self ) return sym_help . _size_helper ( g , self , dim ) "
"def hardtanh ( g , self , min_val , max_val ) : dtype = self . type ( ) . scalarType ( ) if dtype is not None : dtype = 6 else : dtype = sym_help . scalar_type_to_onnx . index ( sym_help . cast_pytorch_to_onnx [ dtype ] ) ","def hardtanh ( g , self , min_val , max_val ) : dtype = self . type ( ) . scalarType ( ) if dtype is None : dtype = 6 else : dtype = sym_help . scalar_type_to_onnx . index ( sym_help . cast_pytorch_to_onnx [ dtype ] ) "
"pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , ( pad , pad ) , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) return torch . _C . _VariableFunctions . stft ( input , n_fft , hop_length , win_length , window , normalized , onesided ) del torch . unique_dim ","pad = int ( n_fft input = F . pad ( input . view ( extended_shape ) , ( pad , pad ) , pad_mode ) input = input . view ( input . shape [ - signal_dim : ] ) return _VF . stft ( input , n_fft , hop_length , win_length , window , normalized , onesided ) del torch . unique_dim "
"def register_all ( mod ) : for name in dir ( mod ) : v = getattr ( mod , name ) if callable ( v ) : _builtin_ops . append ( ( v , ""aten::"" + name ) ) for mod in _modules_containing_builtins : register_all ( mod ) ","def register_all ( mod ) : for name in dir ( mod ) : v = getattr ( mod , name ) if callable ( v ) and not _is_special_functional_bound_op ( v ) : _builtin_ops . append ( ( v , ""aten::"" + name ) ) for mod in _modules_containing_builtins : register_all ( mod ) "
"self . scale = None self . zero_point = None self . dtype = self . activation_post_process . dtype self . qscheme = self . activation_post_process . qscheme ","self . scale = torch . tensor ( [ 1 . 0 ] ) self . zero_point = torch . tensor ( [ 0 ] ) self . dtype = self . activation_post_process . dtype self . qscheme = self . activation_post_process . qscheme "
"func = func_overload . overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE : return CURRENT_DECOMPOSITION_TABLE [ func_overload ] ( * args , * * kwargs ) ","func = func_overload . overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE : with proxy_mode . restore ( ) : return CURRENT_DECOMPOSITION_TABLE [ func_overload ] ( * args , * * kwargs ) "
"def _onnx_opset_unsupported_detailed ( op_name , current_opset , supported_opset , reason ) : raise RuntimeError ( f ""Unsupported: ONNX export of {op_name} in "" f ""opset {current_opset}. {reason}. Please try opset version {supported_opset}."" ) ","def _onnx_opset_unsupported_detailed ( op_name : str , current_opset : int , supported_opset : int , reason : str ) : raise RuntimeError ( f ""Unsupported: ONNX export of {op_name} in "" f ""opset {current_opset}. {reason}. Please try opset version {supported_opset}."" ) "
"if GLOBALS . export_onnx_opset_version < 13 : raise ValueError ( f ""Opset version must be >= 13 for Squeeze with dynamic axes. {input.node().sourceRange()}"" ) axes_t = axes_i [ 0 ] axes_rank = _get_tensor_rank ( axes_t ) if axes_rank > 1 : raise ValueError ( ""For Squeeze axses as input, the axes rank must be one in ONNX spec."" ) ","if GLOBALS . export_onnx_opset_version < 13 : raise errors . SymbolicValueError ( ""Opset version must be >= 13 for Squeeze with dynamic axes."" , input ) axes_t = axes_i [ 0 ] axes_rank = _get_tensor_rank ( axes_t ) assert axes_rank is not None if axes_rank > 1 : raise errors . SymbolicValueError ( ""For Squeeze axses as input, the axes rank must be one in ONNX spec."" , input ) "
"if weight is None or _is_none ( weight ) : if channel_size is None : raise RuntimeError ( ""Unsupported: ONNX export of batch_norm for unknown "" ""channel size."" ) weight_value = torch . tensor ( [ 1 . 0 ] * channel_size , ","if weight is None or _is_none ( weight ) : if channel_size is None : raise errors . SymbolicValueError ( ""Unsupported: ONNX export of batch_norm for unknown channel size."" , input , ) weight_value = torch . tensor ( [ 1 . 0 ] * channel_size , "
"weight = g . op ( ""Constant"" , value_t = weight_value ) if bias is None or _is_none ( bias ) : if channel_size is None : raise RuntimeError ( ""Unsupported: ONNX export of batch_norm for unknown "" ""channel size."" ) bias_value = torch . tensor ( [ 0 . 0 ] * channel_size , ","weight = g . op ( ""Constant"" , value_t = weight_value ) if bias is None or _is_none ( bias ) : if channel_size is None : raise errors . SymbolicValueError ( ""Unsupported: ONNX export of batch_norm for unknown channel size."" , input , ) bias_value = torch . tensor ( [ 0 . 0 ] * channel_size , "
"tensor , scale , zero_point = unpacked_qtensors [ : 3 ] axis = unpacked_qtensors [ 3 ] if len ( unpacked_qtensors ) >= 4 else None axis_i = _get_const ( axis , ""i"" , ""axis"" ) input_qdtype = _type_utils . JitScalarType . from_name ( tensor . type ( ) . scalarType ( ) ) if qdtype is None : if input_qdtype is not None : ","tensor , scale , zero_point = unpacked_qtensors [ : 3 ] axis = unpacked_qtensors [ 3 ] if len ( unpacked_qtensors ) >= 4 else None axis_i = _get_const ( axis , ""i"" , ""axis"" ) input_scalar_type = tensor . type ( ) . scalarType ( ) assert input_scalar_type is not None input_qdtype = _type_utils . JitScalarType . from_name ( tensor . type ( ) . scalarType ( ) ) if qdtype is None : if input_qdtype is not None : "
") ndim = symbolic_helper . _get_tensor_rank ( input ) perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) ",") ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( range ( 0 , ndim ) ) perm . append ( perm . pop ( dimension ) ) "
"def numpy_T ( g , input ) : ndim = symbolic_helper . _get_tensor_rank ( input ) perm = list ( reversed ( range ( 0 , ndim ) ) ) return g . op ( ""Transpose"" , input , perm_i = perm ) ","def numpy_T ( g , input ) : ndim = symbolic_helper . _get_tensor_rank ( input ) assert ndim is not None perm = list ( reversed ( range ( 0 , ndim ) ) ) return g . op ( ""Transpose"" , input , perm_i = perm ) "
"unsqueeze_axes = list ( range ( 1 , symbolic_helper . _get_tensor_rank ( self ) + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , ","tensor_rank = symbolic_helper . _get_tensor_rank ( self ) assert tensor_rank is not None unsqueeze_axes = list ( range ( 1 , tensor_rank + 1 ) ) expanded_boundaries = expand ( g , symbolic_helper . _unsqueeze_helper ( g , boundaries , unsqueeze_axes ) , "
"rank = symbolic_helper . _get_tensor_rank ( x1 ) broadcasted_x1 = symbolic_helper . _unsqueeze_helper ( g , x1 , [ rank - 1 ] ) broadcasted_x2 = symbolic_helper . _unsqueeze_helper ( g , x2 , [ rank - 2 ] ) return pairwise_distance ( ","rank = symbolic_helper . _get_tensor_rank ( x1 ) assert rank is not None broadcasted_x1 = symbolic_helper . _unsqueeze_helper ( g , x1 , [ rank - 1 ] ) broadcasted_x2 = symbolic_helper . _unsqueeze_helper ( g , x2 , [ rank - 2 ] ) return pairwise_distance ( "
"composite_ops = get_ops_for_key ( mpositeImplicitAutograd noncomposite_ops = all_ops - composite_ops ops = yaml . load ( open ( / . . / pytorch / aten / src / ATen / native / native_functions . yaml 'r' ) . read ( ) , Loader = yaml . CLoader ) annotated_ops = { a . strip ( ) : b . strip ( ) for a , b in list ( csv . reader ( open ( notated_ops ) } from collections import defaultdict ","composite_ops = get_ops_for_key ( mpositeImplicitAutograd noncomposite_ops = all_ops - composite_ops ops = yaml . load ( open ( / . . / aten / src / ATen / native / native_functions . yaml 'r' ) . read ( ) , Loader = yaml . CLoader ) annotated_ops = { a . strip ( ) : b . strip ( ) for a , b in list ( csv . reader ( open ( notated_ops ) } from collections import defaultdict "
"symbolic_helper . _set_opset_version ( opset_version ) symbolic_helper . _set_operator_export_type ( operator_export_type ) symbolic_helper . _set_onnx_shape_inference ( True ) with exporter_context ( model , training , verbose ) : val_keep_init_as_ip = _decide_keep_init_as_input ( keep_initializers_as_inputs , operator_export_type , opset_version ","symbolic_helper . _set_opset_version ( opset_version ) symbolic_helper . _set_operator_export_type ( operator_export_type ) with exporter_context ( model , training , verbose ) : val_keep_init_as_ip = _decide_keep_init_as_input ( keep_initializers_as_inputs , operator_export_type , opset_version "
"def op_name_from_native_function ( f : NativeFunction ) -> str : return f ""aten::{f.func.name}"" ","def op_name_from_native_function ( f : NativeFunction ) -> str : return f ""{f.namespace}::{f.func.name}"" "
"flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped ","flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if len ( batch_sizes ) == 0 : raise ValueError ( ap : Expected at least one Tensor to vmap over if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped "
"def _div_aten ( a , b ) : if isinstance ( a , ( bool , int ) ) : return torch . div ( a , b , rounding_mode = ""trunc"" ) return torch . true_divide ( a , b ) div = _make_elementwise_binary_prim ( ","def _div_aten ( a , b ) : is_integral = isinstance ( a , ( bool , int ) ) or ( isinstance ( a , torch . Tensor ) and utils . is_integer_dtype ( a . dtype ) ) if is_integral : return torch . div ( a , b , rounding_mode = ""trunc"" ) else : return torch . true_divide ( a , b ) div = _make_elementwise_binary_prim ( "
"backend = get_backend ( p2p_op_list [ 0 ] . group ) if not all ( backend == get_backend ( p2p_op . group ) for p2p_op in p2p_op_list ) : raise RuntimeError ( ""All groups need to use the same backend."" ) def is_mpi_available ( ) : ","group = p2p_op_list [ 0 ] . group if not all ( group == p2p_op . group for p2p_op in p2p_op_list ) : raise RuntimeError ( ""All ops need to use the same group."" ) def is_mpi_available ( ) : "
"def _batch_p2p_manager ( backend ) : if backend == Backend . NCCL : ProcessGroupNCCL . _group_start ( ) try : yield finally : if backend == Backend . NCCL : ProcessGroupNCCL . _group_end ( ) def batch_isend_irecv ( p2p_op_list ) : ","def _coalescing_manager ( group , reqs ) : if group is None : group = _get_default_group ( ) group . _start_coalescing ( ) try : yield finally : group . _end_coalescing ( reqs ) def batch_isend_irecv ( p2p_op_list ) : "
"_check_p2p_op_list ( p2p_op_list ) backend = get_backend ( p2p_op_list [ 0 ] . group ) reqs = [ ] with _batch_p2p_manager ( backend ) : for p2p_op in p2p_op_list : op = p2p_op . op tensor = p2p_op . tensor ","_check_p2p_op_list ( p2p_op_list ) group = p2p_op_list [ 0 ] . group reqs = [ ] with _coalescing_manager ( group , reqs ) : for p2p_op in p2p_op_list : op = p2p_op . op tensor = p2p_op . tensor "
"raise AttributeError ( f ""'_OpNamespace' object has no attribute '{op_name}'"" ) from e ","raise AttributeError ( f ""'_OpNamespace' '{self.name}' object has no attribute '{op_name}'"" ) from e "
"continue storage_key = storage_md . storage_key target_tensor = shard . tensor . detach ( ) offsets = [ ] lengths = [ ] ","continue storage_key = storage_metadata [ MetadataIndex ( fqn , storage_md . offsets ) ] target_tensor = shard . tensor . detach ( ) offsets = [ ] lengths = [ ] "
"tensor_read_requests = [ ] bytes_read_requests = [ ] for fqn , obj in state_dict . items ( ) : md = metadata_from_storage . state_dict_metadata [ fqn ] if isinstance ( obj , ShardedTensor ) : ","tensor_read_requests = [ ] bytes_read_requests = [ ] storage_md = cast ( Dict [ MetadataIndex , str ] , metadata_from_storage . storage_data ) for fqn , obj in state_dict . items ( ) : md = metadata_from_storage . state_dict_metadata [ fqn ] if isinstance ( obj , ShardedTensor ) : "
"bytes_io = io . BytesIO ( ) brr = BytesReadRequest ( bytes = bytes_io , storage_key = md . storage_key , fqn = fqn ) bytes_read_requests . append ( brr ) ","bytes_io = io . BytesIO ( ) brr = BytesReadRequest ( bytes = bytes_io , storage_key = storage_md [ MetadataIndex ( fqn ) ] , fqn = fqn ) bytes_read_requests . append ( brr ) "
"try : result = map_fun ( ) except BaseException as e : result = CheckpointException ( step , { self . rank : e } ) final_result = self . broadcast_object ( result ) if isinstance ( final_result , CheckpointException ) : raise final_result ","try : result = map_fun ( ) except BaseException as e : result = CheckpointException ( step , { self . rank : _wrap_exception ( e ) } ) final_result = self . broadcast_object ( result ) if isinstance ( final_result , CheckpointException ) : raise final_result "
"AGGRESSIVE_RECOMPUTATION = False def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) ","AGGRESSIVE_RECOMPUTATION = False def _maybe_size_of ( node ) : if nsor_meta n node . meta : return _size_of ( node . meta [ nsor_meta return 0 def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) "
"then we don allow recomputation . if nsor_meta ot in node . meta : return False input_tensors_size = sum ( _size_of ( i . meta [ nsor_meta for i in node . args if isinstance ( i , fx . Node ) ) output_size = _size_of ( node . meta [ nsor_meta return ( output_size * 4 < input_tensors_size ) ","then we don allow recomputation . if nsor_meta ot in node . meta : return False input_tensors_size = sum ( _maybe_size_of ( i ) for i in node . args if isinstance ( i , fx . Node ) ) output_size = _size_of ( node . meta [ nsor_meta return ( output_size * 4 < input_tensors_size ) "
"isinstance ( x , torch . Tensor ) and not isinstance ( x , FakeTensor ) and type ( x ) is not torch . Tensor ) tree_map ( check_non_fake_tensor , args ) ","isinstance ( x , torch . Tensor ) and not isinstance ( x , FakeTensor ) and type ( x ) is not torch . Tensor and type ( x ) is not torch . nn . Parameter ) tree_map ( check_non_fake_tensor , args ) "
"def gen_unwraps ( flat_arguments : List [ Argument ] , cur_level_var : str ) -> Tuple [ List [ str ] , List [ str ] ] : arg_names = [ a . name for a in flat_arguments ] arg_types = [ a . type for a in flat_arguments ] ","def gen_unwraps ( flat_arguments : Sequence [ Argument ] , cur_level_var : str ) -> Tuple [ str , List [ str ] ] : arg_names = [ a . name for a in flat_arguments ] arg_types = [ a . type for a in flat_arguments ] "
"for opt_tensor in optional_tensors : unwraps += unwrap_optional_tensor ( opt_tensor , cur_level_var ) unwraps = ""n"" . join ( unwraps ) unwrapped_arg_list = [ ] for arg in arg_names : ","for opt_tensor in optional_tensors : unwraps += unwrap_optional_tensor ( opt_tensor , cur_level_var ) unwrap_code = ""n"" . join ( unwraps ) unwrapped_arg_list = [ ] for arg in arg_names : "
"def gen_returns ( returns : List [ Return ] , cur_level_var : str , results_var : str ) -> str : idx = 0 wrapped_returns = [ ] for ret in returns : ","def gen_returns ( returns : Tuple [ Return , . . . ] , cur_level_var : str , results_var : str ) -> str : idx = 0 wrapped_returns = [ ] for ret in returns : "
"def gen_vmap_plumbing ( native_function : NativeFunction ) -> str : schema = native_function . func sig = DispatcherSignature . from_schema ( schema ) returns = schema . returns ","def gen_vmap_plumbing ( native_function : NativeFunction ) -> Optional [ str ] : schema = native_function . func sig = DispatcherSignature . from_schema ( schema ) returns = schema . returns "
"def gen_all_vmap_plumbing ( native_functions ) : body = ""n"" . join ( list ( mapMaybe ( ComputeBatchRulePlumbing ( ) , native_functions ) ) ) return f """" ","def gen_all_vmap_plumbing ( native_functions : Sequence [ NativeFunction ] ) -> str : body = ""n"" . join ( list ( mapMaybe ( ComputeBatchRulePlumbing ( ) , native_functions ) ) ) return f """" "
"eps = group [ s lr = group [ beta1 , beta2 = group [ tas maximize = group [ ximize for p in group [ rams if p . grad is not None : ","eps = group [ s lr = group [ beta1 , beta2 = group [ tas maximize = group . get ( ximize False ) for p in group [ rams if p . grad is not None : "
"has_sparse_grad : bool ) : for i , param in enumerate ( params ) : d_p = d_p_list [ i ] if weight_decay != 0 : d_p = d_p . add ( param , alpha = weight_decay ) ","has_sparse_grad : bool ) : for i , param in enumerate ( params ) : d_p = d_p_list [ i ] if not maximize else - d_p_list [ i ] if weight_decay != 0 : d_p = d_p . add ( param , alpha = weight_decay ) "
"else : d_p = buf alpha = lr if maximize else - lr param . add_ ( d_p , alpha = alpha ) def _multi_tensor_sgd ( params : List [ Tensor ] , ","else : d_p = buf param . add_ ( d_p , alpha = - lr ) def _multi_tensor_sgd ( params : List [ Tensor ] , "
"if has_sparse_grad is None : has_sparse_grad = any ( grad . is_sparse for grad in grads ) if weight_decay != 0 : grads = torch . _foreach_add ( grads , params , alpha = weight_decay ) ","if has_sparse_grad is None : has_sparse_grad = any ( grad . is_sparse for grad in grads ) if maximize : grads = torch . _foreach_neg ( tuple ( grads ) ) type : ignore [ assignment ] if weight_decay != 0 : grads = torch . _foreach_add ( grads , params , alpha = weight_decay ) "
"def is_symbolic_op ( func ) : return func in [ aten . sym_size . default , aten . dim . default , aten . is_contiguous . default , aten . stride ] def handle_symbolic_op ( func , args , kwargs ) : ","def is_symbolic_op ( func ) : return func in [ aten . sym_size . default , aten . dim . default , aten . is_contiguous . default , aten . stride . default ] def handle_symbolic_op ( func , args , kwargs ) : "
"if func == torch . ops . aten . is_contiguous . default : return True TODO : hack , we don currently support symbolic strides properly if func == torch . ops . aten . stride : return create_contiguous ( args [ 0 ] . shape ) TODO : An incomplete list ","if func == torch . ops . aten . is_contiguous . default : return True TODO : hack , we don currently support symbolic strides properly if func == torch . ops . aten . stride . default : return create_contiguous ( args [ 0 ] . shape ) TODO : An incomplete list "
"logging . debug ( ""partitioner_cache hit!"" ) fused_graph_module = self . partitioner_cache [ graph_module ] else : partitioner = CapabilityBasedPartitioner ( graph_module , self . supported_ops ) fused_graph_module = partitioner . partition_and_fuse ( ) self . partitioner_cache [ graph_module ] = fused_graph_module ","logging . debug ( ""partitioner_cache hit!"" ) fused_graph_module = self . partitioner_cache [ graph_module ] else : partitioner = CapabilityBasedPartitioner ( graph_module , self . supported_ops , allows_single_node_partition = False ) fused_graph_module = partitioner . partition_and_fuse ( ) self . partitioner_cache [ graph_module ] = fused_graph_module "
"Overriding fused_module __call__ ( ) function with lower_to_prims_and_execute ( ) for node in fused_graph_module . graph . nodes : TODO : use a better way to identify fused submodule if ""fused_"" in node . name : fused_module = getattr ( fused_graph_module , node . name ) fused_module . _wrapped_call = self . lower_to_prims_and_execute ","Overriding fused_module __call__ ( ) function with lower_to_prims_and_execute ( ) for node in fused_graph_module . graph . nodes : TODO : use a better way to identify fused submodule if node . op == ""call_module"" and ""fused_"" in node . name : fused_module = getattr ( fused_graph_module , node . name ) fused_module . _wrapped_call = self . lower_to_prims_and_execute "
"except ImportError : raise RuntimeError ( ""Need networkx installed to perform smart recomputation heuristics"" ) strip_overloads ( joint_module ) joint_module . graph . eliminate_dead_code ( ) joint_module . recompile ( ) fx_g = joint_module . graph ","except ImportError : raise RuntimeError ( ""Need networkx installed to perform smart recomputation heuristics"" ) joint_module . graph . eliminate_dead_code ( ) joint_module . recompile ( ) fx_g = joint_module . graph "
"def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd node . target in unrecomputable_ops ) else : if node . op != ll_function ","def ban_recomputation ( node ) : if AGGRESSIVE_RECOMPUTATION : return ( node . op == ll_function nd get_aten_target ( node ) in unrecomputable_ops ) else : if node . op != ll_function "
"with preserve_rng_state ( ) : Set input tensors that require grad to leaves flat_tensor_args = pytree . tree_map ( lambda x : x . detach ( ) . requires_grad_ ( x . requires_grad ) , flat_tensor_args ) with torch . set_grad_enabled ( grad_state ) : out = flat_fn ( * flat_tensor_args ) ","with preserve_rng_state ( ) : Set input tensors that require grad to leaves flat_tensor_args = pytree . tree_map ( lambda x : x . detach ( ) . requires_grad_ ( x . requires_grad ) if isinstance ( x , Tensor ) else x , flat_tensor_args ) with torch . set_grad_enabled ( grad_state ) : out = flat_fn ( * flat_tensor_args ) "
"step 4 : assign the new grads , delete the sample grads for param , param_grad in zip ( model . parameters ( ) , grads ) : param . grad = param_grad del param . grad_sample ","step 4 : assign the new grads , delete the sample grads for param , param_grad in zip ( model . parameters ( ) , grads ) : param . grad = param_grad / batch_size del param . grad_sample "
"else : args . append ( tensor_args [ tensor_index ] ) tensor_index += 1 while tensor_index < len ( tensor_args ) : args . append ( tensor_args [ tensor_index ] ) ","else : args . append ( tensor_args [ tensor_index ] ) tensor_index += 1 index += 1 while tensor_index < len ( tensor_args ) : args . append ( tensor_args [ tensor_index ] ) "
"mem_sz = _size_of ( node . meta [ nsor_meta Heuristic to bias towards nodes closer to the backwards pass mem_sz = int ( mem_sz + node . dist_from_fw ) if is_materialized ( node ) : return mem_sz ","mem_sz = _size_of ( node . meta [ nsor_meta Heuristic to bias towards nodes closer to the backwards pass mem_sz = int ( mem_sz + node . dist_from_bw ) if is_materialized ( node ) : return mem_sz "
"return _old_backward ( * args , * * kwargs ) torch . Tensor . backwrd = _backward ","return _old_backward ( * args , * * kwargs ) torch . Tensor . backward = _backward "
"def _dict_flatten ( d : Dict [ Any , Any ] ) -> Tuple [ List [ Any ] , Context ] : keys = list ( sorted ( d . keys ( ) ) ) values = [ d [ key ] for key in keys ] return values , keys ","def _dict_flatten ( d : Dict [ Any , Any ] ) -> Tuple [ List [ Any ] , Context ] : keys = sorted ( d . keys ( ) ) values = [ d [ key ] for key in keys ] return values , keys "
"assert self . spec is None or self . spec == spec self . spec = spec if type ( self . spec ) in [ tuple , list ] and all ( [ isinstance ( i , pytree . LeafSpec ) for i in spec . children_specs ] ) : self . is_simple = True if isinstance ( self . spec , pytree . LeafSpec ) : ","assert self . spec is None or self . spec == spec self . spec = spec if type ( self . spec ) in [ tuple , list ] and all ( isinstance ( i , pytree . LeafSpec ) for i in spec . children_specs ) : self . is_simple = True if isinstance ( self . spec , pytree . LeafSpec ) : "
"fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Dict = { } , hasher_type : str = ""StaticShapeHasher"" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : ","fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = ""StaticShapeHasher"" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : "
"fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Dict = { } , hasher_type : str = ""StaticShapeHasher"" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : ","fw_compiler : Callable , bw_compiler : Optional [ Callable ] = None , partition_fn : Callable = default_partition , decompositions : Optional [ Dict ] = None , hasher_type : str = ""StaticShapeHasher"" , static_argnums : Optional [ Tuple [ int ] ] = None , ) -> Callable : "
"return _old_backward ( * args , * * kwargs ) setattr ( torch . Tensor , ckward _backward ) ","return _old_backward ( * args , * * kwargs ) torch . Tensor . backwrd = _backward "
"for node in graph . nodes : if node . op == ll_function cnt [ node . target . __name__ ] += 1 print ( sorted ( list ( cnt . items ( ) ) , key = lambda x : x [ 1 ] , reverse = True ) ) def min_cut_rematerialization_partition ( ","for node in graph . nodes : if node . op == ll_function cnt [ node . target . __name__ ] += 1 print ( sorted ( cnt . items ( ) , key = lambda x : x [ 1 ] , reverse = True ) ) def min_cut_rematerialization_partition ( "
"if node . op == aceholder return True return not all ( [ is_fusible ( node , user ) for user in node . users ] ) def get_node_weight ( node ) : mem_sz = _size_of ( node . meta [ nsor_meta ","if node . op == aceholder return True return not all ( is_fusible ( node , user ) for user in node . users ) def get_node_weight ( node ) : mem_sz = _size_of ( node . meta [ nsor_meta "
"return wrapped def make_fx ( f , decomposition_table = { } ) : def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) ","return wrapped def make_fx ( f , decomposition_table = None ) : if decomposition_table is None : decomposition_table = { } def wrapped ( * args ) : phs = pytree . tree_map ( lambda x : fx . PH , args ) "
"flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( [ size != batch_sizes [ 0 ] for size in batch_sizes ] ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension ","flat_args : List ) -> int : batch_sizes = [ arg . size ( in_dim ) for in_dim , arg in zip ( flat_in_dims , flat_args ) if in_dim is not None ] if batch_sizes and any ( size != batch_sizes [ 0 ] for size in batch_sizes ) : raise ValueError ( f ap : Expected all tensors to have the same size in the mapped mension , got sizes { batch_sizes } for the mapped dimension "
"def rsub ( self : Tensor , other : Tensor , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) def rsub ( self : Tensor , other : float , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) ","def rsub_Tensor ( self : Tensor , other : Tensor , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) def rsub_Scalar ( self : Tensor , other : float , alpha : float = 1 ) -> Tensor : return torch . sub ( other , self , alpha = alpha ) "
"Only support cases where all returns are Tensors or vector<Tensor> if len ( returns ) == 0 : return None if not all ( ret . type . is_tensor_like ( ) for ret in returns ) : return None if not accepts_at_least_one_tensor_input ( schema ) : ","Only support cases where all returns are Tensors or vector<Tensor> if len ( returns ) == 0 : return gen_vmap_plumbing_no_returns ( native_function ) if not all ( ret . type . is_tensor_like ( ) for ret in returns ) : return None if not accepts_at_least_one_tensor_input ( schema ) : "
"cur_weight = weight for _ in range ( 2 , grad_output . dim ( ) ) : cur_weight = cur_weight . unsqueeze ( -1 ) input_grad = torch . where ( self > 0 , grad_output , cur_weight * grad_output ) weight_grad_collector = torch . where ( self > 0 , grad_output . new_zeros ( ( ) ) , self * grad_output ) out = weight_grad_collector . sum_to_size ( cur_weight . shape ) ","cur_weight = _unsqueeze_to_dim ( weight , self . dim ( ) - 1 ) input_grad = torch . where ( self > 0 , grad_output , cur_weight * grad_output ) weight_grad_collector = torch . where ( self > 0 , grad_output . new_zeros ( ( ) ) , self * grad_output ) out = weight_grad_collector . sum_to_size ( cur_weight . shape ) "
"return sum / n register_decomposition ( aten . std ) def std_decomposition ( x : Tensor , dims : List [ int ] , correction : int = 0 , keepdim : bool = False ) : return torch . sqrt ( torch . var ( x , dims , correction = correction , keepdim = keepdim ) ) ","return sum / n register_decomposition ( aten . std . correction ) def std_decomposition ( x : Tensor , dims : List [ int ] , correction : int = 0 , keepdim : bool = False ) : return torch . sqrt ( torch . var ( x , dims , correction = correction , keepdim = keepdim ) ) "
"def _functorch_str ( tensor ) : level = _C . maybe_get_level ( tensor ) if level == -1 : return _old_str ( tensor ) ","def _functorch_str ( tensor , * , tensor_contents = None ) : level = _C . maybe_get_level ( tensor ) if level == -1 : return _old_str ( tensor ) "
"def var_decomposition ( x : Tensor , dims : List [ int ] , correction : int = 0 , keepdim : bool = False ) : if dims is None : dims = [ ] if isinstance ( dims , ( tuple , list ) ) and len ( dims ) == 0 : n = x . numel ( ) else : n = 1 ","def var_decomposition ( x : Tensor , dims : Optional [ List [ int ] ] , correction : int = 0 , keepdim : bool = False ) : if dims is None : dims = [ ] if len ( dims ) == 0 : n = x . numel ( ) else : n = 1 "
"if node . name not in forward_node_names : continue Since we can save tuple of tensor values , we need to flatten out what we saving if nsor_meta ot in node . meta : users = node . users assert all ( [ user . target == operator . getitem for user in users ] ) for user in users : ","if node . name not in forward_node_names : continue Since we can save tuple of tensor values , we need to flatten out what we saving if nsor_meta ot in node . meta and node . op == ll_function users = node . users assert all ( [ user . target == operator . getitem for user in users ] ) for user in users : "
"r = torch . Tensor . _make_subclass ( cls , elem , elem . requires_grad ) r . proxy = proxy if not elem . is_sparse : proxy . node . meta [ nsor_meta = _extract_tensor_metadata ( r ) return r ","r = torch . Tensor . _make_subclass ( cls , elem , elem . requires_grad ) r . proxy = proxy if elem . is_sparse : proxy . node . meta [ nsor_meta = { } else : proxy . node . meta [ nsor_meta = _extract_tensor_metadata ( r ) return r "
"args = list ( node . args ) args [ 1 ] = [ 1 ] node . args = tuple ( args ) elif node . target == torch . ops . aten . avg_pool2d_backward : Handle empty strides if node . args [ 3 ] == [ ] : args = list ( node . args ) args [ 3 ] = [ 1 , 1 ] node . args = tuple ( args ) for node in fx_g . graph . nodes : new_kwargs = { } ","args = list ( node . args ) args [ 1 ] = [ 1 ] node . args = tuple ( args ) for node in fx_g . graph . nodes : new_kwargs = { } "
"func_args = _wrap_all_tensors_to_functional ( args , func_level ) func_kwargs = _wrap_all_tensors_to_functional ( kwargs , func_level ) flattened_unwrapped_args = tree_flatten ( args ) flattened_wrapped_args = tree_flatten ( func_args ) flattened_unwrapped_kwargs = tree_flatten ( kwargs ) flattened_wrapped_kwargs = tree_flatten ( func_kwargs ) func_outputs = func ( * func_args , * * func_kwargs ) outputs = _unwrap_all_tensors_from_functional ( func_outputs ) ","func_args = _wrap_all_tensors_to_functional ( args , func_level ) func_kwargs = _wrap_all_tensors_to_functional ( kwargs , func_level ) flattened_unwrapped_args , _ = tree_flatten ( args ) flattened_wrapped_args , _ = tree_flatten ( func_args ) flattened_unwrapped_kwargs , _ = tree_flatten ( kwargs ) flattened_wrapped_kwargs , _ = tree_flatten ( func_kwargs ) func_outputs = func ( * func_args , * * func_kwargs ) outputs = _unwrap_all_tensors_from_functional ( func_outputs ) "
"if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : ","if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = None if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : "
"else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : ","else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : "
"else : d_bias = grad_out else : d_bias = aten . new_empty ( input , ( 0 , ) ) return ( d_input , d_weight , d_bias ) ","else : d_bias = grad_out else : d_bias = None return ( d_input , d_weight , d_bias ) "
"if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = None if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : ","if output_mask [ 0 ] : d_input = aten . mul ( aten . div ( rstd , N ) , inner ) else : d_input = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 1 ] and weight is not None : if len ( outer_dim_indices ) > 0 : "
"else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = None if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : ","else : d_weight = aten . mul ( grad_out , x_hat ) else : d_weight = aten . new_empty ( input , ( 0 , ) ) if output_mask [ 2 ] and bias is not None : if len ( outer_dim_indices ) > 0 : "
"else : d_bias = grad_out else : d_bias = None return ( d_input , d_weight , d_bias ) register_decomposition ( aten . addmm ) ","else : d_bias = grad_out else : d_bias = aten . new_empty ( input , ( 0 , ) ) return ( d_input , d_weight , d_bias ) register_decomposition ( aten . addmm ) "
"if len ( buffers ) > 0 : raise RuntimeError ( ke_functional_deprecated_v1 ( model ) : model has buffers . Please use '
' make_functional_with_buffers_deprecated_v1 ( model ) instead . weights , descriptors = extract_weights ( model ) def fun ( weights , data ) : mutable_model = copy . deepcopy ( model ) ","if len ( buffers ) > 0 : raise RuntimeError ( ke_functional_deprecated_v1 ( model ) : model has buffers . Please use '
' make_functional_with_buffers_deprecated_v1 ( model ) instead . weights , descriptors , _ = extract_weights ( model ) def fun ( weights , data ) : mutable_model = copy . deepcopy ( model ) "
"weights , weight_descriptors = extract_weights ( model ) buffers , buf_descriptors = extract_buffers ( model ) def fun ( weights , buffers , data ) : mutable_model = copy . deepcopy ( model ) ","weights , weight_descriptors , _ = extract_weights ( model ) buffers , buf_descriptors , _ = extract_buffers ( model ) def fun ( weights , buffers , data ) : mutable_model = copy . deepcopy ( model ) "
"default_decompositions = { k : v for k , v in decomposition_table . items ( ) if k in default_decompositions } def print_compile ( fx_g , _ ) : ","default_decompositions = get_decompositions ( default_decompositions ) def print_compile ( fx_g , _ ) : "
"proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get ","proxy_out . node . meta [ nsor_meta = _extract_tensor_metadata ( args [ 0 ] ) with no_dispatch ( ) : real_out = func_overload ( * args , * * kwargs ) def wrap_with_proxy ( e , proxy ) : Some ops ( like native_batch_norm_backward ) return undefined tensors that get "
"auto maybe_layer = maybeCurrentDynamicLayer ( ) ; TORCH_INTERNAL_ASSERT ( maybe_layer . has_value ( ) ) ; int64_t cur_level = maybe_layer -> layerId ( ) ; { textwrap . indent ( unwraps , ""  "" ) } auto results = batch_rule ( { oin ( unwrapped_arg_list ) } ) ; { wrapped_returns } ","auto maybe_layer = maybeCurrentDynamicLayer ( ) ; TORCH_INTERNAL_ASSERT ( maybe_layer . has_value ( ) ) ; int64_t cur_level = maybe_layer -> layerId ( ) ; { textwrap . indent ( bdims_all_none_case , ""  "" ) } { textwrap . indent ( unwraps , ""  "" ) } auto results = batch_rule ( { oin ( unwrapped_arg_list ) } ) ; { wrapped_returns } "
"training_size = 1000 r = torch . linspace ( 0 . 5 , 2 * sigma , requires_grad = True ) Create a bunch of vectors that point along positive - x drs = torch . outer ( r , torch . tensor ( [ 1 . 0 , 0 , 0 ] ) ) ","training_size = 1000 r = torch . linspace ( 0 . 5 , 2 * sigma , steps = training_size , requires_grad = True ) Create a bunch of vectors that point along positive - x drs = torch . outer ( r , torch . tensor ( [ 1 . 0 , 0 , 0 ] ) ) "
"if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) ","if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) "
"if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) ","if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) "
"t_params = params for k in range ( 5 ) : t_f = net ( t_x , t_params ) t_loss = F . l1_loss ( t_f , t_y ) grads = torch . autograd . grad ( t_loss , t_params , create_graph = True ) ","t_params = params for k in range ( 5 ) : t_f = net ( t_params , t_x ) t_loss = F . l1_loss ( t_f , t_y ) grads = torch . autograd . grad ( t_loss , t_params , create_graph = True ) "
"test_x = torch . arange ( -2 * math . pi , 2 * math . pi , step = 0 . 01 ) . unsqueeze ( 1 ) test_y = t_A * torch . sin ( test_x + t_b ) test_f = net ( test_x , t_params ) plt . plot ( test_x . data . numpy ( ) , test_y . data . numpy ( ) , label = n ( x ) plt . plot ( test_x . data . numpy ( ) , test_f . data . numpy ( ) , label = t ( x ) ","test_x = torch . arange ( -2 * math . pi , 2 * math . pi , step = 0 . 01 ) . unsqueeze ( 1 ) test_y = t_A * torch . sin ( test_x + t_b ) test_f = net ( t_params , test_x ) plt . plot ( test_x . data . numpy ( ) , test_y . data . numpy ( ) , label = n ( x ) plt . plot ( test_x . data . numpy ( ) , test_f . data . numpy ( ) , label = t ( x ) "
"if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) ","if it % 100 == 0 : print ( eration % d - - Outer Loss : % . 4 f ( it , loss2 ) ) losses . append ( loss2 . detach ( ) ) t_A = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 1 , 0 . 5 ) t_b = torch . tensor ( 0 . 0 ) . uniform_ ( 0 . 0 , math . pi ) "
"return mem_sz * 2 for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + ""_in"" , ""sink"" , capacity = math . inf ) continue ","return mem_sz * 2 for node in full_bw_graph . nodes : if node in tangent_closure and node . op != tput nx_graph . add_edge ( node . name + ""_in"" , ""sink"" , capacity = math . inf ) continue "
"contiguous_args = [ t . contiguous ( ) for t in flat_args ] contiguous_args = [ t for t in flat_args ] out = normalize_as_list ( compiled_bw ( * ctx . saved_tensors , * contiguous_args ) ) out_iter = iter ( out ) grad_out = [ next ( out_iter ) if p else None for p in ctx . needs_input_grad ] return tuple ( grad_out ) return CompiledFunction ","contiguous_args = [ t . contiguous ( ) for t in flat_args ] contiguous_args = [ t for t in flat_args ] out = normalize_as_list ( compiled_bw ( * ctx . saved_tensors , * contiguous_args ) ) return tuple ( out ) return CompiledFunction "
"def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , * args , * * kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) ","def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , args , kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) "
"def compiled_module ( mod , * args , * * kwargs ) : func_mod , params , buffers = make_functional_with_buffers ( mod ) compiled_f = compiled_function ( func_mod , * args , * * kwargs ) class CompiledModule ( nn . Module ) : def __init__ ( self ) : ","def compiled_module ( mod , * args , * * kwargs ) : def functional_call ( named_params , named_buffers , * args , * * kwargs ) : params_and_buffers = { * * named_params , * * named_buffers } import pdb ; pdb . set_trace ( ) return _stateless . functional_call ( mod , params_and_buffers , * args , * * kwargs ) compiled_f = compiled_function ( functional_call , * args , * * kwargs ) class CompiledModule ( nn . Module ) : def __init__ ( self ) : "
"def forward ( self , * args , * * kwargs ) : return compiled_f ( tuple ( self . parameters ( ) ) , tuple ( self . buffers ( ) ) , * args , * * kwargs ) ","def forward ( self , * args , * * kwargs ) : return compiled_f ( dict ( self . orig_module . named_parameters ( ) ) , dict ( self . orig_module . named_buffers ( ) ) , * args , * * kwargs ) "
"for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + ""_in"" , ""sink"" , capacity = math . inf ) ","ops = set ( [ i . target for i in joint_module . graph . nodes if i . op == ll_function print ( ops - recomputable_ops ) AGGRESSIVE_RECOMPUTATION = False for node in full_bw_graph . nodes : if node in tangent_closure : nx_graph . add_edge ( node . name + ""_in"" , ""sink"" , capacity = math . inf ) "
"def get_output_device ( devices , op ) : if len ( devices ) == 1 : return devices [ 0 ] else : ","def get_output_device ( devices , op ) : The device propagation is a bit sketchy . aten::index ( CPU , CUDA ) = > CPU tensor aten::index ( CUDA , CPU ) = > CUDA tensor if op == aten . index : return devices [ 0 ] devices = list ( set ( devices ) ) if len ( devices ) == 1 : return devices [ 0 ] else : "
"elif isinstance ( real_out , list ) : return list ( [ wrap_with_proxy ( e , proxy_out [ idx ] ) for idx , e in enumerate ( real_out ) ] ) elif isinstance ( real_out , torch . Tensor ) : return PythonTensor ( real_out , proxy_out , output_device ) else : return real_out ","elif isinstance ( real_out , list ) : return list ( [ wrap_with_proxy ( e , proxy_out [ idx ] ) for idx , e in enumerate ( real_out ) ] ) elif isinstance ( real_out , torch . Tensor ) : return wrap_with_proxy ( real_out , proxy_out ) else : return real_out "
"register_decomposition ( aten . tanh_backward ) def tanh_backward_decomposition ( out_grad : Tensor , y : Tensor ) : return out_grad * ( - y * y + 1 ) register_decomposition ( aten . sigmoid_backward ) def sigmoid_backward_decomposition ( out_grad : Tensor , y : Tensor ) : ","register_decomposition ( aten . tanh_backward ) def tanh_backward_decomposition ( out_grad : Tensor , y : Tensor ) : return out_grad * ( 1 - y * y ) register_decomposition ( aten . sigmoid_backward ) def sigmoid_backward_decomposition ( out_grad : Tensor , y : Tensor ) : "
"def safe_unflatten ( tensor , dim , shape ) : if len ( shape ) == 0 : assert tensor . numel ( ) == 1 return tensor . squeeze ( ) return tensor . unflatten ( dim , shape ) def jacfwd ( f , argnums = 0 ) : ","def safe_unflatten ( tensor , dim , shape ) : if len ( shape ) == 0 : assert tensor . shape [ dim ] == 1 return tensor . squeeze ( dim ) return tensor . unflatten ( dim , shape ) def jacfwd ( f , argnums = 0 ) : "
"results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( _as_tuple ( primals_out ) ) def wrapper ( * cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( ","results = _undo_create_differentiable ( primals_out , level ) flat_diff_primals , primals_spec = tree_flatten ( diff_primals ) flat_primals_out , primals_out_spec = tree_flatten ( primals_out ) def wrapper ( cotangents , retain_graph = True , create_graph = True ) : flat_cotangents , cotangents_spec = tree_flatten ( cotangents ) if primals_out_spec != cotangents_spec : raise RuntimeError ( "
"with torch . enable_grad ( ) : fx_g = make_fx ( vjpfull ) ( fn , args , ( torch . ones_like ( out ) , ) ) fw_module , bw_module = partition_backwards ( fx_g ) garbage_hack = torch . randn ( ( ) ) fw_args = ( garbage_hack , ) + args ","with torch . enable_grad ( ) : fx_g = make_fx ( vjpfull ) ( fn , args , ( torch . ones_like ( out ) , ) ) fw_module , bw_module = partition_backwards ( fx_g ) print ( fw_module . code , bw_module . code ) garbage_hack = torch . randn ( ( ) ) fw_args = ( garbage_hack , ) + args "
"tune_option = auto_scheduler . TuningOptions ( num_measure_trials = 100 , change this to 20000 to achieve the best performance measure_callbacks = [ auto_scheduler . RecordToFile ( log_file ) ] , early_stopping = 1000 , verbose = 2 , ) tuner . tune ( tune_option ) ","tune_option = auto_scheduler . TuningOptions ( num_measure_trials = 100 , change this to 20000 to achieve the best performance measure_callbacks = [ auto_scheduler . RecordToFile ( log_file ) ] , early_stopping = 1000 , verbose = 2 , ) tuner . tune ( tune_option ) "
"def bench ( func ) : begin = time . time ( ) for _ in range ( iters ) : out = func ( a ) out . sum ( ) . backward ( ) mod . zero_grad ( ) print ( time . time ( ) - begin ) def bench_jax ( ) : ","def bench ( func ) : begin = time . time ( ) for _ in range ( iters ) : out = func ( a ) . sin ( ) out . sum ( ) . backward ( ) a . grad = None print ( time . time ( ) - begin ) def bench_jax ( ) : "
"return CompiledFunction def compiled_function ( fn , fw_compiler , bw_compiler , partition_fn = partition_backwards ) : saved_fn = None def returned_function ( * args , * * kwargs ) : ","return CompiledFunction def compiled_function ( fn , fw_compiler , bw_compiler , partition_fn = default_partition ) : saved_fn = None def returned_function ( * args , * * kwargs ) : "
"def draw_joint_graph ( graph , joint_inputs , file_name = ""full_graph.png"" ) : draw_graph ( graph , file_name ) return partition_backwards ( graph , joint_inputs ) def create_compiled_function ( flat_fn , fw_compiler , bw_compiler , partition_fn ) : joint_forward_backward = create_joint_forward_backward ( flat_fn ) ","def draw_joint_graph ( graph , joint_inputs , file_name = ""full_graph.png"" ) : draw_graph ( graph , file_name ) return default_partition ( graph , joint_inputs ) def create_compiled_function ( flat_fn , fw_compiler , bw_compiler , partition_fn ) : joint_forward_backward = create_joint_forward_backward ( flat_fn ) "
"getattr ( x , ""write_"" + ext . lstrip ( ""."" ) ) ( fname ) todo ( chilli ) : clean this up / make it more understandable def partition_backwards ( fx_module : fx . GraphModule , _joint_inputs ) : bw_nodes = set ( ) saved_nodes = set ( ) output_node = None ","getattr ( x , ""write_"" + ext . lstrip ( ""."" ) ) ( fname ) todo ( chilli ) : clean this up / make it more understandable def default_partition ( fx_module : fx . GraphModule , _joint_inputs ) : bw_nodes = set ( ) saved_nodes = set ( ) output_node = None "
"else : num_outs = 1 joint_inputs = ( flat_args , ( out , ) ) with torch . enable_grad ( ) : fx_g = make_fx ( joint_forward_backward ) ( * joint_inputs ) fw_module , bw_module = partition_fn ( fx_g , joint_inputs ) ","else : num_outs = 1 joint_inputs = ( flat_args , out ) with torch . enable_grad ( ) : fx_g = make_fx ( joint_forward_backward ) ( * joint_inputs ) fw_module , bw_module = partition_fn ( fx_g , joint_inputs ) "
"def unwrap_tensors ( x ) : if isinstance ( x , torch . Tensor ) : return _unwrap_for_grad ( x , level ) assert False return tree_map ( unwrap_tensors , inps ) ","def unwrap_tensors ( x ) : if isinstance ( x , torch . Tensor ) : return _unwrap_for_grad ( x , level ) raise AssertionError ( ) return tree_map ( unwrap_tensors , inps ) "
"def f ( * inps , out_tensors = None ) : inps = fx_model . graph . flatten_inps ( * inps ) if out_tensors is None : results = alloc_results else : results = out_tensors full_inps = module_stuff + list ( inps ) + results ","def f ( * inps , out_tensors = None ) : inps = fx_model . graph . flatten_inps ( * inps ) if out_tensors is None : results = [ torch . empty ( shape , dtype = dtype ) for shape , dtype in outs [ 1 ] ] results = alloc_results else : results = out_tensors full_inps = module_stuff + list ( inps ) + results "
"class PythonTensor ( object ) : def __init__ ( self , out , proxy ) : if isinstance ( out , torch . Tensor ) : self . value = torch . empty_like ( out ) else : self . value = torch . empty ( out ) self . proxy = proxy ","class PythonTensor ( object ) : def __init__ ( self , out , proxy ) : if isinstance ( out , torch . Tensor ) : self . value = torch . clone ( out ) else : self . value = torch . empty ( out ) self . proxy = proxy "
"assert ( len ( flat_args ) == len ( flat_inps ) ) for idx , arg in enumerate ( flat_args ) : if isinstance ( flat_inps [ idx ] , torch . Tensor ) : flat_args [ idx ] = addPythonKey ( PythonTensor ( flat_inps [ idx ] . shape , arg ) ) else : flat_args [ idx ] = flat_inps [ idx ] tree_args = pytree . tree_unflatten ( flat_args , args_spec ) ","assert ( len ( flat_args ) == len ( flat_inps ) ) for idx , arg in enumerate ( flat_args ) : if isinstance ( flat_inps [ idx ] , torch . Tensor ) : flat_args [ idx ] = addPythonKey ( PythonTensor ( flat_inps [ idx ] , arg ) ) else : flat_args [ idx ] = flat_inps [ idx ] tree_args = pytree . tree_unflatten ( flat_args , args_spec ) "
"return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) ","return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) "
"return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : if new_type is Tensor : return func ( * args ) ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) ","return ret def _rebuild_from_type_v2 ( func , new_type , args , state ) : ret = func ( * args ) if type ( ret ) is not new_type : ret = ret . as_subclass ( new_type ) "
"e1 = symbols [ n . args [ 0 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e1 , n . args [ 1 ] , op_add ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter ","e1 = symbols [ n . args [ 0 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e1 , n . args [ 1 ] , op_code ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter "
"e2 = symbols [ n . args [ 1 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e2 , n . args [ 0 ] , op_add ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter ","e2 = symbols [ n . args [ 1 ] ] we will propagate the runtime value here since this is regular addition c = Conj ( [ BinConstraintD ( my_output , BinConstraintD ( e2 , n . args [ 0 ] , op_code ) , op_eq ) , BinConstraintD ( 0 , my_output , op_leq ) ] ) return [ c ] , counter "
"padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , dilation , padding , stride ) ) input = unfold_func ( ) ","padding = ( 0 , padding [ 0 ] ) , stride = ( 1 , stride [ 0 ] ) ) , lambda : F . unfold ( input , kernel_size , dilation = dilation , padding = padding , stride = stride ) , lambda : unfold3d ( input , kernel_size , padding , stride , dilation ) ) input = unfold_func ( ) "
"env_callable = lambda fn : { ""definitions"" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 5 , sharded_keys = { ""definitions"" } , ) cpu_fm . write ( ","env_callable = lambda fn : { ""definitions"" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { ""definitions"" } , ) cpu_fm . write ( "
"env_callable = lambda fn : { ""unboxed_ops"" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 10 , sharded_keys = { ""unboxed_ops"" } , ) ","env_callable = lambda fn : { ""unboxed_ops"" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { ""unboxed_ops"" } , ) "
"for i in range ( len ( current_commits . commits ) ) : c = current_commits . commits [ i ] if categorized n str ( c ) : current_commits . commits [ i ] = CommitList . categorize ( c . commit_hash , c . title ) current_commits . write_result ( ) def get_hash_or_pr_url ( commit : Commit ) : ","for i in range ( len ( current_commits . commits ) ) : c = current_commits . commits [ i ] if categorized n str ( c ) : feature_item = get_commit_data_cache ( ) . get ( c . commit_hash ) features = features_to_dict ( feature_item ) category , topic = CommitList . categorize ( features ) current_commits [ i ] = dataclasses . replace ( c , category = category , topic = topic ) current_commits . write_result ( ) def get_hash_or_pr_url ( commit : Commit ) : "
"_apply_func_submodules ( _create_swap_params ( parameters_and_buffers ) , module , name . split ( ""."" ) , name , ( tensor , ) ) yield for name in parameters_and_buffers : _apply_func_submodules ( _remove_swap , module , name . split ( ""."" ) , name , ( ) ) def _apply_func_submodules ( ","_apply_func_submodules ( _create_swap_params ( parameters_and_buffers ) , module , name . split ( ""."" ) , name , ( tensor , ) ) try : yield finally : for name in parameters_and_buffers : _apply_func_submodules ( _remove_swap , module , name . split ( ""."" ) , name , ( ) ) def _apply_func_submodules ( "
"hardshrink ( x ) = x if x > lambd = x if x < - lambd = 0 otherwise return refs . where ( abs ( a ) > abs ( lambd ) , a , 0 ) register_decomposition ( torch . ops . aten . softshrink ) ","hardshrink ( x ) = x if x > lambd = x if x < - lambd = 0 otherwise return refs . where ( refs . logical_and ( a >= - lambd , a <= lambd ) , 0 , a ) register_decomposition ( torch . ops . aten . softshrink ) "
"def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_functional_fn ( ) : name += ""_functional"" if func . is_symint_fn ( ) : name += ""_symint"" if func . is_out_fn ( ) : ","def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_symint_fn ( ) : name += ""_symint"" if func . is_out_fn ( ) : "
"def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_functional_fn ( ) : name += ""_functional"" if func . is_symint_fn ( ) : name += ""_symint"" if func . is_out_fn ( ) : ","def name ( func : FunctionSchema , * , faithful_name_for_out_overloads : bool = False ) -> str : name = str ( func . name . name ) if func . is_symint_fn ( ) : name += ""_symint"" if func . is_out_fn ( ) : "
"raise RuntimeError ( f ""The codegen expects to be able to generate '{generated_fns_str}'."" f "" To do so, it expects a line: 'autogen: {generated_fns_str}'."" f "" Instead, it found 'autogen: {generated_fns_str}'"" ) def signature ( self ) -> ""FunctionSchema"" : ","raise RuntimeError ( f ""The codegen expects to be able to generate '{generated_fns_str}'."" f "" To do so, it expects a line: 'autogen: {generated_fns_str}'."" f "" Instead, it found 'autogen: {expected_generated_fns_str}'"" ) def signature ( self ) -> ""FunctionSchema"" : "
"T = TypeVar ( 'T' , bound = ion [ _StorageBase , _TypedStorage ] class _StorageBase ( object ) : _cdata : Any is_cuda : bool = False is_sparse : bool = False is_sparse_csr : bool = False device : torch . device ","T = TypeVar ( 'T' , bound = ion [ _StorageBase , _TypedStorage ] class _StorageBase ( object ) : _cdata : Any is_sparse : bool = False is_sparse_csr : bool = False device : torch . device "
"torch . _C . _log_api_usage_once ( ""quantization_api.quantize.prepare"" ) if prepare_custom_config_dict is None : prepare_custom_config_dict = { } custom_module_class_mapping = prepare_custom_config_dict . get ( ""float_to_observed_custom_module_class"" , { } ) if not inplace : ","torch . _C . _log_api_usage_once ( ""quantization_api.quantize.prepare"" ) if prepare_custom_config_dict is None : prepare_custom_config_dict = get_default_custom_config_dict ( ) custom_module_class_mapping = prepare_custom_config_dict . get ( ""float_to_observed_custom_module_class"" , { } ) if not inplace : "
"instance to the hook as the first parameter . handle = hooks . RemovableHandle ( self . _load_state_dict_pre_hooks ) if with_module : hook = functools . partial ( hook , self ) self . _load_state_dict_pre_hooks [ handle . id ] = hook return handle def register_load_state_dict_post_hook ( self , hook ) : ","instance to the hook as the first parameter . handle = hooks . RemovableHandle ( self . _load_state_dict_pre_hooks ) self . _load_state_dict_pre_hooks [ handle . id ] = _WrappedHook ( hook , self if with_module else None ) return handle def register_load_state_dict_post_hook ( self , hook ) : "
"then we need to make sure get_attr is copied to the new graph . for x in flatten ( output_node . args [ 0 ] ) : if x . op == ""get_attr"" : setattr ( main_root , x . name , getattr ( gm , x . target ) ) type : ignore [ arg - type ] return torch . fx . GraphModule ( main_root , main_g ) ","then we need to make sure get_attr is copied to the new graph . for x in flatten ( output_node . args [ 0 ] ) : if x . op == ""get_attr"" : setattr ( main_root , x . name , getattr_recursive ( gm , x . target ) ) type : ignore [ arg - type ] return torch . fx . GraphModule ( main_root , main_g ) "
"super ( ) . __init__ ( ) self . mod = mod self . checkpoint_impl = checkpoint_impl self . offload_to_cpu = offload_to_cpu state_dict post hook to remove prefix to allow loading into a ","super ( ) . __init__ ( ) self . _checkpoint_wrapped_module = mod self . checkpoint_impl = checkpoint_impl self . offload_to_cpu = offload_to_cpu state_dict post hook to remove prefix to allow loading into a "
"def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = ne ( a , 0 ) if not utils . is_boolean_dtype ( b . dtype ) : b = ne ( b , 0 ) return bitwise_and ( a , b ) logical_and = _make_elementwise_binary_reference ( ","def _logical_and ( a : TensorLikeType , b : TensorLikeType ) : if not utils . is_boolean_dtype ( a . dtype ) : a = a != 0 if not utils . is_boolean_dtype ( b . dtype ) : b = b != 0 return a & b logical_and = _make_elementwise_binary_reference ( "
"All - gather full parameters , moving them to compute device if necessary . self . _rebuild_full_params ( ) Wait for all_gather to finish before computation torch . cuda . current_stream ( ) . wait_stream ( self . _streams [ ""all_gather"" ] ) ","All - gather full parameters , moving them to compute device if necessary . self . _rebuild_full_params ( ) self . _pre_backward_hook_full_params_prefetched = False Wait for all_gather to finish before computation torch . cuda . current_stream ( ) . wait_stream ( self . _streams [ ""all_gather"" ] ) "
"def scalar ( name , scalar , collections = None , new_style = False , double_precision = False ) : ","def scalar ( name , tensor , collections = None , new_style = False , double_precision = False ) : "
"value = [ Summary . Value ( tag = name , tensor = tensor , metadata = smd , ) ] ","value = [ Summary . Value ( tag = name , tensor = tensor_proto , metadata = smd , ) ] "
"paths . append ( p ) if not found_one : raise RuntimeError ( ""Didn't find any test reports in s3, there is probably a bug!"" ) return paths ","paths . append ( p ) if not found_one : print ( ""::warning title=s3 artifacts not found::"" ""Didn't find any test reports in s3, there might be a bug!"" ) return paths "
"paths . append ( p ) if not found_one : print ( ""::warning title=s3 artifacts not found::"" ""Didn't find any test reports in s3, there might be a bug!"" ) return paths ","paths . append ( p ) if not found_one : raise RuntimeError ( ""Didn't find any test reports in s3, there is probably a bug!"" ) return paths "
"paths . append ( p ) if not found_one : raise RuntimeError ( ""Didn't find any test reports in s3, there is probably a bug!"" ) return paths ","paths . append ( p ) if not found_one : print ( ""::warning title=s3 artifacts not found::"" ""Didn't find any test reports in s3, there might be a bug!"" ) return paths "
"def main ( ) -> None : parser = argparse . ArgumentParser ( description = ""Generate unboxing source files"" ) ","def main ( args : List [ str ] ) -> None : parser = argparse . ArgumentParser ( description = ""Generate unboxing source files"" ) "
"env_callable = lambda fn : { ""unboxed_ops"" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { ""unboxed_ops"" } , ) ","env_callable = lambda fn : { ""unboxed_ops"" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 10 , sharded_keys = { ""unboxed_ops"" } , ) "
"torch . _C . _log_api_usage_once ( ""torch.package.PackageImporter"" ) modules : Dict [ str , types . ModuleType ] ","modules : Dict [ str , types . ModuleType ] "
"elif node . target in FUNS_IO_TYPE_INT8 : return ( NodeInputOrOutputType . INT8 , NodeInputOrOutputType . INT8 ) elif node . target in FUNS_IO_TYPE_FP32_OR_INT8 : first_arg = node . args [ 0 ] assert isinstance ( first_arg , Node ) ( _prev_node_input_type , ","elif node . target in FUNS_IO_TYPE_INT8 : return ( NodeInputOrOutputType . INT8 , NodeInputOrOutputType . INT8 ) elif node . target in FUNS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) ( _prev_node_input_type , "
"prev_node = node . args [ 0 ] assert isinstance ( prev_node , Node ) ","prev_node = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( prev_node , Node ) "
"prev_node = node . args [ 0 ] assert isinstance ( prev_node , Node ) ","prev_node = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( prev_node , Node ) "
"return ( prev_node_output_type , NodeInputOrOutputType . FP16 ) elif node . target in METHS_IO_TYPE_FP32_OR_INT8 : first_arg = node . args [ 0 ] assert isinstance ( first_arg , Node ) ","return ( prev_node_output_type , NodeInputOrOutputType . FP16 ) elif node . target in METHS_IO_TYPE_FP32_OR_INT8 : first_arg = get_normalized_nth_input ( node , gm , 0 ) assert isinstance ( first_arg , Node ) "
"env_callable = lambda fn : { ""definitions"" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 5 , sharded_keys = { ""definitions"" } , ) ","env_callable = lambda fn : { ""definitions"" : [ ComputeUnboxingFunctions ( Target . DEFINITION , selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 5 , sharded_keys = { ""definitions"" } , ) "
"env_callable = lambda fn : { ""unboxed_ops"" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 10 , sharded_keys = { ""unboxed_ops"" } , ) ","env_callable = lambda fn : { ""unboxed_ops"" : [ ComputeCodegenUnboxedKernels ( selector ) ( fn ) ] } , num_shards = 1 if selected_op_num < sharding_threshold else 10 , sharded_keys = { ""unboxed_ops"" } , ) "
"functools . wraps ( next_func ) def wrap_next ( * args , * * kwargs ) : with context ( ) : return next_func ( * args , * * kwargs ) namespace [ next__ = wrap_next ","functools . wraps ( next_func ) def wrap_next ( * args , * * kwargs ) : if torch . autograd . _profiler_enabled ( ) : return next_func ( * args , * * kwargs ) else : return next_func ( * args , * * kwargs ) namespace [ next__ = wrap_next "
"check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , f ""3D or 4D (batch mode) tensor expected for input, but got: {self}"" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape ","check ( ( self . ndim == 3 and valid_dims ) or ( self . ndim == 4 and valid_dims and self . size ( 3 ) != 0 ) , lambda : f ""3D or 4D (batch mode) tensor expected for input, but got: {self}"" ) if self . ndim == 4 : nbatch , nplane , input_h , input_w = self . shape "
"def meta_dot ( self , tensor ) : check ( self . dim ( ) == 1 and tensor . dim ( ) == 1 , f ""1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors"" ) return self . new_empty ( ( ) ) ","def meta_dot ( self , tensor ) : check ( self . dim ( ) == 1 and tensor . dim ( ) == 1 , lambda : f ""1D tensors expected, but got {self.dim()}D and {tensor.dim()}D tensors"" ) return self . new_empty ( ( ) ) "
"def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : ( self . path / req . storage_key ) . write_bytes ( req . bytes . getbuffer ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut ","def write_bytes ( self , requests : List [ BytesWriteRequest ] ) -> Future [ None ] : for req in requests : with ( self . path / req . storage_key ) . open ( ""wb"" ) as w : w . write ( req . bytes . getbuffer ( ) ) os . fsync ( w . fileno ( ) ) fut : Future [ None ] = Future ( ) fut . set_result ( None ) return fut "
"with torch . autograd . graph . saved_tensors_hooks ( pack , unpack ) : output = function ( * args ) if torch . cuda . _initialized and not had_cuda_in_fwd : Cuda was not initialized before running the forward , so we didn stash the CUDA state . ","with torch . autograd . graph . saved_tensors_hooks ( pack , unpack ) : output = function ( * args ) if torch . cuda . _initialized and preserve_rng_state and not had_cuda_in_fwd : Cuda was not initialized before running the forward , so we didn stash the CUDA state . "
"self . _op_impls = set ( ) self . kind = kind self . dispatch_key = dispatch_key if kind != ""IMPL"" and kind != ""DEF"" : raise ValueError ( ""Unsupported kind: "" , kind ) def __repr__ ( self ) : return ""Library(kind={}, ns={}, dispatch_key={})>"" . format ( self . kind , self . ns , self . dispatch_key ) ","self . _op_impls = set ( ) self . kind = kind self . dispatch_key = dispatch_key def __repr__ ( self ) : return ""Library(kind={}, ns={}, dispatch_key={})>"" . format ( self . kind , self . ns , self . dispatch_key ) "
"grad_output : Tensor , output : Tensor , dim : int , input_dtype : int ) : new_grad = grad_output * output return new_grad - output * torch . sum ( new_grad , dim = dim , keepdim = True ) register_decomposition ( aten . _log_softmax_backward_data ) ","grad_output : Tensor , output : Tensor , dim : int , input_dtype : int ) : new_grad = grad_output * output grad_input = new_grad - output * torch . sum ( new_grad , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) register_decomposition ( aten . _log_softmax_backward_data ) "
"grad_input = grad_output - torch . exp ( output ) * torch . sum ( grad_output , dim = dim , keepdim = True ) return grad_input TODO : the type annotations on arguments are not quite right ","grad_input = grad_output - torch . exp ( output ) * torch . sum ( grad_output , dim = dim , keepdim = True ) return aten . to ( grad_input , dtype = input_dtype ) TODO : the type annotations on arguments are not quite right "
"idx , data = r if not done_event . is_set ( ) and not isinstance ( data , ExceptionWrapper ) : try : data = pin_memory ( data ) except Exception : data = ExceptionWrapper ( where = ""in pin memory thread for device {}"" . format ( device_id ) ) ","index = self . _next_index ( ) may raise StopIteration data = self . _dataset_fetcher . fetch ( index ) may raise StopIteration if self . _pin_memory : data = _utils . pin_memory . pin_memory ( data , self . _pin_memory_device ) return data "
"optimized_scripted_module = optimize_for_mobile ( traced_script_module ) exported_optimized_scripted_module = ( optimized_scripted_module . _save_for_lite_interpreter ( ""model.ptl"" ) ) ","optimized_scripted_module = optimize_for_mobile ( traced_script_module ) exported_optimized_scripted_module = optimized_scripted_module . _save_for_lite_interpreter ( ""model.ptl"" ) "
"report ( ""nCopying extension {}"" . format ( ext . name ) ) relative_site_packages = ( sysconfig . get_path ( ""purelib"" ) . replace ( sysconfig . get_path ( ""data"" ) , """" ) . lstrip ( os . path . sep ) ) src = os . path . join ( ""torch"" , relative_site_packages , filename ) ","report ( ""nCopying extension {}"" . format ( ext . name ) ) relative_site_packages = sysconfig . get_path ( relib replace ( sysconfig . get_path ( ta . lstrip ( os . path . sep ) src = os . path . join ( ""torch"" , relative_site_packages , filename ) "
"if RUN_BUILD_DEPS : build_deps ( ) ( extensions , cmdclass , packages , entry_points , extra_install_requires , ) = configure_extension_build ( ) install_requires += extra_install_requires ","if RUN_BUILD_DEPS : build_deps ( ) extensions , cmdclass , packages , entry_points , extra_install_requires = configure_extension_build ( ) install_requires += extra_install_requires "
"X = torch . triangular_solve ( phi , L . transpose ( -1 , -2 ) . conj ( ) , upper = True ) . solution A_grad = ( torch . triangular_solve ( X . transpose ( -1 , -2 ) . conj ( ) P . transpose ( -1 , -2 ) , U , upper = True ) . solution . transpose ( -1 , -2 ) . conj ( ) ) return A_grad , None , None ","X = torch . triangular_solve ( phi , L . transpose ( -1 , -2 ) . conj ( ) , upper = True ) . solution A_grad = torch . triangular_solve ( X . transpose ( -1 , -2 ) . conj ( ) P . transpose ( -1 , -2 ) , U , upper = True ) . solution . transpose ( -1 , -2 ) . conj ( ) return A_grad , None , None "
"data_value = data_buf . getvalue ( ) return ( data_value , serialized_storages , serialized_dtypes , importer . zip_reader if importer else None , ) def _load_storages ( id , zip_reader , obj_bytes , serialized_storages ) : ","data_value = data_buf . getvalue ( ) return data_value , serialized_storages , serialized_dtypes , importer . zip_reader if importer else None def _load_storages ( id , zip_reader , obj_bytes , serialized_storages ) : "
"LockType = _dummy_thread . LockType boolean_dispatched : ""weakref.WeakKeyDictionary[Callable, Dict[str, Callable]]"" = ( weakref . WeakKeyDictionary ( ) ) def createResolutionCallbackFromEnv ( lookup_base ) : ","LockType = _dummy_thread . LockType boolean_dispatched : akref . WeakKeyDictionary [ Callable , Dict [ str , Callable ] ] weakref . WeakKeyDictionary ( ) def createResolutionCallbackFromEnv ( lookup_base ) : "
"for arg in f . args . args : arg_annotation_str = ( get_annotation_str ( arg . annotation ) if arg . annotation else None ) ","for arg in f . args . args : arg_annotation_str = get_annotation_str ( arg . annotation ) if arg . annotation else None "
"return_annotation = signature . return_annotation valid_return_annotation_type = ( return_annotation is not inspect . Parameter . empty and not isinstance ( return_annotation , str ) ) if valid_literal_annotation and valid_return_annotation_type : ","return_annotation = signature . return_annotation valid_return_annotation_type = return_annotation is not inspect . Parameter . empty and not isinstance ( return_annotation , str ) if valid_literal_annotation and valid_return_annotation_type : "
"fns = [ getattr ( cls , name ) for name in cls . __dict__ if inspect . isroutine ( getattr ( cls , name ) ) ] captures = { } for fn in fns : ","fns = [ getattr ( cls , name ) for name in cls . __dict__ if inspect . isroutine ( getattr ( cls , name ) ) ] captures = { } for fn in fns : "
"""default"" : default , ""arg_name"" : arg_name , } return fn ","""default"" : default , ""arg_name"" : arg_name } return fn "
"DEFAULT = ""default (compile if called from a exported function / forward)"" COPY_TO_SCRIPT_WRAPPER = ( ""if this method is not scripted, copy the python method onto the scripted model"" ) def export ( fn ) : ","DEFAULT = ""default (compile if called from a exported function / forward)"" COPY_TO_SCRIPT_WRAPPER = ""if this method is not scripted, copy the python method onto the scripted model"" def export ( fn ) : "
"if prop . fset : setattr ( prop . fset , ""_torchscript_modifier"" , FunctionModifiers . UNUSED ) noqa : B010 return prop ","if prop . fset : setattr ( prop . fset , ""_torchscript_modifier"" , FunctionModifiers . UNUSED ) noqa : B010 return prop "
"if len ( body ) != 1 or not ( is_pass ( body [ 0 ] ) or is_ellipsis ( body [ 0 ] ) ) : msg = ( ""Only `pass` statement or `...` can be the body of overload declaration:n"" ) msg += ""n"" . join ( parsed_def . source . split ( ""n"" ) [ : 3 ] ) ","if len ( body ) != 1 or not ( is_pass ( body [ 0 ] ) or is_ellipsis ( body [ 0 ] ) ) : msg = ""Only `pass` statement or `...` can be the body of overload declaration:n"" msg += 'n' . join ( parsed_def . source . split ( ""n"" ) [ : 3 ] ) "
"for i in range ( 2 ) : assert ( current_frame is not None ) current_frame = current_frame . f_back ","for i in range ( 2 ) : assert current_frame is not None Optional [ FrameType ] current_frame = current_frame . f_back "
"( * A . shape [ : -1 ] , A . size ( -1 ) - D . size ( -1 ) ) , dtype = A . dtype , device = A . device , generator = gen , ) ) U_ortho_t = U_ortho . transpose ( -2 , -1 ) . contiguous ( ) ","( * A . shape [ : -1 ] , A . size ( -1 ) - D . size ( -1 ) ) , dtype = A . dtype , device = A . device , generator = gen ) ) U_ortho_t = U_ortho . transpose ( -2 , -1 ) . contiguous ( ) "
"if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) : return handle_torch_function ( ","if not torch . jit . is_scripting ( ) : tensor_ops = ( A , B , X , iK ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( "
"n = ( k if n is None else n ) if X is None else X . shape [ -1 ] if m < 3 * n : raise ValueError ( ""LPBPCG algorithm is not applicable when the number of A rows (={})"" "" is smaller than 3 x the number of requested eigenpairs (={})"" . format ( m , n ) ) method = ""ortho"" if method is None else method ","n = ( k if n is None else n ) if X is None else X . shape [ -1 ] if ( m < 3 * n ) : raise ValueError ( BPCG algorithm is not applicable when the number of A rows ( = { } ) s smaller than 3 x the number of requested eigenpairs ( = { } ) . format ( m , n ) ) method = tho f method is None else method "
"B_ = bB [ i ] if bB is not None else None X_ = ( torch . randn ( ( m , n ) , dtype = dtype , device = device ) if bX is None else bX [ i ] ) assert len ( X_ . shape ) == 2 and X_ . shape == ( m , n ) , ( X_ . shape , ( m , n ) ) ","B_ = bB [ i ] if bB is not None else None X_ = torch . randn ( ( m , n ) , dtype = dtype , device = device ) if bX is None else bX [ i ] assert len ( X_ . shape ) == 2 and X_ . shape == ( m , n ) , ( X_ . shape , ( m , n ) ) "
"self . tracker = tracker m = iparams [ ""m"" ] n = iparams [ ""n"" ] variable parameters self . X = X ","self . tracker = tracker m = iparams [ 'm' ] n = iparams [ 'n' ] variable parameters self . X = X "
"E , X , R = self . E , self . X , self . R rerr = ( torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 ) converged = rerr < tol ","E , X , R = self . E , self . X , self . R rerr = torch . norm ( R , 2 , ( 0 , ) ) * ( torch . norm ( X , 2 , ( 0 , ) ) * ( A_norm + E [ : X . shape [ -1 ] ] * B_norm ) ) * * -1 converged = rerr < tol "
"count += 1 assert count >= prev_count , ( ""the number of converged eigenpairs "" ""(was {}, got {}) cannot decrease"" . format ( prev_count , count ) ) ","count += 1 assert count >= prev_count , e number of converged eigenpairs as { } , got { } ) cannot decrease ormat ( prev_count , count ) "
"d_col = d_row . reshape ( d_row . shape [ 0 ] , 1 ) TODO : Consider reordering the operations to work with lower - triangular matrices R = ( torch . linalg . cholesky ( ( ( SBS * d_row ) * d_col ) . transpose ( -2 , -1 ) . conj ( ) ) . transpose ( -2 , -1 ) . conj ( ) ) ","d_col = d_row . reshape ( d_row . shape [ 0 ] , 1 ) TODO : Consider reordering the operations to work with lower - triangular matrices R = torch . linalg . cholesky ( ( ( SBS * d_row ) * d_col ) . transpose ( -2 , -1 ) . conj ( ) ) . transpose ( -2 , -1 ) . conj ( ) "
"if not torch . jit . is_scripting ( ) : tensor_ops = ( A , M ) if not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) : return handle_torch_function ( svd_lowrank , tensor_ops , A , q = q , niter = niter , M = M ) return _svd_lowrank ( A , q = q , niter = niter , M = M ) ","if not torch . jit . is_scripting ( ) : tensor_ops = ( A , M ) if ( not set ( map ( type , tensor_ops ) ) . issubset ( ( torch . Tensor , type ( None ) ) ) and has_torch_function ( tensor_ops ) ) : return handle_torch_function ( svd_lowrank , tensor_ops , A , q = q , niter = niter , M = M ) return _svd_lowrank ( A , q = q , niter = niter , M = M ) "
"for overloads and raise an exception if there are more than one . qualified_op_name = ""{}::{}"" . format ( self . name , op_name ) op = torch . _C . _jit_get_operation ( qualified_op_name ) ","for overloads and raise an exception if there are more than one . qualified_op_name = : : { } ormat ( self . name , op_name ) op = torch . _C . _jit_get_operation ( qualified_op_name ) "
"if ( ""CompositeImplicitAutograd"" in dispatchKeys and ""CompositeExplicitAutograd"" in dispatchKeys ) : raise RuntimeError ( ""Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed."" ) ","if mpositeImplicitAutograd n dispatchKeys and mpositeExplicitAutograd n dispatchKeys : raise RuntimeError ( ""Registration to both CompositeImplicitAutograd and CompositeExplicitAutograd is not allowed."" ) "
"if ( getattr ( ret . __class__ , ""__setstate__"" , Tensor . __setstate__ ) is not Tensor . __setstate__ ) : ret . __setstate__ ( state ) else : if isinstance ( state , tuple ) : ","if getattr ( ret . __class__ , ""__setstate__"" , Tensor . __setstate__ ) is not Tensor . __setstate__ : ret . __setstate__ ( state ) else : if isinstance ( state , tuple ) : "
